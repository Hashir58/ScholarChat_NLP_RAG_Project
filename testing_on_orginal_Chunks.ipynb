{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f99110f2",
   "metadata": {},
   "source": [
    "# üìö Original Document Processing Pipeline (No Additional Chunking)\n",
    "\n",
    "## üéØ Project Overview\n",
    "This pipeline processes the **original 568 manually-created document chunks** without performing any additional automatic chunking. The goal is to preserve the semantic integrity and contextual boundaries that were carefully established during manual preprocessing.\n",
    "\n",
    "## üîÑ Processing Philosophy: \"Respect the Original Chunking\"\n",
    "\n",
    "### üìã Background Context\n",
    "- **Initial Dataset**: 4 comprehensive academic/reference materials\n",
    "  - üìñ NLP Course Material\n",
    "  - ‚òÅÔ∏è Cloud Computing (ICC) Course Material  \n",
    "  - üáµüá∞ Pakistan Studies Course Material\n",
    "  - üìú \"The Sealed Nectar\" - Award-winning biography of Prophet Muhammad\n",
    "- **Manual Preprocessing**: Each lecture/chapter (2000-5000 words) was manually divided into ~500-word contextually coherent segments\n",
    "- **Result**: 568 semantically meaningful document chunks with preserved context boundaries\n",
    "\n",
    "### üö´ What We're NOT Doing\n",
    "- ‚ùå **No Additional Chunking**: Avoiding further fragmentation that might break semantic flow\n",
    "- ‚ùå **No Token-Based Splitting**: Not using arbitrary token limits to split documents\n",
    "- ‚ùå **No Automatic Boundary Detection**: Trusting human judgment over algorithmic splitting\n",
    "- ‚ùå **No Size Normalization**: Preserving natural document length variations\n",
    "\n",
    "### ‚úÖ What We ARE Doing\n",
    "- ‚úÖ **Metadata Enhancement**: Enriching each document with comprehensive metadata\n",
    "- ‚úÖ **Keyword Extraction**: Advanced filename + content-based keyword identification\n",
    "- ‚úÖ **Category Optimization**: Domain-specific processing weights (Technical vs Narrative)\n",
    "- ‚úÖ **Quality Analysis**: Document quality metrics and content type classification\n",
    "- ‚úÖ **Preservation Focus**: Maintaining original chunk boundaries and context\n",
    "\n",
    "## üéØ Core Hypothesis\n",
    "> **\"Manual chunking with domain expertise produces better semantic units than algorithmic splitting\"**\n",
    "\n",
    "We hypothesize that the original 568 manually-created chunks will provide:\n",
    "- Better contextual coherence\n",
    "- Improved retrieval accuracy  \n",
    "- More meaningful document boundaries\n",
    "- Enhanced user experience in RAG applications\n",
    "\n",
    "## üìä Processing Methodology\n",
    "\n",
    "### üîç Document Analysis Pipeline\n",
    "1. **File Ingestion**: Read original 568 .txt files as individual units\n",
    "2. **Content Cleaning**: Normalize whitespace and formatting without altering structure\n",
    "3. **Metadata Extraction**: \n",
    "   - Filename-based keywords\n",
    "   - Content-based technical terms, acronyms, definitions\n",
    "   - Document statistics (tokens, words, sentences, paragraphs)\n",
    "4. **Category-Specific Weighting**:\n",
    "   - **ICC/NLP**: Higher technical weight (1.5-1.8) for CS concepts\n",
    "   - **Pakistan Studies/Sealed Nectar**: Narrative weight (1.0-1.2) for historical content\n",
    "5. **Quality Metrics**: Readability scores, content type classification, keyword density\n",
    "\n",
    "### üìÅ Output Structure\n",
    "```\n",
    "processed_documents_568/\n",
    "‚îú‚îÄ‚îÄ processed_documents.json          # All 568 documents with metadata\n",
    "‚îú‚îÄ‚îÄ documents_icc.json                # Cloud Computing documents\n",
    "‚îú‚îÄ‚îÄ documents_nlp.json                # NLP course documents  \n",
    "‚îú‚îÄ‚îÄ documents_pakistan_studies.json   # Pakistan Studies documents\n",
    "‚îú‚îÄ‚îÄ documents_sealed_nectar.json      # Biography documents\n",
    "‚îú‚îÄ‚îÄ processing_statistics.json        # Comprehensive analytics\n",
    "‚îî‚îÄ‚îÄ summary_report.txt               # Human-readable summary\n",
    "```\n",
    "\n",
    "## üß™ Experimental Design\n",
    "This processing approach enables direct comparison between:\n",
    "- **Original 568 Chunks** (this pipeline) vs **704 Auto-Generated Chunks** (previous pipeline)\n",
    "- Manual semantic boundaries vs algorithmic splitting\n",
    "- Domain expertise vs automated processing\n",
    "- Context preservation vs token optimization\n",
    "\n",
    "## üéØ Success Metrics\n",
    "- **Retrieval Quality**: Semantic relevance of retrieved documents\n",
    "- **Context Coherence**: Completeness of information in individual chunks  \n",
    "- **User Experience**: Readability and usefulness of retrieved content\n",
    "- **System Performance**: Processing efficiency and response accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Ready for RAG Pipeline Integration\n",
    "The output maintains the **semantic integrity** of your original preprocessing while adding **enhanced metadata** for optimal retrieval performance. Each document preserves its intended contextual boundaries while gaining advanced searchability features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3269d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Document Processing Started (No Additional Chunking - v2 with 'content' & 'content_type')\n",
      "======================================================================\n",
      "üìÅ Processing: ICC (icc_text_files)\n",
      "   ‚úÖ icc01_API_Evolution_Data_Formats_&_The_Emergence_of_Standards_SOAP_&_REST.txt: 676 tokens, 489 words [issue, acid, wsdl]\n",
      "   ‚úÖ icc02_Advantages_r_Disadvantages_of_Private_Cloud_&_Hybrid_Cloud_Introduction.txt: 563 tokens, 445 words [because, compared, public]\n",
      "   ‚úÖ icc03_Advantages_r_Disadvantages_of_Public_Cloud_&_Private_Cloud_Introduction.txt: 535 tokens, 429 words [because, public, minimal investment]\n",
      "   ‚úÖ icc04_Alternatives_to_VMs_Containers_Introduction.txt: 552 tokens, 441 words [to resource, portability, because]\n",
      "   ‚úÖ icc05_Basic_Security_Terms_Continued_&_Threat_Agents.txt: 512 tokens, 422 words [the probability of a threat successfully occurring, continued, authentication]\n",
      "   ‚úÖ icc06_Benefits_and_Steps_of_Cloud_Threat_Modeling.txt: 564 tokens, 396 words [clearly, define mitigation strategies:, privilege]\n",
      "   ‚úÖ icc07_Big_Data_Definition_Sources_Examples_&_Four_Dimensions.txt: 633 tokens, 442 words [examples, volume, big data]\n",
      "   ‚úÖ icc08_Big_Data_Market_Driving_Factors_&_Data_Analytical_Tools.txt: 610 tokens, 451 words [apache spark: a fast and general-purpose cluster co, key, cassandra]\n",
      "   ‚úÖ icc09_CAP_Theorem_Introduction_and_Core_Properties.txt: 480 tokens, 387 words [icc09, brewer, a venn diagram is often used to illustrate this, wi]\n",
      "   ‚úÖ icc10_CAP_Theorem_Cases_Importance_of_Partition_Tolerance_&_Database_as_a_Service_DynamoDB_Intro.txt: 622 tokens, 473 words [therefore, purpose: dynamodb is specifically used to design hi, continued]\n",
      "   ‚úÖ icc11_Classic_Consistent_Hashing_Pros_r_Cons_&_Consistent_Hashing_with_T_Tokens_per_Node.txt: 587 tokens, 455 words [improvement, multiple tokens, reduced rehashing on node changes: when a new physi]\n",
      "   ‚úÖ icc12_Cloud_Bursting_Architecture.txt: 489 tokens, 375 words [cloud bursting architecture \n",
      "cloud bursting, how it works, threshold]\n",
      "   ‚úÖ icc13_Cloud_Computing_Players_&_Ecosystem_Complexity.txt: 489 tokens, 356 words [ibm, google app engine, oracle]\n",
      "   ‚úÖ icc14_Cloud_Economics_Cost_Efficiency_Scalability_and_Flexibility.txt: 414 tokens, 323 words [efficiency, flexibility, with]\n",
      "   ‚úÖ icc15_Cloud_Risks_and_Challenges_Security_Compliance_and_Downtime.txt: 342 tokens, 280 words [gdpr, dependence, finally]\n",
      "   ‚úÖ icc16_Cloud_Security_Fundamentals.txt: 630 tokens, 474 words [customer relationship management, cloud security fundamentals, pii]\n",
      "   ‚úÖ icc17_Cloud_Security_Threats_&_Introduction_to_Cloud_Threat_Modeling.txt: 622 tokens, 454 words [privilege, elevation, man]\n",
      "   ‚úÖ icc18_Cloud_Storage_Overview_and_Types.txt: 480 tokens, 375 words [center, examples, storage]\n",
      "   ‚úÖ icc19_Community_Cloud_Use_Cases_&_Advantages_r_Disadvantages.txt: 533 tokens, 440 words [because, healthcare consortium: multiple hospitals, clinics,, disadvantages]\n",
      "   ‚úÖ icc20_Community_Cloud_Definition_Logical_Boundaries_&_Purpose.txt: 526 tokens, 449 words [by a combination of one or more organizations in th, common security, compliance needs]\n",
      "   ‚úÖ icc21_Consistent_Hashing_Details_Use_Cases_Advantages_&_Modular_Hashing.txt: 644 tokens, 489 words [distributed hash tables, comparison context, continued]\n",
      "   ‚úÖ icc22_Containers_vs_Docker_vs_Kubernetes_&_Side_by_Side_Comparison.txt: 705 tokens, 459 words [app a, guest os, guest operating \n",
      "system]\n",
      "   ‚úÖ icc23_DATA_NEVER_SLEEPS_2.0_&_The_Big_Data_Explosion.txt: 526 tokens, 405 words [volume, big data, describes]\n",
      "   ‚úÖ icc24_Data_Replication_The_Need_and_Rationale.txt: 549 tokens, 456 words [increased data availability, the context, backups and restores take a long time:]\n",
      "   ‚úÖ icc25_Drawbacks_of_Virtualization_Performance_&_Common_Hardware.txt: 587 tokens, 450 words [common hardware \n",
      "another, drawbacks of virtualization, memory]\n",
      "   ‚úÖ icc26_Drawbacks_of_Virtualization_Resource_Consumption_&_VM_Density.txt: 578 tokens, 439 words [drawbacks of virtualization, risk: concentrating too many critical vms on a sing, disk space]\n",
      "   ‚úÖ icc27_Dynamic_Scalability_Architecture.txt: 515 tokens, 399 words [sale state, blessed friday sale, load balancer: a load balancer is in place to evenl]\n",
      "   ‚úÖ icc28_DynamoDB_Security_Cost_Durability_and_Streams.txt: 556 tokens, 385 words [storage, no-trust policy for sensitive data & kms integratio, consuming streams]\n",
      "   ‚úÖ icc29_Elastic_Disk_Provisioning_Architecture.txt: 518 tokens, 403 words [works, initial provisioning, how]\n",
      "   ‚úÖ icc30_Elastic_Resource_Capacity_Architecture.txt: 551 tokens, 409 words [aws \n",
      "lambda, immediate release, resources]\n",
      "   ‚úÖ icc31_File_Storage_Overview.txt: 449 tokens, 339 words [azure files, center, examples]\n",
      "   ‚úÖ icc32_Full_Virtualization.txt: 285 tokens, 193 words [guest os, examples, image description]\n",
      "   ‚úÖ icc33_Googles_Mission_Willpower_&_Electricity_Management_Cooling.txt: 570 tokens, 449 words [liquid cooling innovation, efficiency, traditional cooling: traditionally, data centers we]\n",
      "   ‚úÖ icc34_HDFS_Data_Storage_Block_Size_Metadata_and_Replication_Factor.txt: 653 tokens, 493 words [information, storage, mapreduce]\n",
      "   ‚úÖ icc35_Hadoop_Ecosystem_Components_&_HDFS_Details.txt: 617 tokens, 451 words [mapreduce, big data, efficient data processing]\n",
      "   ‚úÖ icc36_Hadoop_Ecosystem_Overview_&_Core_Components_HDFS_MapReduce_YARN.txt: 697 tokens, 458 words [function, data ingestion, hadoop yarn]\n",
      "   ‚úÖ icc37_Handling_Temporary_Failures_Sloppy_Quorum_&_Hinted_Handoff.txt: 352 tokens, 274 words [benefits, handling, flexibility]\n",
      "   ‚úÖ icc38_Hardware_Assisted_Virtualization.txt: 577 tokens, 443 words [efficiency, equivalence, amd]\n",
      "   ‚úÖ icc39_High_Availability_for_Writes_and_Recovering_from_Permanent_Failures.txt: 452 tokens, 376 words [permanent, entropy using merkle trees, a merkle tree]\n",
      "   ‚úÖ icc40_Higher_Availability_through_Replication_&_Introduction_to_RAID.txt: 600 tokens, 468 words [increased data availability, data distribution: data is split into equal-sized b, minimum disks]\n",
      "   ‚úÖ icc41_How_is_Cloud_Computing_Different_Introduction_&_Paradigm_Context.txt: 438 tokens, 353 words [paradigm context, infrastructure, how]\n",
      "   ‚úÖ icc42_Hybrid_Cloud_Use_Cases_Advantages_&_Disadvantages.txt: 542 tokens, 434 words [because, flexibility, complexity]\n",
      "   ‚úÖ icc43_Introduction_to_Consistent_Hashing_&_Modulo_Hashing.txt: 666 tokens, 516 words [inefficiency, then, the need]\n",
      "   ‚úÖ icc44_Introduction_to_Virtualization_Approaches_&_Full_Virtualization.txt: 620 tokens, 432 words [virtualization approaches, guest os, para-virtualization: guest os is modified to be awa]\n",
      "   ‚úÖ icc45_Introduction_to_Web_Services_&_Early_Evolution.txt: 502 tokens, 365 words [remote procedure call (rpc), a microsoft, early http apis]\n",
      "   ‚úÖ icc46_Key_Technologies_in_SOA_SOAP_WSDL_JSON_HTTP_&_SOAP_vs_JSON.txt: 582 tokens, 433 words [wsdl, soap (simple object access protocol):, key technologies]\n",
      "   ‚úÖ icc47_Mitigation_Information_Disclosure_DoS_EoP_&_Cloud_Security_Mechanisms.txt: 776 tokens, 561 words [elevation, mechanisms, mitigation techniques]\n",
      "   ‚úÖ icc48_Mitigation_Strategies_Spoofing_Tampering_Repudiation.txt: 652 tokens, 452 words [data integrity checks:, auditing, immutable logs:]\n",
      "   ‚úÖ icc49_Modern_Data_Center_Interiors_&_Components.txt: 599 tokens, 457 words [center, seismic bracing, modern data center interiors]\n",
      "   ‚úÖ icc50_Modular_Hashing_Pros_r_Cons_&_Classic_Consistent_Hashing.txt: 532 tokens, 408 words [modular hashing (continued):, simple, continued]\n",
      "   ‚úÖ icc51_More_SOA_Principles_&_Introduction_to_Web_Services.txt: 484 tokens, 365 words [platform independence, benefits, continued]\n",
      "   ‚úÖ icc52_More_Web_Service_Principles_in_SOA_Context.txt: 550 tokens, 437 words [independent operation, hiding internal complexity: web services follow the, combining services: web services can be combined or]\n",
      "   ‚úÖ icc53_Multitenant_Technology_and_Multitenancy_vs_Virtualization.txt: 367 tokens, 273 words [multitenant technology, metered usage, part]\n",
      "   ‚úÖ icc54_NIST_Cloud_Model_Essential_Characteristics.txt: 377 tokens, 279 words [ibm, microsoft azure regions, users]\n",
      "   ‚úÖ icc55_Network_Hardware_and_Storage_Hardware.txt: 311 tokens, 259 words [storage, san fabric ‚Äî network that links storage devices (li, san fabric]\n",
      "   ‚úÖ icc56_NoSQL_Databases_DynamoDB_Features_&_Administrative_Overhead_Reduction.txt: 666 tokens, 470 words [different solution,, features, storage]\n",
      "   ‚úÖ icc57_Para_Virtualization_&_Binary_Translation_Software_based_Virtualization.txt: 609 tokens, 440 words [virtualization approaches, because, works]\n",
      "   ‚úÖ icc58_Para_Virtualization.txt: 294 tokens, 209 words [virtualization \n",
      "in, optimized code paths, image description]\n",
      "   ‚úÖ icc59_Parity_Visual_RAID_5_Block_Level_Striping_with_Distributed_Parity.txt: 573 tokens, 437 words [read performance, write performance, minimum disks]\n",
      "   ‚úÖ icc60_Power_in_Data_Centers_&_Googles_Top_Secret_Data_Center_Introduction.txt: 600 tokens, 451 words [center, pue explained, gmail]\n",
      "   ‚úÖ icc61_Principles_Used_in_Dynamo.txt: 307 tokens, 230 words [flexible, incremental scalability:, benefits]\n",
      "   ‚úÖ icc62_Private_Cloud_Details_&_Use_Cases.txt: 458 tokens, 383 words [banking system: a bank often builds a private cloud, large hospital, internal development]\n",
      "   ‚úÖ icc63_Public_Cloud_Definition_and_Use_Cases.txt: 525 tokens, 428 words [google app engine, public, mobile app startup: a small startup developing a mo]\n",
      "   ‚úÖ icc64_RAID_0_Visual_RAID_1_Mirroring_&_Introduction_to_Parity.txt: 566 tokens, 452 words [disk 1 contains segments labeled a, c, e, g (from t, read performance, write performance]\n",
      "   ‚úÖ icc65_RAID_5_Visual_RAID_10_1+0_and_Block_Level_RAID_Operations.txt: 598 tokens, 445 words [minimum disks, during, level raid works]\n",
      "   ‚úÖ icc66_RESTful_Architecture_Principles_Verbs_and_Resource_Design.txt: 932 tokens, 645 words [design, key, for]\n",
      "   ‚úÖ icc67_Racks_&_Data_Centers.txt: 558 tokens, 420 words [scalable physical space, data center \n",
      "a, server]\n",
      "   ‚úÖ icc68_Real_World_Cloud_Threat_Modeling_in_AWS_Scope_Data_Flows_Threats.txt: 665 tokens, 470 words [function, find threats, code modification]\n",
      "   ‚úÖ icc69_Redundant_Storage_Architecture.txt: 500 tokens, 390 words [because, storage, redundant]\n",
      "   ‚úÖ icc70_Resource_Pooling_Architecture.txt: 571 tokens, 440 words [ubuntu, sibling pools: pools at the same hierarchical level, virtual]\n",
      "   ‚úÖ icc71_SOA_Architecture_Diagram_&_Core_SOA_Principles.txt: 596 tokens, 418 words [delivery, service contract, wsdl]\n",
      "   ‚úÖ icc72_SOA_Terms_Discovery_Binding_&_Interaction_Model.txt: 592 tokens, 427 words [function, discovery:, basic terms familiarity]\n",
      "   ‚úÖ icc73_SOA_and_Web_Services_Principles_Embodied.txt: 508 tokens, 380 words [flexibility and scalability: this independence ensu, wsdl, flexibility]\n",
      "   ‚úÖ icc74_Servers_Compact_Servers_&_Data_Center_Aesthetics.txt: 535 tokens, 430 words [center, server, modern data center: shows a clean, organized aisle]\n",
      "   ‚úÖ icc75_Service_Load_Balancing_Architecture.txt: 481 tokens, 396 words [users, server a, service]\n",
      "   ‚úÖ icc76_Service_Oriented_Architecture_SOA_Core_Concepts_&_Examples.txt: 608 tokens, 455 words [examples, soa terms, service 1: search for products]\n",
      "   ‚úÖ icc77_Soap_vs_rest.txt: 477 tokens, 353 words [post, representational state transfer, soap]\n",
      "   ‚úÖ icc78_Techniques_Used_in_Dynamo.txt: 207 tokens, 159 words [helps, improves, dynamo \n",
      "consistent hashing]\n",
      "   ‚úÖ icc79_The_Digital_Universe_2020_2025_&_Data_Growth_2010_2025.txt: 558 tokens, 401 words [big data, digital, data growth]\n",
      "   ‚úÖ icc80_Types_of_Big_Data_Tools_File_Systems_Distributed_File_Systems.txt: 465 tokens, 374 words [examples, hadoop mapreduce, big data]\n",
      "   ‚úÖ icc81_Web_Applications_and_Multitenant_Technology_Part_1.txt: 384 tokens, 281 words [multitenant technology, part, multitenant]\n",
      "   ‚úÖ icc82_Web_Service_Features_Definition_by_W3C_&_Key_Components.txt: 651 tokens, 452 words [internet-based protocols: interactions with the goo, web service integration, practice]\n",
      "   ‚úÖ icc83_Web_Technology_and_Basic_Web_Technology.txt: 369 tokens, 296 words [describes, image description, hypertext transfer protocol]\n",
      "   ‚úÖ icc84_Web_Services_Early_Protocols_&_Architectural_Constraints.txt: 632 tokens, 476 words [prior, early http apis, before rest]\n",
      "   ‚úÖ icc85_What_is_Cloud_Computing.txt: 450 tokens, 319 words [ibm, microservices, equinix]\n",
      "   ‚úÖ icc86_Why_Study_Cloud_Computing.txt: 466 tokens, 380 words [innovation facilitator, tesla, cloud consultant]\n",
      "   ‚úÖ icc87_Workload_Distribution_Architecture.txt: 497 tokens, 385 words [under-utilization: where resources are not used eff, over, advanced]\n",
      "   üìä Summary: 87 files processed\n",
      "   üè∑Ô∏è  Keywords: 2590 unique, avg 45.9 per document\n",
      "   üìù Content Summary (based on flags): 87 technical, 71 structured, 38 with definitions\n",
      "\n",
      "üìÅ Processing: NLP (NLP_text_files)\n",
      "   ‚úÖ lec1-(a) Natural Language Processing.txt: 322 tokens, 224 words [language, chatbot\n",
      "you, search engine\n",
      "you]\n",
      "   ‚úÖ lec1-(b) NL is Confusing for Computers.txt: 520 tokens, 386 words [language, does, idioms]\n",
      "   ‚úÖ lec1-(c) Common Applications.txt: 658 tokens, 475 words [information retrieval, translating, text summarization\n",
      "\n",
      "nlp]\n",
      "   ‚úÖ lec1-(d) Job Market for NLP.txt: 391 tokens, 272 words [reading, healthcare, tensorflow]\n",
      "   ‚úÖ lec1-(e) Basic Vocabulary.txt: 977 tokens, 632 words [play, then, after]\n",
      "   ‚úÖ lec1-(f) basic Natural Language Processing (NLP) system.txt: 486 tokens, 356 words [language, a basic natural language processing, examples]\n",
      "   ‚úÖ lec1-(g) importance of NLP.txt: 116 tokens, 91 words [massive text data everywhere\n",
      "with, gap between humans, human]\n",
      "   ‚úÖ lec1-(h) NLP need special techniques.txt: 431 tokens, 349 words [language, handling, over]\n",
      "   ‚úÖ lec2-(a) Information Extraction.txt: 510 tokens, 386 words [information, what does, unstructured text]\n",
      "   ‚úÖ lec2-(b) Typical Information Extraction Applications.txt: 420 tokens, 306 words [information, scientific research, product]\n",
      "   ‚úÖ lec2-(c) Information Retrieval.txt: 390 tokens, 317 words [information, information retrieval, query]\n",
      "   ‚úÖ lec2-(d) Question Answering.txt: 279 tokens, 206 words [joe biden, yes, how]\n",
      "   ‚úÖ lec2-(e) Dialogue Systems.txt: 235 tokens, 171 words [examples, text, what‚Äôs the weather like today?]\n",
      "   ‚úÖ lec2-(f) Summarization.txt: 327 tokens, 242 words [text, for, real]\n",
      "   ‚úÖ lec2-(g) Machine Translation & Multilinguality.txt: 283 tokens, 211 words [because, hello, key challenges]\n",
      "   ‚úÖ lec2-(h) speech recognition.txt: 245 tokens, 194 words [recognition, text, opposite technology]\n",
      "   ‚úÖ lec2-(i) Sentiment Analysis.txt: 166 tokens, 119 words [negative, positive\n",
      "\n",
      "text, lec2]\n",
      "   ‚úÖ lec2-(j) Discourse Analysis.txt: 77 tokens, 62 words [lec2, analysis, discourse analysis]\n",
      "   ‚úÖ lec2-(k) Pragmatic Analysis.txt: 71 tokens, 54 words [pragmatic analysis, lec2, could]\n",
      "   ‚úÖ lec2-(l) Popular NLP Libraries and Tools.txt: 179 tokens, 116 words [openai api, python, tools]\n",
      "   ‚úÖ lec3-(a) Search.txt: 113 tokens, 98 words [in nlp, they, retrieval models\n",
      "\n",
      "these]\n",
      "   ‚úÖ lec3-(b) Boolean Model.txt: 188 tokens, 151 words [boolean, the boolean model, boolean model]\n",
      "   ‚úÖ lec3-(c)Boolean Models Problems.txt: 387 tokens, 284 words [islamabad, query, problems]\n",
      "   ‚úÖ lec3-(d) Text Classification.txt: 135 tokens, 109 words [language, text, figuring]\n",
      "   ‚úÖ lec3-(e) Naive Bayes Classifier.txt: 537 tokens, 397 words [classifier, evaluation, text]\n",
      "   ‚úÖ lec3-(f) Evaluation measure.txt: 170 tokens, 134 words [evaluation, recall, confusion matrix]\n",
      "   ‚úÖ lec3-(g) Confusion Matrix.txt: 294 tokens, 222 words [negative, false negative, false positive]\n",
      "   ‚úÖ lec3-(h) accuaracy , precision , recall.txt: 762 tokens, 509 words [accuracy\n",
      "\n",
      "definition, recall, sensitivity]\n",
      "   ‚úÖ lec3-(i) Precision vs recall.txt: 303 tokens, 232 words [these, when, precision]\n",
      "   ‚úÖ lec3-(j) reacll hard to determined.txt: 76 tokens, 68 words [using, reacll, recall]\n",
      "   ‚úÖ lec3-(k) F-measure.txt: 519 tokens, 392 words [when, precision, the f]\n",
      "   ‚úÖ lec3-(l) Precision-Critical vs. Recall-Critical Tasks.txt: 111 tokens, 82 words [precision, recall, web]\n",
      "   ‚úÖ lec3-(m) Multi-class Confusion Matrix.txt: 378 tokens, 254 words [evaluation\n",
      "\n",
      "for, recall, matrix]\n",
      "   ‚úÖ lec3_(n)_Example_of_Confusion_matrix_percision_recall_accuracy_F1_Score.txt: 1040 tokens, 580 words [recall, score, matrix]\n",
      "   ‚úÖ lec4-(a) Probability.txt: 1211 tokens, 765 words [wsd, pos, identifies]\n",
      "   ‚úÖ lec4-(b) Conditional Probability.txt: 1215 tokens, 694 words [language, lec4, in nlp]\n",
      "   ‚úÖ lec4-(c) Language Models.txt: 1018 tokens, 286 words [language, lec4, because]\n",
      "   ‚úÖ lec4-(d) Chain Rule of Probability.txt: 1212 tokens, 695 words [lec4, rule, text]\n",
      "   ‚úÖ lec4-(e) N-gram Model.txt: 1080 tokens, 545 words [assumes, lec4, examples]\n",
      "   ‚úÖ lec4-(f) Markov Assumption.txt: 497 tokens, 151 words [lec4, transparent¬†that, markov assumption\n",
      "\n",
      "the markov]\n",
      "   ‚úÖ lec4-(g) Spelling Correction Using N-grams and Edit Distance.txt: 474 tokens, 262 words [helps, lec4, tire]\n",
      "   ‚úÖ lec4-(h) Google Search Operators.txt: 546 tokens, 305 words [lec4, google search operators, similar]\n",
      "   ‚úÖ lec5-(a) Low-level Information Extraction.txt: 487 tokens, 333 words [information, underlying techniques, london]\n",
      "   ‚úÖ lec5-(b) Named Entity Recognition (NER).txt: 170 tokens, 104 words [others, recognition, location]\n",
      "   ‚úÖ lec5-(c) NE is NOT.txt: 49 tokens, 36 words [not\n",
      "\n",
      "not, does, ner]\n",
      "   ‚úÖ lec5-(d) Challenges in NER.txt: 127 tokens, 83 words [boundary, john smith, same]\n",
      "   ‚úÖ lec5-(e) Approaches to NER.txt: 143 tokens, 84 words [shallow parsing, examples, location]\n",
      "   ‚úÖ lec5-(f) Example NER.txt: 59 tokens, 29 words [example ner output, foreign, ner]\n",
      "   ‚úÖ lec5-(g) Evaluation Metrics for NER.txt: 65 tokens, 55 words [boundary, ner, evaluation]\n",
      "   ‚úÖ lec5-(h) Applications of NER.txt: 45 tokens, 34 words [ner, lec5, applications]\n",
      "   ‚úÖ lec6-(a) Similarity Measure.txt: 2069 tokens, 1135 words [sittin, cosine similarity, specifically levenshtein distance]\n",
      "   ‚úÖ lec6-(b) Jaccard Similarity Coefficient.txt: 1742 tokens, 893 words [islamabad, bahria university, simple]\n",
      "   ‚úÖ lec6-(c) Cosine Similarity.txt: 1999 tokens, 938 words [magnitude, islamabad, works]\n",
      "   ‚úÖ lec6-(d) Pros & Cons of Cosine Similarity.txt: 229 tokens, 154 words [computationally, when, handles]\n",
      "   ‚úÖ lec6-(e) TF-IDF.txt: 1642 tokens, 918 words [language, assumes, count]\n",
      "   ‚úÖ lec6-(f) Vector Space Models (VSMs).txt: 2684 tokens, 1343 words [translating, cosine similarity, discovering]\n",
      "   ‚úÖ lec7-(a).txt: 1179 tokens, 808 words [syntactic processing, user question, ask]\n",
      "   ‚úÖ lec7-(b).txt: 2585 tokens, 1571 words [rule, simple, askjeeves\n",
      "\n",
      "about]\n",
      "   üìä Summary: 58 files processed\n",
      "   üè∑Ô∏è  Keywords: 1474 unique, avg 39.1 per document\n",
      "   üìù Content Summary (based on flags): 55 technical, 13 structured, 24 with definitions\n",
      "\n",
      "üìÅ Processing: Pakistan Studies (pakSt_text_files)\n",
      "   ‚úÖ (a)_Current_Situation_of_Pakistan.txt: 201 tokens, 154 words [musharraf, islam, corruption]\n",
      "   ‚úÖ (b)_Role_of_Leaders_in_the_Ideology_of_Pakistan.txt: 195 tokens, 156 words [wanted muslims, simple, leaders]\n",
      "   ‚úÖ (c)_Need_of_Ideology_of_Pakistan.txt: 140 tokens, 116 words [simple, muslim, pakistan]\n",
      "   ‚úÖ (e)_Historical_Foundations_of_Pakistan_Ideology.txt: 173 tokens, 133 words [pure, simple, pakistan]\n",
      "   ‚úÖ (F)_Political_Evolution_of_Pakistan.txt: 220 tokens, 149 words [simple, army involvement \n",
      "\n",
      "army, leaders]\n",
      "   ‚úÖ lec10_a_Cultural_Diversity_of_Pakistan.txt: 279 tokens, 222 words [balochistan, azad jammu, iran]\n",
      "   ‚úÖ lec10_b_Political_Structure.txt: 218 tokens, 186 words [balochistan, sindh, pakistan]\n",
      "   ‚úÖ lec10_c_Contitutional_Structure_and_Language_Diversity_in_Pakistan.txt: 322 tokens, 269 words [language, contitutional, pakistan]\n",
      "   ‚úÖ lec10_d_Religious_Diversity_and_Custom_and_tradition_of_pakistan.txt: 538 tokens, 413 words [christmas, weddings, muslim]\n",
      "   ‚úÖ lec10_e_Diversity_in_regions_of_Pakistan.txt: 908 tokens, 646 words [balochistan, azad jammu, sindh]\n",
      "   ‚úÖ lec10_f_Challenges_and_difficulties_of_Cultural_Diversity_Pakistan.txt: 446 tokens, 354 words [pakistan, these, shia muslims]\n",
      "   ‚úÖ lec11_(a)_Pakistan_and_Regional_Peace.txt: 168 tokens, 139 words [peace, lec11, muslim]\n",
      "   ‚úÖ lec11_(b)_SAARC.txt: 470 tokens, 372 words [cooperate, summit, march]\n",
      "   ‚úÖ lec11_(c)_Pakistan_as_Member_of_SAARC.txt: 142 tokens, 123 words [flexible, lec11, india]\n",
      "   ‚úÖ lec11_(d)_SAARC_Summits.txt: 84 tokens, 81 words [india\n",
      "third, pakistan, bangladesh\n",
      "second]\n",
      "   ‚úÖ lec11_(e)_SAARC_Failure.txt: 230 tokens, 181 words [south asian free trade agreement, india, lec11]\n",
      "   ‚úÖ lec11_(f)_Latest_Developments_(SAARC).txt: 245 tokens, 197 words [summit, islamabad, post]\n",
      "   ‚úÖ lec11_(g)_Shanghai_Cooperation_Organization_(SCO).txt: 638 tokens, 477 words [turkey, iran, pakistan]\n",
      "   ‚úÖ lec11_(h)_Economic_Cooperation_Organization_(ECO).txt: 398 tokens, 325 words [summit, soviet republics, eco member states]\n",
      "   ‚úÖ lec11_(i)_Objectives_of_SAARC.txt: 120 tokens, 94 words [lec11, saarc, the]\n",
      "   ‚úÖ lec11_(j)_Countries_Included_in_SAARC.txt: 52 tokens, 38 words [lec11, saarc, afghanistan\n",
      "\n",
      "bangladesh\n",
      "\n",
      "bhutan\n",
      "\n",
      "india\n",
      "\n",
      "maldives\n",
      "\n",
      "nepal\n",
      "\n",
      "pakistan\n",
      "\n",
      "sri lanka\n",
      "\n",
      "saarc]\n",
      "   ‚úÖ lec11_(k)_SAARC_role_in_South_Asia.txt: 133 tokens, 101 words [encouraging, lec11, enhancing]\n",
      "   ‚úÖ lec11_(l)_RCD.txt: 95 tokens, 75 words [iran\n",
      "\n",
      "pakistan\n",
      "\n",
      "turkey\n",
      "\n",
      "the, lec11, economic cooperation organization]\n",
      "   ‚úÖ lec12_(a)_AI_in_Pakistan.txt: 135 tokens, 110 words [artificial, pakistan, with]\n",
      "   ‚úÖ lec12_(b)_Applications_of_AI_in_Pakistan.txt: 298 tokens, 250 words [these, policies, agriculture\n",
      "\n",
      "pakistan]\n",
      "   ‚úÖ lec12_(c)_Challenges_and_Opportunities_in_AI.txt: 90 tokens, 78 words [opportunities\n",
      "\n",
      "while, ethical, efforts]\n",
      "   ‚úÖ lec12_(d)_Real_Life_Examples_of_AI_in_PAakistan.txt: 118 tokens, 92 words [real, schools, paakistan]\n",
      "   ‚úÖ lec12_(e)_Pros_&_Cons_of_AI.txt: 395 tokens, 278 words [lec12, human touch, overreliance]\n",
      "   ‚úÖ lec13_(a)_Sustainable_Development_Goals.txt: 335 tokens, 272 words [in september, communities\n",
      "\n",
      "goal, global goals]\n",
      "   ‚úÖ lec13_(b)_Sustainable_Developmental_Goals_and_Pakistan.txt: 374 tokens, 317 words [lec13, pakistan, dealing]\n",
      "   ‚úÖ lec13_(c)_strategy_for_SDGs.txt: 2077 tokens, 1732 words [infrastructure, gender, for]\n",
      "   ‚úÖ lec13_(d)_17_Goals_to_Transform_Our_World.txt: 71 tokens, 62 words [justice, our, being\n",
      "\n",
      "quality education\n",
      "\n",
      "gender equality\n",
      "\n",
      "clean water]\n",
      "   ‚úÖ lec13_(e)_Significance_of_SDGs.txt: 251 tokens, 198 words [these, governments, achieving]\n",
      "   ‚úÖ lec14_(a)_Global_Environmental_Problems_and_Pakistan.txt: 321 tokens, 271 words [problems, pakistan, global environmental problems]\n",
      "   ‚úÖ lec14_(b)_National_commitments_on_Climate_Change.txt: 45 tokens, 34 words [system restoration fund, national, eco]\n",
      "   ‚úÖ lec14_(c)_Climate_Changes.txt: 401 tokens, 326 words [deforestation, deforestation\n",
      "\n",
      "there, pakistan]\n",
      "   ‚úÖ lec15_(a)_main_sectors_of_Pakistan's_economy.txt: 377 tokens, 284 words [pakistan, natural, key]\n",
      "   ‚úÖ lec15_(b)_significant_tourism_potential.txt: 452 tokens, 319 words [ushu forest, kalam, infrastructure]\n",
      "   ‚úÖ lec15_(c)_Foreign_Policy_Post_911.txt: 550 tokens, 411 words [war, post, pakistan]\n",
      "   ‚úÖ lec15_(d)_Pakistan_Regional_Role.txt: 514 tokens, 389 words [south asian association, pakistan, successfully]\n",
      "   ‚úÖ lec16_(a)_Cultural_Diversity_of_Pakistan.txt: 1937 tokens, 1441 words [lec16, faiz ahmad faiz, ajaz anwar]\n",
      "   ‚úÖ lec1_(a)_Bitter_Experiences_of_Muslim_under_British_rule.txt: 307 tokens, 263 words [rule, war, the muslim]\n",
      "   ‚úÖ lec1_(b)_Partition_of_Bengal.txt: 211 tokens, 183 words [orissa, muslim, viceroy]\n",
      "   ‚úÖ lec1_(c)_Muslim_League.txt: 114 tokens, 90 words [nawab salimullah khan, india, india muslim league]\n",
      "   ‚úÖ lec1_(d)_Annulment_of_Bengal.txt: 145 tokens, 118 words [his majesty george v, indian affairs, delhi]\n",
      "   ‚úÖ lec1_(e)_Lucknow_Pact.txt: 96 tokens, 82 words [muslim league, indian national congress, india]\n",
      "   ‚úÖ lec1_(f)_Nehru_Report.txt: 301 tokens, 265 words [report, indian affairs, delhi]\n",
      "   ‚úÖ lec1_(g)_Nehru_Report_Points.txt: 246 tokens, 213 words [nehru report demands, judiciary, report]\n",
      "   ‚úÖ lec1_(h)_Nehru_Report_Response.txt: 296 tokens, 254 words [report, balochistan, muslim]\n",
      "   ‚úÖ lec1_(i)_Nehru_Report_Reveiw.txt: 287 tokens, 257 words [reveiw, congress, report]\n",
      "   ‚úÖ lec2_(a)_Simon_Commission.txt: 179 tokens, 165 words [muslim, simon, therefore]\n",
      "   ‚úÖ lec2_(b)_Round_Table_Conference.txt: 368 tokens, 312 words [indian national congress, march, muslim]\n",
      "   ‚úÖ lec2_(c)_First_Round_Table_Conference.txt: 253 tokens, 200 words [on january, maulana muhammad ali jauhar, provinces]\n",
      "   ‚úÖ lec2_(d)_GANDHI_IRWIN_PACT.txt: 85 tokens, 66 words [congress, irwin pact\n",
      "\n",
      "on january, delhi]\n",
      "   ‚úÖ lec2_(e)_Second_Round_Table_Conference.txt: 175 tokens, 145 words [lord willingdon, congress, lord irwin]\n",
      "   ‚úÖ lec2_(f)_Failure_of_Second_Round_Table_Conference.txt: 154 tokens, 135 words [the hindu, august, muslims]\n",
      "   ‚úÖ lec2_(g)_Third_Round_Table_Conference.txt: 97 tokens, 85 words [india act, indian national congress, british parliament]\n",
      "   ‚úÖ lec2_(h)_Conclusion_of_Table_Conferences.txt: 277 tokens, 241 words [congress, viceroy, first round table]\n",
      "   ‚úÖ lec2_(i)_All_India_Muslim_League.txt: 207 tokens, 180 words [india act, indian national congress, muslim]\n",
      "   ‚úÖ lec2_(j)_Election_of_1937.txt: 410 tokens, 341 words [india act, congress, muslim]\n",
      "   ‚úÖ lec2_(k)_Congress_and_Hindus_Tyranny_in_1937_39.txt: 131 tokens, 110 words [these, united india, congress]\n",
      "   ‚úÖ lec2_(l)_Lahore_Resolution_1940.txt: 165 tokens, 142 words [march, the all india muslim league resolution, muslim]\n",
      "   ‚úÖ lec2_(m)_3rd_June_Plan.txt: 306 tokens, 269 words [india act, congress, azam]\n",
      "   ‚úÖ lec2_(n)_Radcliff_Award.txt: 187 tokens, 156 words [radcliffe, muslim, gurdaspur]\n",
      "   ‚úÖ lec3_(a)_MINORITY_RELIGION.txt: 26 tokens, 22 words [what is minority religion, religion, lec3]\n",
      "   ‚úÖ lec3_(b)_PAKISTAN_AND_MINORITIES.txt: 99 tokens, 90 words [state, pakistan, pakistan and minorities\n",
      "\n",
      "pakistan]\n",
      "   ‚úÖ lec3_(c)_CHRISTIANS_IN_STRUGGLE_FOR_PAKISTAN.txt: 493 tokens, 429 words [struggle, congress, pakistan]\n",
      "   ‚úÖ lec3_(d)_Diwan_Bahadur_Singha.txt: 109 tokens, 84 words [christian, diwan, british india]\n",
      "   ‚úÖ lec3_(e)_QUAID_E_AZAM_AND_MINORITIES.txt: 756 tokens, 608 words [march, muslim, therefore]\n",
      "   ‚úÖ lec3_(f)_CONSTITUTION_AND_RIGHTS_FOR_MINORITIES.txt: 476 tokens, 394 words [muslim, pakistan, provincial assemblies]\n",
      "   ‚úÖ lec3_(g)_MINORITIES_AND_PRESENT_PAKISTAN.txt: 326 tokens, 254 words [judiciary, dr kelash garvada, muslim]\n",
      "   ‚úÖ lec3_(h)_Minority_Representation.txt: 219 tokens, 181 words [representation, march, minorities representation in senate of pakistan\n",
      "\n",
      "minorities representation in national assembly\n",
      "\n",
      "minorities representation in provincial assemblies of pakistan\n",
      "\n",
      "the]\n",
      "   ‚úÖ lec4_(a)_Pakistan.txt: 190 tokens, 138 words [lec4, islamabad, muslim]\n",
      "   ‚úÖ lec4_(b)_Pakistan_Religion.txt: 143 tokens, 115 words [zoroastrians, lec4, the muslim]\n",
      "   ‚úÖ lec4_(c)_Components_of_Pakistan_Ideology.txt: 17 tokens, 16 words [lec4, pakistan, fundamental human rights]\n",
      "   ‚úÖ lec4_(d)_Pakistan_Neighboring_Countries_&_Culture.txt: 186 tokens, 154 words [lec4, culture, balochistan]\n",
      "   ‚úÖ lec4_(e)_Census_of_Pakistan.txt: 66 tokens, 60 words [lec4, pakistani, march]\n",
      "   ‚úÖ lec4_(f)_Languages_in_Pakistan.txt: 320 tokens, 213 words [lec4, others, balochistan]\n",
      "   ‚úÖ lec4_(g)_Pakistan_sea_ports.txt: 148 tokens, 108 words [lec4, ports, glaciers]\n",
      "   ‚úÖ lec4_(h)_Deserts_of_Pakistan.txt: 40 tokens, 34 words [lec4, the, pakistan]\n",
      "   ‚úÖ lec4_(i)_Rivers_of_Pakistan.txt: 401 tokens, 313 words [lec4, specific descriptions\n",
      "\n",
      "indus river, river ravi]\n",
      "   ‚úÖ lec4_(j)_Pakistan_Rivers_&_Glaciers.txt: 371 tokens, 276 words [lec4, sindh, glaciers\n",
      "\n",
      "a]\n",
      "   ‚úÖ lec5_(a)_Constitutional_Development_in_Pakistan.txt: 263 tokens, 236 words [india act, pakistan, british parliament]\n",
      "   ‚úÖ lec5_(b)_First_Constituent_Assembly.txt: 86 tokens, 66 words [september, lec5, the]\n",
      "   ‚úÖ lec5_(c)_Objectives_Resolution.txt: 256 tokens, 216 words [march, pakistan, about]\n",
      "   ‚úÖ lec5_(d)_Objective_Resolution_Salient_features.txt: 195 tokens, 182 words [supreme authority, features, people\n",
      "the]\n",
      "   ‚úÖ lec5_(e)_Constitution_of_1956.txt: 606 tokens, 513 words [general iskander mirza, march, muslim]\n",
      "   ‚úÖ lec5_(f)_Constitution_of_1962.txt: 857 tokens, 761 words [west pakistan, east, pakistan]\n",
      "   ‚úÖ lec5_(g)_Constitution_of_1973.txt: 475 tokens, 399 words [muslim, sindh, laws]\n",
      "   ‚úÖ lec5_(h)_Constitution_of_1973_Amendments.txt: 655 tokens, 554 words [east pakistan, eighth amendment, prime]\n",
      "   ‚úÖ lec6_(a)_Political_History_&_policy_of_Pakistan.txt: 150 tokens, 117 words [pakistan, interim constitution, major policies \n",
      "\n",
      "the]\n",
      "   ‚úÖ lec6_(b)_Labour_Policy.txt: 37 tokens, 35 words [health policy, labour policy, lec6]\n",
      "   ‚úÖ lec6_(c)_Administrative_Reforms.txt: 234 tokens, 187 words [rule, problems, hundreds]\n",
      "   ‚úÖ lec6_(d)_Major_Policies.txt: 689 tokens, 508 words [benazir bhutto october, army chief, then]\n",
      "   ‚úÖ lec6_(e)_General_Musharraf__Policies.txt: 183 tokens, 141 words [held general elections, poverty reduction, democracy]\n",
      "   ‚úÖ lec7_(a)_Main_Sector_of_the_Economy.txt: 170 tokens, 148 words [main, economy, foreign]\n",
      "   ‚úÖ lec7_(b)_backwardness_in_agriculture.txt: 337 tokens, 293 words [they, their, our]\n",
      "   ‚úÖ lec7_(c)_Natural_Resources___Minerals.txt: 629 tokens, 490 words [resources, khuzdar, chaghi]\n",
      "   ‚úÖ lec7_(d)_Natural_Resources___Rivers.txt: 546 tokens, 433 words [specific descriptions\n",
      "\n",
      "indus river, river ravi, resources]\n",
      "   ‚úÖ lec7_(e)_lakes_&_Glaciers.txt: 370 tokens, 276 words [sindh, glaciers\n",
      "\n",
      "a, pakistan]\n",
      "   ‚úÖ lec7_(f)_Forest,Animal,FishesCrops.txt: 395 tokens, 309 words [deforestation, tobacco, pakistan]\n",
      "   ‚úÖ lec7_(g)_Industrial_Development.txt: 459 tokens, 351 words [east pakistan, major industries\n",
      "\n",
      "textiles, defence industry]\n",
      "   ‚úÖ lec7_(h)_Imports_&_Exports_in_Pakistan.txt: 338 tokens, 219 words [s exports ‚Äì country, pakistan, increased]\n",
      "   ‚úÖ lec8_(a)_Tourism_&_Tourist_Potential_of_Pakistan.txt: 118 tokens, 80 words [pakistan, patriata, gilgit]\n",
      "   ‚úÖ lec8_(b)_Ecotourism.txt: 66 tokens, 49 words [responsible, the international ecotourism society, ecotourism\n",
      "\n",
      "according]\n",
      "   ‚úÖ lec8_(c)_Why_people_travel_to_Pakistan.txt: 71 tokens, 61 words [why, pakistan, people]\n",
      "   ‚úÖ lec8_(d)_Factors_involved_for_tourism_industry.txt: 51 tokens, 43 words [level, the, marketing]\n",
      "   ‚úÖ lec8_(e)_International_tourists_or_visitors.txt: 235 tokens, 172 words [pakistan, places, for]\n",
      "   ‚úÖ lec8_(f)_Attraction_for_tourists.txt: 1985 tokens, 1496 words [himalaya, kalam, pakistan]\n",
      "   ‚úÖ lec8_(g)_Challenges_faced_by_Pakistan's_Tourism_Industry.txt: 84 tokens, 67 words [pakistan, faced, underdeveloped]\n",
      "   ‚úÖ lec8_(h)_How_to_increase_tourism.txt: 411 tokens, 285 words [conclusion\n",
      "\n",
      "the, how, pakistan]\n",
      "   ‚úÖ lec9_(a)_Pakistan_in_Global_Affairs.txt: 124 tokens, 106 words [the, there, foreign policy]\n",
      "   ‚úÖ lec9_(b)_Pakistan_and_US_Relations_and_War_on_Terror.txt: 529 tokens, 416 words [war, pressler, post]\n",
      "   ‚úÖ lec9_(c)_Steps_Taken_By_Pakistan_against_Terrorism.txt: 369 tokens, 234 words [location, balochistan, pakistan armed forces]\n",
      "   ‚úÖ lec9_(d)_Pakistan_Role_in_Global_Peace.txt: 146 tokens, 129 words [peace, global peace\n",
      "\n",
      "to control terrorism pakistan, taliban]\n",
      "   ‚úÖ lec9_(e)_India_and_Pakistan_Relations.txt: 385 tokens, 319 words [pakistan, south asia, the maharaja]\n",
      "   ‚úÖ lec9_(f)_Kashmir_Issue.txt: 616 tokens, 524 words [issue, pakistani war, kashmiri muslims]\n",
      "   ‚úÖ lec9_(g)_Significance_of_Kashmir_Issue.txt: 291 tokens, 242 words [issue, indus water treaty, pakistan]\n",
      "   ‚úÖ lec9_(h)_Pulwama_Attack.txt: 94 tokens, 73 words [india, the, pakistan]\n",
      "   ‚úÖ lec9_(i)_Abrogation_of_Article_370_&_impact.txt: 331 tokens, 264 words [on august, tension, impact\n",
      "\n",
      "the]\n",
      "   üìä Summary: 120 files processed\n",
      "   üè∑Ô∏è  Keywords: 1946 unique, avg 29.5 per document\n",
      "   üìù Content Summary (based on flags): 110 technical, 0 structured, 17 with definitions\n",
      "\n",
      "üìÅ Processing: Sealed Nectar (Sealed_nectar_text_files)\n",
      "   ‚úÖ part_001_Introduction_and_Arab_Tribes_Overview.txt: 367 tokens, 306 words [location, iraq, part]\n",
      "   ‚úÖ part_002_Arab_Tribes_Qahtanian_and_Adnanian_Origins.txt: 271 tokens, 178 words [ishmael, origins, jamhur]\n",
      "   ‚úÖ part_003_Kahlan_Migrations_and_Ishmaels_Family.txt: 769 tokens, 527 words [ishmael, mar az, aja]\n",
      "   ‚úÖ part_004_Abraham_Ishmael_and_Kaabah_Foundations.txt: 765 tokens, 550 words [ishmael, will, abbas]\n",
      "   ‚úÖ part_005_Adnanian_Arabs_and_Prophetic_Lineage.txt: 814 tokens, 476 words [tareekh ard al, ishmael, anazah]\n",
      "   ‚úÖ part_006_Dispersion_of_Adnanide_Tribes.txt: 344 tokens, 213 words [yamama, autas, iraq]\n",
      "   ‚úÖ part_007_Rulership_in_Pre_Islamic_Arabia_and_Yemen_Introduction.txt: 219 tokens, 174 words [yemen \n",
      "the, arabs, part]\n",
      "   ‚úÖ part_008_Rulership_in_Yemen_Sheba_Decline_and_Foreign_Invasions.txt: 483 tokens, 341 words [nabetean, ethiopian, noble qur]\n",
      "   ‚úÖ part_009_Rulership_in_Yemen_Abyssinian_and_Persian_Rule.txt: 315 tokens, 211 words [rule, ethiopian, arabs]\n",
      "   ‚úÖ part_010_Rulership_in_Heerah_Lakhmi_Kings.txt: 512 tokens, 369 words [adnanians, nobody, ardashir]\n",
      "   ‚úÖ part_011_Rulership_in_Heerah_End_of_Lakhmi_Rule_and_Dhee_Qar.txt: 425 tokens, 283 words [rule, because, dhee qar]\n",
      "   ‚úÖ part_012_Rulership_in_Geographical_Syria_and_Hijaz_Pre_Qusai.txt: 377 tokens, 239 words [tareekh ard al, 012, umar]\n",
      "   ‚úÖ part_013_Adnanide_Resurgence_and_Khuzaa_Reign.txt: 729 tokens, 450 words [ishmael, ghawth, black stone]\n",
      "   ‚úÖ part_014_Qusai_bin_Kilab_and_Quraish_Supremacy.txt: 569 tokens, 363 words [bani bakr, ishmael, war]\n",
      "   ‚úÖ part_015_Qusais_Reign_and_Makkah_Administration.txt: 780 tokens, 518 words [presiding, arabs, part]\n",
      "   ‚úÖ part_016_Pre_Islamic_Arab_Governance_Structure_and_Offices.txt: 204 tokens, 140 words [governance, abbas, part]\n",
      "   ‚úÖ part_017_Rulership_in_Pan_Arabia_and_Political_Situation_Overview.txt: 597 tokens, 464 words [pan, heerah, part]\n",
      "   ‚úÖ part_018_Religions_of_the_Arabs_Introduction_and_Polytheism.txt: 263 tokens, 197 words [ishmael, arabs, part]\n",
      "   ‚úÖ part_019_Idol_Worship_Practices_and_Sacrifices.txt: 244 tokens, 155 words [traditions, part, 019]\n",
      "   ‚úÖ part_020_Idol_Worship_Practices_Consecrations_and_Votive_Offerings.txt: 770 tokens, 542 words [arabs, part, currying favours with these idols through votive of]\n",
      "   ‚úÖ part_021_Divination_Soothsayers_and_Omens.txt: 689 tokens, 468 words [superstition, 021, muslim]\n",
      "   ‚úÖ part_022_Abrahamic_Traditions_and_Pre_Islamic_Innovations.txt: 241 tokens, 159 words [arafah, then, traditions]\n",
      "   ‚úÖ part_023_Pre_Islamic_Pilgrimage_Practices_and_Religious_Overview.txt: 319 tokens, 204 words [birr, part, such]\n",
      "   ‚úÖ part_024_Judaism_in_Arabia_and_Najran_Massacre.txt: 377 tokens, 269 words [bani quraizah, khabeer, part]\n",
      "   ‚úÖ part_025_Christianity_Magianism_Sabianism_and_Religious_Summary.txt: 687 tokens, 501 words [tareekh ard al, ethiopian, iraq]\n",
      "   ‚úÖ part_026_Aspects_of_Pre_Islamic_Arabian_Society_Social_Life.txt: 741 tokens, 611 words [aspects, the pr, part]\n",
      "   ‚úÖ part_027_Pre_Islamic_Arabian_Society_Offspring_and_Tribalism.txt: 221 tokens, 172 words [pre, offspring, another]\n",
      "   ‚úÖ part_028_Inter_Tribal_Relations_and_Social_Summary.txt: 295 tokens, 201 words [028, arabs, part]\n",
      "   ‚úÖ part_029_Economic_Situation_and_Introduction_to_Ethics.txt: 260 tokens, 204 words [admittedly, arabs, part]\n",
      "   ‚úÖ part_030_Ethics_Hospitality_Wine_Gambling_and_Covenant.txt: 156 tokens, 130 words [covenant, gambling, they]\n",
      "   ‚úÖ part_031_Ethics_Sense_of_Honour_Will_Forbearance_and_Conclusion.txt: 411 tokens, 311 words [pure, will, arabs]\n",
      "   ‚úÖ part_032_Lineage_of_Prophet_Muhammad_Overview.txt: 537 tokens, 295 words [khuzaiman, ishmael, add]\n",
      "   ‚úÖ part_033_The_Prophetic_Family_Hashim.txt: 373 tokens, 246 words [in \n",
      "madinah, madinah, part]\n",
      "   ‚úÖ part_034_The_Prophetic_Family_Abdul_Muttalib_Early_Life.txt: 738 tokens, 460 words [part, bani an, abd munaf]\n",
      "   ‚úÖ part_035_The_Prophetic_Family_Abdul_Muttalib_Zamzam_and_Elephant_Incident.txt: 774 tokens, 548 words [whenever, the quraishites, tafheemul]\n",
      "   ‚úÖ part_036_Abdul_Muttalib_Elephant_Aftermath_and_Abdullah_Biography.txt: 1066 tokens, 697 words [aftermath, ishmael, safar]\n",
      "   ‚úÖ part_037_Muhammad_Birth_and_Babyhood.txt: 828 tokens, 452 words [uzza, abu sufyan, muhadarat tareekh al]\n",
      "   ‚úÖ part_038_Babyhood_with_Haleemah_Full_Narrative.txt: 892 tokens, 705 words [messenger, traditions, part]\n",
      "   ‚úÖ part_039_Orphanhood_and_Abu_Talibs_Care.txt: 700 tokens, 488 words [arfuta, muslim, talqeeh furoom ahl]\n",
      "   ‚úÖ part_040_Bahira_the_Monk_and_Sacrilegious_Wars.txt: 452 tokens, 321 words [messenger, abu talib, how]\n",
      "   ‚úÖ part_041_Al_Fudoul_Confederacy_and_Early_Jobs.txt: 630 tokens, 426 words [uzza, fudoul, part]\n",
      "   ‚úÖ part_042_Marriage_to_Khadijah_and_Family.txt: 347 tokens, 228 words [nafisa, messenger, khadijah\n",
      "when]\n",
      "   ‚úÖ part_043_Rebuilding_Al_Kaabah_and_Arbitration_Issue.txt: 808 tokens, 601 words [issue, messenger, ishmael]\n",
      "   ‚úÖ part_044_Rapid_Review_of_Pre_Prophethood_Biography_Character_and_Al_Ameen.txt: 795 tokens, 548 words [rapid review, the building, uzza]\n",
      "   ‚úÖ part_045_Prophet_Muhammad_Pre_Revelation_Meditation_and_First_Encounter_with_Gabriel.txt: 313 tokens, 231 words [nour, part, sawiq]\n",
      "   ‚úÖ part_046_Interruption_and_Resumption_of_Revelation_and_First_Verses_of_Al_Muddathir.txt: 603 tokens, 438 words [whenever, messenger, verses]\n",
      "   ‚úÖ part_047_Successive_Stages_of_Revelation_to_the_Prophet.txt: 591 tokens, 441 words [qayyim, messenger, the fifth]\n",
      "   ‚úÖ part_048_Proclaiming_Allah_and_Immediate_Constituents_of_the_Message.txt: 897 tokens, 684 words [proclaiming all, noble qur, part]\n",
      "   ‚úÖ part_049_Core_Constituents_of_the_Call_and_The_Secret_Phase_of_Dawah.txt: 2313 tokens, 1605 words [may \n",
      "all, messengership, abi al]\n",
      "   ‚úÖ part_050_Second_Phase_Open_Preaching_and_Calling_Closest_Kinspeople.txt: 760 tokens, 565 words [messenger, chronologically, abu talib]\n",
      "   ‚úÖ part_051_Public_Call_on_Mount_Safa_and_Quraish_Reaction.txt: 705 tokens, 454 words [perish, messenger, public]\n",
      "   ‚úÖ part_052_Open_Proclamation_of_Truth_and_Quraish_Outrage.txt: 715 tokens, 525 words [052, messenger, reaction \n",
      "the prophet]\n",
      "   ‚úÖ part_053_Quraish_Advisory_Council_to_Debar_Pilgrims_and_Allegations.txt: 649 tokens, 476 words [others, advisory, then]\n",
      "   ‚úÖ part_054_Quraish_Attempts_to_Halt_Islam_Part_1_Scoffing_and_Distortion.txt: 1257 tokens, 816 words [does, whenever, messenger]\n",
      "   ‚úÖ part_055_Quraish_Attempts_to_Halt_Islam_Part_2_Compromise_and_Persecutions.txt: 885 tokens, 573 words [messenger, compromise, should]\n",
      "   ‚úÖ part_056_Severe_Persecutions_of_Prophet_Muhammad_by_Abu_Lahab_and_Quraish.txt: 814 tokens, 544 words [ruqaiya, abu sufyan, noble qur]\n",
      "   ‚úÖ part_057_Continued_Persecutions_Against_Prophet_and_Divine_Protection.txt: 783 tokens, 482 words [messenger, then, continued]\n",
      "   ‚úÖ part_058_Tortures_of_the_Weak_Converts_and_Abu_Bakrs_Role.txt: 927 tokens, 621 words [sometimes, umar, uzza]\n",
      "   ‚úÖ part_059_House_of_Al_Arqam_and_First_Migration_to_Abyssinia.txt: 1034 tokens, 774 words [the crowds, part, arqam \n",
      "in]\n",
      "   ‚úÖ part_060_Quraish_Reaction_to_Surah_An_Najm_and_Second_Migration.txt: 949 tokens, 687 words [suddenly, messenger, listen]\n",
      "   ‚úÖ part_061_Quraishs_Machination_Against_Emigrants_in_Abyssinia_and_Jafars_Defense.txt: 948 tokens, 732 words [the muslim, muslim, then]\n",
      "   ‚úÖ part_062_Abu_Talibs_Unwavering_Support_and_Quraishs_Threats.txt: 442 tokens, 340 words [messenger, come, abu talib]\n",
      "   ‚úÖ part_063_Tyrants_Decision_to_Kill_Prophet_and_Divine_Protection_Incidents.txt: 825 tokens, 548 words [messenger, kill, abul qasim]\n",
      "   ‚úÖ part_064_Conversion_of_Hamzah_bin_Abdul_Muttalib.txt: 531 tokens, 340 words [bin, part, muttalib \n",
      "in]\n",
      "   ‚úÖ part_065_Conversion_of_Umar_bin_Al_Khattab_Part_1_Initial_Struggle.txt: 797 tokens, 554 words [struggle, initial, umar]\n",
      "   ‚úÖ part_066_Conversion_of_Umar_bin_Al_Khattab_Part_2_Sisters_House_and_Shahada.txt: 841 tokens, 555 words [nay,, umar, messenger]\n",
      "   ‚úÖ part_067_Impact_of_Umars_Conversion_and_Quraishs_Shift_in_Strategy.txt: 899 tokens, 594 words [nobody, shift, umar]\n",
      "   ‚úÖ part_068_Quraishs_Diplomatic_Overture_to_the_Prophet_Utbahs_Negotiation.txt: 1070 tokens, 759 words [well abu al, giving, messenger]\n",
      "   ‚úÖ part_069_Abu_Talib_Assembles_Bani_Hashim_and_The_General_Social_Boycott.txt: 503 tokens, 336 words [umar, banu hashim, abu talib]\n",
      "   ‚úÖ part_070_The_Boycott_in_Shib_Abi_Talib_and_Breaking_of_the_Pact.txt: 1164 tokens, 849 words [whenever, 070, banu hashim]\n",
      "   ‚úÖ part_071_Final_Diplomacy_and_Negotiation_Between_Quraish_and_Prophet.txt: 1129 tokens, 832 words [umar, abu sufyan, he \n",
      "said,]\n",
      "   ‚úÖ part_072_The_Year_of_Grief_Abu_Talibs_Death.txt: 703 tokens, 469 words [messenger, the prophet [pbuh] said:, abbas]\n",
      "   ‚úÖ part_073_The_Year_of_Grief_Khadijahs_Passing.txt: 747 tokens, 503 words [messenger, passing, abu talib]\n",
      "   ‚úÖ part_074_Marriage_to_Sawdah.txt: 179 tokens, 126 words [part, khadijah, ethiopia]\n",
      "   ‚úÖ part_075_The_Third_Phase_Calling_unto_Islam_beyond_Makkah_At_Taif.txt: 714 tokens, 532 words [messenger, o \n",
      "lord, should]\n",
      "   ‚úÖ part_076_Encounter_with_Addas_and_Return_from_Taif.txt: 723 tokens, 486 words [muslim, part, akhshabain]\n",
      "   ‚úÖ part_077_Jinns_Listen_to_the_Noble_Qur√¢n.txt: 359 tokens, 220 words [recital, listen, noble qur]\n",
      "   ‚úÖ part_078_Divine_Support_and_Prophets_Return_to_Makkah.txt: 683 tokens, 472 words [whence, how, part]\n",
      "   ‚úÖ part_079_Islam_Introduced_to_Arabian_Tribes_and_Individuals.txt: 577 tokens, 378 words [firras, fazarah, should]\n",
      "   ‚úÖ part_080_Early_Individual_Converts_Swaid_bin_Samit_and_Eyas_bin_Muadh.txt: 455 tokens, 322 words [bin, noble qur, part]\n",
      "   ‚úÖ part_081_Early_Individual_Converts_Abu_Dhar_Al_Ghifari.txt: 273 tokens, 213 words [abbas, 081, part]\n",
      "   ‚úÖ part_082_Early_Individual_Converts_Tufail_bin_Amr_Ad_Dausi.txt: 428 tokens, 336 words [tufail, bin, noble qur]\n",
      "   ‚úÖ part_083_Early_Individual_Converts_Dhumad_Al_Azdi.txt: 167 tokens, 112 words [messenger, part, prophet]\n",
      "   ‚úÖ part_084_Hope_Inspiring_Breezes_from_the_Madinese.txt: 673 tokens, 475 words [yes., yes, then]\n",
      "   ‚úÖ part_085_Marriage_of_the_Prophet_to_Aishah.txt: 84 tokens, 49 words [marriage, part, prophet]\n",
      "   ‚úÖ part_086_Al_Isra_and_Al_Miraj_Night_Journey_and_Ascent_to_Heavens.txt: 1014 tokens, 754 words [journey, then, ascent]\n",
      "   ‚úÖ part_087_Al_Isra_and_Al_Miraj_Significant_Incidents_and_Public_Reaction.txt: 965 tokens, 752 words [public, yes, abbas]\n",
      "   ‚úÖ part_088_Al_Isra_and_Al_Miraj_Divine_Justification_and_Transfer_of_Authority.txt: 955 tokens, 732 words [journey, messenger, muslim]\n",
      "   ‚úÖ part_089_The_First_Aqabah_Pledge.txt: 340 tokens, 207 words [abbas, part, dhakwan]\n",
      "   ‚úÖ part_090_The_Muslim_Envoy_in_Madinah_Musab_bin_Umairs_Mission.txt: 820 tokens, 590 words [090, hudair, musab]\n",
      "   ‚úÖ part_091_The_Second_Aqabah_Pledge_Preparation_and_Initial_Meeting.txt: 602 tokens, 440 words [initial, messenger, abbas]\n",
      "   ‚úÖ part_092_The_Second_Aqabah_Pledge_Articles_of_Allegiance.txt: 491 tokens, 353 words [092, messenger, then]\n",
      "   ‚úÖ part_093_The_Second_Aqabah_Pledge_Ratification_and_Deputies.txt: 838 tokens, 608 words [hudair, messenger, should]\n",
      "   ‚úÖ part_094_The_Second_Aqabah_Pledge_Quraish_Reaction_and_Significance.txt: 765 tokens, 550 words [abbas, part, his \n",
      "messenger]\n",
      "   ‚úÖ part_095_The_Vanguard_of_Migration_Early_Challenges_and_Abu_Salamah.txt: 365 tokens, 282 words [umm salamah, muslim, part]\n",
      "   ‚úÖ part_096_The_Vanguard_of_Migration_Suhaib_and_Umar_bin_Al_Khattab.txt: 516 tokens, 380 words [umar, bin, then]\n",
      "   ‚úÖ part_097_The_Vanguard_of_Migration_Prophets_Preparations_for_Migration.txt: 305 tokens, 225 words [command, madinah, part]\n",
      "   ‚úÖ part_098_In_An_Nadwah_Council_House_Quraish_Plotting.txt: 683 tokens, 436 words [safar, abu sufyan, uzza]\n",
      "   ‚úÖ part_099_In_An_Nadwah_Council_House_Assassination_Plot_Against_Prophet.txt: 199 tokens, 164 words [expulsion, arabs, part]\n",
      "   ‚úÖ part_100_Migration_of_the_Prophet_Departure_and_Hiding_in_Cave_Thawr.txt: 783 tokens, 534 words [gardens, departure, command]\n",
      "   ‚úÖ part_101_Migration_of_the_Prophet_Quraish_Search_and_Divine_Protection.txt: 922 tokens, 646 words [safar, suddenly abu bakr, madinah]\n",
      "   ‚úÖ part_102_Migration_of_the_Prophet_Journey_from_Cave_and_Suraqahs_Conversion.txt: 962 tokens, 708 words [journey, then, malik]\n",
      "   ‚úÖ part_103_Migration_of_the_Prophet_Umm_Mabad_Al_Khuzaiyah_Incident.txt: 515 tokens, 397 words [khuzaiyah, madinah, part]\n",
      "   ‚úÖ part_104_Migration_of_the_Prophet_Abu_Buraidah_and_Az_Zubair_Encounters.txt: 158 tokens, 112 words [rahmat, islam, buraidah]\n",
      "   ‚úÖ part_105_Migration_of_the_Prophet_Arrival_at_Quba_and_Mosque_Foundation.txt: 365 tokens, 229 words [qayyim, messenger, maula]\n",
      "   ‚úÖ part_106_Migration_of_the_Prophet_Entry_into_Madinah_and_First_Friday_Prayer.txt: 404 tokens, 292 words [messenger, banu najjar, city]\n",
      "   ‚úÖ part_107_Migration_of_the_Prophet_Family_Arrives_and_Prayer_for_Madinah.txt: 239 tokens, 147 words [messenger, umm aiman, abi al]\n",
      "   ‚úÖ part_108_Life_in_Madinah_Three_Phases_of_the_Madinese_Era.txt: 127 tokens, 92 words [hudaibiyah peace \n",
      "treaty, arabia, islam]\n",
      "   ‚úÖ part_109_Life_in_Madinah_Status_Quo_Companions_Muhajirun_Ansar.txt: 940 tokens, 669 words [109, messenger, verses]\n",
      "   ‚úÖ part_110_Life_in_Madinah_Status_Quo_Madinese_Polytheists_and_Hypocrites.txt: 247 tokens, 181 words [war, muslim, islamically]\n",
      "   ‚úÖ part_111_Life_in_Madinah_Status_Quo_The_Jews_Part_1_Characteristics.txt: 421 tokens, 316 words [whenever, arabs, part]\n",
      "   ‚úÖ part_112_Life_in_Madinah_Status_Quo_The_Jews_Part_2_Enmity_and_Incidents.txt: 831 tokens, 600 words [abu yasir, whenever, nadir]\n",
      "   ‚úÖ part_113_Life_in_Madinah_Status_Quo_Quraish_Threat_and_Prophets_Leadership.txt: 417 tokens, 302 words [messengership, arabs, part]\n",
      "   ‚úÖ part_114_A_New_Society_Built_Construction_of_the_Prophets_Mosque.txt: 424 tokens, 307 words [messenger, muslim, 114]\n",
      "   ‚úÖ part_115_A_New_Society_Built_Brotherhood_between_Muhajirun_and_Ansar.txt: 160 tokens, 115 words [malik, part, prophet]\n",
      "   ‚úÖ part_116_A_New_Society_Built_Spirit_of_Brotherhood_and_Generosity.txt: 475 tokens, 342 words [madinah, part, iddah]\n",
      "   ‚úÖ part_117_A_Charter_of_Islamic_Alliance_Introduction_and_Provisions_Part_1.txt: 350 tokens, 270 words [the emigrants, messenger, muslim]\n",
      "   ‚úÖ part_118_A_Charter_of_Islamic_Alliance_Provisions_Part_2_and_Conclusion.txt: 247 tokens, 190 words [whenever, part, 118]\n",
      "   ‚úÖ part_119_Prophets_Teachings_on_Muslim_Conduct_Part_1.txt: 340 tokens, 217 words [messenger, the muslim, muslim]\n",
      "   ‚úÖ part_120_Prophets_Teachings_on_Muslim_Conduct_Part_2.txt: 202 tokens, 117 words [muslim, part, and said:]\n",
      "   ‚úÖ part_121_Brotherhood_Abstention_and_Model_Companions.txt: 684 tokens, 528 words [messenger, muslim, therefore]\n",
      "   ‚úÖ part_122_Cooperation_and_Non_Aggression_Pact_with_Jews.txt: 452 tokens, 355 words [geographically, the signatories to this treaty shall boycott qurais, should]\n",
      "   ‚úÖ part_123_Prophet_on_the_Battlefield_and_Permission_to_Fight.txt: 840 tokens, 563 words [skirmishes, messenger, were]\n",
      "   ‚úÖ part_124_Pre_Badr_Military_Strategy_and_Divine_Command.txt: 576 tokens, 380 words [muslim, command, military]\n",
      "   ‚úÖ part_125_Pre_Badr_Missions_and_Early_Invasions_Part_1.txt: 859 tokens, 573 words [kinaz, messenger, bahr platoon]\n",
      "   ‚úÖ part_126_Pre_Badr_Missions_and_Early_Invasions_Part_2.txt: 1198 tokens, 791 words [126, hadrami, before]\n",
      "   ‚úÖ part_127_Transition_to_War_and_Divine_Injunctions.txt: 487 tokens, 331 words [war, 127, part]\n",
      "   ‚úÖ part_128_The_Battle_of_Badr_Reasons_and_Muslim_Preparation.txt: 1471 tokens, 1042 words [the muslim, the battle, hell]\n",
      "   ‚úÖ part_129_The_Battle_of_Badr_Quraish_Mobilization_and_Obstinacy.txt: 876 tokens, 625 words [banu bakr, soon, abu sufyan]\n",
      "   ‚úÖ part_130_The_Battle_of_Badr_Muslim_Consultation_and_Tactics.txt: 1445 tokens, 1083 words [admittedly, umar, 130]\n",
      "   ‚úÖ part_131_The_Battle_of_Badr_Prophets_Vigil_and_Battle_Lines.txt: 702 tokens, 522 words [the muslim, muslim, part]\n",
      "   ‚úÖ part_132_The_Battle_of_Badr_Initial_Clashes_and_Divine_Assistance.txt: 1269 tokens, 886 words [initial, should, part]\n",
      "   ‚úÖ part_133_The_Battle_of_Badr_Muslim_Triumph_and_Abu_Jahls_End.txt: 1436 tokens, 1046 words [uzza, jahls, who]\n",
      "   ‚úÖ part_134_The_Battle_of_Badr_Instances_of_Devotion_and_Ethics.txt: 1224 tokens, 840 words [the prop, on the day of badr, the sword of ‚Äòukashah bin mihsa, messenger]\n",
      "   ‚úÖ part_135_The_Battle_of_Badr_Outcome_and_Prophets_Address_to_Slain.txt: 330 tokens, 240 words [the prophet [pbuh] answered:, messenger, umar]\n",
      "   ‚úÖ part_136_Aftermath_of_Badr_Reactions_in_Makkah_and_Madinah.txt: 375 tokens, 277 words [aftermath, abu sufyan, muslim]\n",
      "   ‚úÖ part_137_Badr_Spoils_of_War_Legislation_and_Prisoner_Management.txt: 1943 tokens, 1355 words [war, who, hell]\n",
      "   ‚úÖ part_138_The_Battle_of_Badr_in_its_Qur_anic_Context.txt: 539 tokens, 414 words [messenger, part, eid]\n",
      "   ‚úÖ part_139_Post_Badr_Military_Landscape_and_Hostile_Parties.txt: 335 tokens, 244 words [muslim, post, arabs]\n",
      "   ‚úÖ part_140_Al_Kudr_Invasion_and_Plot_to_Assassinate_the_Prophet.txt: 945 tokens, 681 words [140, umar, uhud invasion]\n",
      "   ‚úÖ part_141_Invasion_of_Bani_Qainuqa_Jewish_Treachery_and_Expulsion.txt: 1499 tokens, 1077 words [messenger, muslim, islamically]\n",
      "   ‚úÖ part_142_As_Sawiq_Invasion_and_Dhi_Amr_Invasion.txt: 779 tokens, 541 words [bani nadeer, safar, muslim]\n",
      "   ‚úÖ part_143_Killing_of_Ka_b_bin_Al_Ashraf_Poet_and_Enemy_of_Islam.txt: 1323 tokens, 930 words [then, gharqad, who]\n",
      "   ‚úÖ part_144_Buhran_Invasion_and_Economic_Siege_on_Quraish.txt: 818 tokens, 569 words [thaniyah, messenger, ula]\n",
      "   ‚úÖ part_145_The_Battle_of_Uhud___Quraish_Preparations.txt: 282 tokens, 206 words [the battle, abu sufyan, part]\n",
      "   ‚úÖ part_146_Quraish_Financial_and_Recruitment_Strategies.txt: 407 tokens, 287 words [146, abu sufyan, then]\n",
      "   ‚úÖ part_147_Madinah_on_Alert_and_Makkan_Advance.txt: 371 tokens, 258 words [hudair, abbas, muslim]\n",
      "   ‚úÖ part_148_Consultation_Assembly_for_Defence_Plan.txt: 671 tokens, 526 words [messenger, muslim, will]\n",
      "   ‚úÖ part_149_Islamic_Army_Organization_and_Departure.txt: 493 tokens, 328 words [hudair, huda, umar]\n",
      "   ‚úÖ part_150_Rejection_of_Jewish_Allies.txt: 71 tokens, 53 words [they, jews, upon]\n",
      "   ‚úÖ part_151_Parading_and_Selection_of_the_Army.txt: 239 tokens, 144 words [umar, zaheer, part]\n",
      "   ‚úÖ part_154_Movement_to_Uhud_and_Prophets_Position.txt: 311 tokens, 218 words [messenger, abu khaithama said:, then]\n",
      "   ‚úÖ part_155_The_Defence_Plan___Archers_and_Flanks.txt: 839 tokens, 636 words [whether, messenger, muslim]\n",
      "   ‚úÖ part_156_Instilling_Bravery_and_Abu_Dujanas_Oath.txt: 355 tokens, 259 words [whenever, messenger, umar]\n",
      "   ‚úÖ part_157_Makkan_Army_Recruitment_and_Standard_Bearers.txt: 430 tokens, 316 words [addressing, should, abu sufyan]\n",
      "   ‚úÖ part_158_Quraish_Political_Manoeuvres_Before_Combat.txt: 93 tokens, 69 words [combat, political, abu sufyan]\n",
      "   ‚úÖ part_159_Abu_Amir_Al_Fasiqs_Treachery.txt: 355 tokens, 244 words [messenger, therefore, part]\n",
      "   ‚úÖ part_160_Quraishite_Women_Inciting_Men.txt: 160 tokens, 104 words [abu sufyan, part, 160]\n",
      "   ‚úÖ part_161_Initial_Combat_and_Fall_of_Standard_Bearers.txt: 957 tokens, 680 words [initial, asim, fall]\n",
      "   ‚úÖ part_162_Abu_Dujanas_Valour_and_Hinds_Incident.txt: 526 tokens, 409 words [messenger, then, part]\n",
      "   ‚úÖ part_163_Hamzahs_Gallantry_and_Assassination.txt: 673 tokens, 492 words [utba, come, will]\n",
      "   ‚úÖ part_164_Muslim_Resilience_and_Hanzala_Al_Ghaseel.txt: 339 tokens, 214 words [umar, muslim, sword]\n",
      "   ‚úÖ part_165_Archers_Crucial_Role_and_Fatal_Mistake.txt: 839 tokens, 628 words [messenger, war, muslim]\n",
      "   ‚úÖ part_166_Prophets_Danger_and_Muslim_Reaction.txt: 1701 tokens, 1208 words [yaman, abu sufyan, then]\n",
      "   ‚úÖ part_167_Prophets_Injury_and_Companions_Sacrifice.txt: 69 tokens, 46 words [messenger, companions, ibn sakan]\n",
      "   ‚úÖ part_168_Talha_Abu_Bakr_Abu_Ubaidahs_Protection.txt: 1378 tokens, 880 words [then, siddiq, who]\n",
      "   ‚úÖ part_169_Continued_Heroism_and_Prophets_Ascent.txt: 2004 tokens, 1394 words [spit it!, abu talhah, then]\n",
      "   ‚úÖ part_170_Ubai_bin_Khalafs_Death_and_Final_Attack.txt: 954 tokens, 646 words [messenger, umar, bin]\n",
      "   ‚úÖ part_171_Mutilation_of_Martyrs_and_Womens_Courage.txt: 473 tokens, 351 words [the muslim, muslim, some muslim women came to the battlefield when the]\n",
      "   ‚úÖ part_172_Prophets_Water_and_Uhud_Post_Battle_Actions.txt: 374 tokens, 250 words [messenger, shoot it, muslim]\n",
      "   ‚úÖ part_173_Abu_Sufyans_Boasting_and_Prophets_Reply.txt: 651 tokens, 402 words [umar, war, come]\n",
      "   ‚úÖ part_174_Tracing_Quraish_and_Discovery_of_Martyrs.txt: 631 tokens, 440 words [messenger, then, discovery]\n",
      "   ‚úÖ part_175_Qazman_and_Jewish_Fighters_Fate.txt: 337 tokens, 240 words [he \n",
      "is an inhabitant of fire., messenger, 175]\n",
      "   ‚úÖ part_176_Burial_of_the_Martyrs_and_Prophets_Grief.txt: 755 tokens, 526 words [jamuh, messenger, ask his wife]\n",
      "   ‚úÖ part_177_Prophets_Post_Uhud_Supplication_and_Return.txt: 441 tokens, 323 words [messenger, post, then]\n",
      "   ‚úÖ part_178_Womens_Devotion_on_Return_and_Casualties.txt: 831 tokens, 558 words [messenger, the prophet [pbuh] said:, then]\n",
      "   ‚úÖ part_179_Uhud_Outcome_Analysis.txt: 2472 tokens, 1806 words [for i, abu sufyan, therefore]\n",
      "   ‚úÖ part_180_Quranic_Observations_on_Uhud.txt: 380 tokens, 297 words [noble qur, part, observations]\n",
      "   ‚úÖ part_181_Lessons_and_Moralities_from_Uhud.txt: 508 tokens, 408 words [should, part, prophet]\n",
      "   ‚úÖ part_182_Post_Uhud_Military_Platoons_and_Missions_Introduction.txt: 361 tokens, 257 words [ula, safar, muslim]\n",
      "   ‚úÖ part_183_Abi_Salamah_Mission.txt: 164 tokens, 109 words [messenger, the muslim, banu asad]\n",
      "   ‚úÖ part_184_An_Errand_led_by_Abdullah_bin_Unais.txt: 178 tokens, 133 words [the muslim, bin, 184]\n",
      "   ‚úÖ part_185_The_Event_of_Ar_Raji.txt: 814 tokens, 575 words [asim, umar, count]\n",
      "   ‚úÖ part_186_Khubaibs_Martyrdom_and_Miracles.txt: 216 tokens, 158 words [asim, lord, zaid]\n",
      "   ‚úÖ part_187_The_Tragedy_of_Mauna_Well.txt: 1151 tokens, 771 words [therefore, then, bani an]\n",
      "   ‚úÖ part_188_Bani_An_Nadeer_Invasion___Jewish_Treachery.txt: 481 tokens, 359 words [bani kalb, umar, ashraf]\n",
      "   ‚úÖ part_189_Bani_An_Nadeer_Siege_and_Expulsion.txt: 1215 tokens, 854 words [bani nadeer, abi al, then]\n",
      "   ‚úÖ part_191_The_Invasion_of_Najd_and_Dhat_Ar_Riqa.txt: 513 tokens, 339 words [ula, muslim, madinah]\n",
      "   ‚úÖ part_192_The_Invasion_of_Badr_the_Second.txt: 484 tokens, 360 words [messenger, mar az, abu sufyan]\n",
      "   ‚úÖ part_193_The_Invasion_of_Doumat_Al_Jandal.txt: 547 tokens, 395 words [messenger, part, with]\n",
      "   ‚úÖ part_194_The_Confederates_Invasion_Preparation_and_Trench_Digging.txt: 1278 tokens, 963 words [geographical \n",
      "syria, digging, messenger]\n",
      "   ‚úÖ part_195_The_Trench_Battle_Confrontation_and_Jewish_Treachery.txt: 1177 tokens, 802 words [messenger, umar, shafa]\n",
      "   ‚úÖ part_196_Banu_Quraizas_Betrayal_and_Hypocrites_Doubts.txt: 1072 tokens, 781 words [doubts, messenger, muslim]\n",
      "   ‚úÖ part_197_Confederate_Dissension_and_End_of_the_Trench_Battle.txt: 897 tokens, 653 words [hijri, messenger, confederates]\n",
      "   ‚úÖ part_198_Invasion_of_Banu_Quraiza_Siege_and_Deliberation.txt: 646 tokens, 489 words [messenger, ibn umm \n",
      "maktum, some muslims]\n",
      "   ‚úÖ part_199_Banu_Quraizas_Judgment_and_Execution.txt: 743 tokens, 557 words [199, messenger, confederates]\n",
      "   ‚úÖ part_200_Aftermath_of_Banu_Quraiza_Sads_Death_and_Abu_Lubabas_Forgiveness.txt: 770 tokens, 479 words [aftermath, hijri, messenger]\n",
      "   ‚úÖ part_201_Military_Activities_Assassination_of_Salam_bin_Abi_Al_Huqaiq.txt: 442 tokens, 316 words [activities, bin, confederates]\n",
      "   ‚úÖ part_202_Punitive_Expeditions_after_Confederates_and_Quraiza.txt: 1817 tokens, 1215 words [yamama, hijri, plenty]\n",
      "   ‚úÖ part_203_Bani_Al_Mustaliq_Muraisi_Ghazwah_and_its_Implications.txt: 401 tokens, 284 words [haseeb al, 203, muslim]\n",
      "   ‚úÖ part_204_The_Treacherous_Role_of_Hypocrites_Abdullah_bin_Ubai.txt: 781 tokens, 551 words [messenger, bin, confederates]\n",
      "   ‚úÖ part_205_The_Slander_Affair_and_Prophets_Marriage_to_Zainab.txt: 1927 tokens, 1364 words [the muslim, confederates, should i]\n",
      "   ‚úÖ part_206_Expeditions_and_Delegations_following_Al_Muraisi_Ghazwah.txt: 771 tokens, 534 words [bani kalb, hijri, the muslim]\n",
      "   ‚úÖ part_207_Al_Hudaibiyah_Treaty_Prelude_and_Prophets_Dream.txt: 654 tokens, 481 words [hijri, the quraishites, part]\n",
      "   ‚úÖ part_208_Al_Hudaibiyah_Treaty_Negotiations_and_Envoys_Impressions.txt: 838 tokens, 601 words [mikraz, muslim, part]\n",
      "   ‚úÖ part_209_Al_Hudaibiyah_Treaty_Pledge_of_Fealty_and_Terms.txt: 811 tokens, 607 words [umar, war, abu sufyan]\n",
      "   ‚úÖ part_210_Al_Hudaibiyah_Treaty_Drafting_Disputes_and_Abu_Jandals_Ordeal.txt: 625 tokens, 432 words [the prophet [pbuh] \n",
      "said:, messenger, umar]\n",
      "   ‚úÖ part_211_Al_Hudaibiyah_Treaty_Immediate_Aftermath_and_New_Rulings.txt: 695 tokens, 493 words [aftermath, umm salamah, umar]\n",
      "   ‚úÖ part_212_Al_Hudaibiyah_Treaty_Socio_Political_Impact_and_Conversions.txt: 1452 tokens, 1045 words [umar, messenger, muslim]\n",
      "   ‚úÖ part_213_The_Second_Stage_New_Phase_of_Islamic_Action.txt: 314 tokens, 224 words [arabs, part, the jews]\n",
      "   ‚úÖ part_214_Prophets_Plans_to_Spread_Islam_Sending_Envoys_to_Kings.txt: 119 tokens, 80 words [214, plans, messenger]\n",
      "   ‚úÖ part_215_Prophets_Letter_to_Negus_King_of_Abyssinia.txt: 1087 tokens, 783 words [ashama, then, christians]\n",
      "   ‚úÖ part_216_Prophets_Letter_to_Muqawqas_Vicegerent_of_Egypt.txt: 799 tokens, 573 words [muqawqas, torah, therefore]\n",
      "   ‚úÖ part_217_Prophets_Letter_to_Chosroes_Emperor_of_Persia.txt: 432 tokens, 346 words [messenger, sahmi, part]\n",
      "   ‚úÖ part_218_Prophets_Letter_to_Caesar_King_of_Rome.txt: 1281 tokens, 957 words [the muslim, messenger, abu sufyan]\n",
      "   ‚úÖ part_219_Prophets_Letters_to_Regional_Governors_Bahrain_Yamama_Damascus_Oman.txt: 2057 tokens, 1490 words [yamama, messengership, muhadarat tareekh al]\n",
      "   ‚úÖ part_220_Conclusion_of_Diplomatic_Efforts.txt: 35 tokens, 28 words [islam, efforts, however]\n",
      "   ‚úÖ part_221_Post_Hudaibiyah_Hostilities_Dhu_Qarad_Invasion.txt: 963 tokens, 692 words [whenever, messenger, come]\n",
      "   ‚úÖ part_222_The_Conquest_of_Khaibar_Context_and_Preparation.txt: 878 tokens, 624 words [hudaibiyah peace treaty, after al, confederates]\n",
      "   ‚úÖ part_223_The_Conquest_of_Khaibar_March_Incidents_and_Arrival.txt: 1089 tokens, 744 words [messenger, march, then]\n",
      "   ‚úÖ part_224_The_Conquest_of_Khaibar_Initial_Fort_Assaults.txt: 798 tokens, 613 words [initial, muslim, part]\n",
      "   ‚úÖ part_225_The_Conquest_of_Khaibar_Second_Part_and_Spoils_Distribution.txt: 1339 tokens, 923 words [messenger, umar, negotiations \n",
      "ibn abi al]\n",
      "   ‚úÖ part_226_The_Conquest_of_Khaibar_Further_Jewish_Settlements_and_Strategic_Foresight.txt: 670 tokens, 485 words [prior, safar, muslim]\n",
      "   ‚úÖ part_227_Sporadic_Invasions_Dhat_ur_Riqa_7_AH_and_its_Impact.txt: 857 tokens, 616 words [the muslim, messenger, confederates]\n",
      "   ‚úÖ part_228_Sporadic_Invasions_Minor_Expeditions_after_Dhat_ur_Riqa.txt: 2124 tokens, 1465 words [banu quda, hunain, muluh]\n",
      "   ‚úÖ part_229_Compensatory_Umrah_Pilgrimage_Preparation_and_Entry_to_Makkah_Umrah_Completion_Marriage_Designations_and_Post-Pilgrimage.txt: 1187 tokens, 797 words [banu quda, the compensatory, after]\n",
      "   ‚úÖ part_230_The_Battle_of_Mutah.txt: 1619 tokens, 1143 words [the muslim, the battle, christians]\n",
      "   ‚úÖ part_231_Dhat_As_Salasil_Campaign.txt: 453 tokens, 328 words [umar, the muslim, part]\n",
      "   ‚úÖ part_232_Khadrah_Campaign.txt: 82 tokens, 58 words [in sha, islam, 232]\n",
      "   ‚úÖ part_233_The_Conquest_of_Makkah_Pre_Conquest_Events.txt: 1891 tokens, 1346 words [abu sufyan, after, qur]\n",
      "   ‚úÖ part_234_The_Conquest_of_Makkah_Entry_and_Idol_Removal.txt: 2726 tokens, 1865 words [a \n",
      "helper, ishmael, mar az]\n",
      "   ‚úÖ part_235_The_Conquest_of_Makkah_Clemency_and_Allegiance.txt: 971 tokens, 689 words [fudalah, messenger, umar]\n",
      "   ‚úÖ part_236_The_Conquest_of_Makkah_Post_Conquest_Actions.txt: 1211 tokens, 904 words [hudaibiyah peace treaty, uzza, muslim]\n",
      "   ‚úÖ part_237_The_Third_Stage_and_Hunain_Ghazwah_Part1.txt: 955 tokens, 714 words [awtas, hunain, messenger]\n",
      "   ‚úÖ part_238_Hunain_Ghazwah_Spy_Reconnaissance_and_Muslim_Retreat.txt: 1093 tokens, 794 words [hunain, abu sufyan, the islamic army]\n",
      "   ‚úÖ part_239_Hunain_Ghazwah_Counterattack_and_Victory.txt: 909 tokens, 619 words [awtas, hunain, folks]\n",
      "   ‚úÖ part_240_Ta_if_Campaign_Siege_and_Early_Actions.txt: 686 tokens, 495 words [messenger, 240, malik]\n",
      "   ‚úÖ part_241_Ta_if_Campaign_Lifting_Siege_and_Booty_Distribution.txt: 1132 tokens, 841 words [messenger, umar, for i]\n",
      "   ‚úÖ part_242_The_Helpers_Discontent_and_Prophets_Address.txt: 864 tokens, 638 words [messenger, hunain, yes]\n",
      "   ‚úÖ part_243_Hawazin_Delegation_and_Return_to_Madinah.txt: 898 tokens, 635 words [messenger, abbas, sard]\n",
      "   ‚úÖ part_244_Missions_and_Platoons_Introduction_and_Bani_Tamim.txt: 1034 tokens, 663 words [therefore, muzainah, thabit]\n",
      "   ‚úÖ part_245_Missions_and_Platoons_Subsequent_Expeditions.txt: 431 tokens, 293 words [kilabi, messenger, safar]\n",
      "   ‚úÖ part_246_Missions_and_Platoons_Adi_bin_Hatim_and_Prophecies.txt: 1142 tokens, 807 words [hirah, messenger, he said.]\n",
      "   ‚úÖ part_247_The_Invasion_of_Tabuk_Reasons_and_Initial_Fear.txt: 805 tokens, 611 words [initial, messenger, umar]\n",
      "   ‚úÖ part_248_Tabuk_Hypocrites_and_Prophets_Resolve.txt: 1013 tokens, 798 words [umar, messenger, hunain]\n",
      "   ‚úÖ part_249_Tabuk_Call_for_Charity_and_Generosity.txt: 561 tokens, 409 words [asim, messenger, umar]\n",
      "   ‚úÖ part_250_Tabuk_Army_Departure_and_Miracles_on_the_Way.txt: 835 tokens, 619 words [messenger, umar, departure]\n",
      "   ‚úÖ part_251_Tabuk_Army_at_Tabuk_and_Treaties.txt: 870 tokens, 670 words [messenger, muslim, therefore]\n",
      "   ‚úÖ part_252_Tabuk_Return_to_Madinah_and_Hypocrites_Plot.txt: 445 tokens, 306 words [messenger, yaman, part]\n",
      "   ‚úÖ part_253_Tabuk_Those_Who_Lagged_Behind.txt: 885 tokens, 648 words [whenever, nobody, messenger]\n",
      "   ‚úÖ part_254_Tabuk_Ramifications_and_Qur_anic_Verses.txt: 901 tokens, 622 words [the qur, messenger, verses]\n",
      "   ‚úÖ part_255_Abu_Bakr_Performs_Pilgrimage.txt: 451 tokens, 323 words [arj, messenger, soon]\n",
      "   ‚úÖ part_256_Meditation_on_the_Ghazawat.txt: 1537 tokens, 1203 words [hunain, messenger, war]\n",
      "   ‚úÖ part_257_People_Embrace_Islam_in_Large_Crowds.txt: 562 tokens, 441 words [muslim, leave, arabs]\n",
      "   ‚úÖ part_258_Overview_of_Delegations_to_Madinah.txt: 61 tokens, 51 words [the delegations \n",
      "the, therefore, ahl al]\n",
      "   ‚úÖ part_259_Delegation_of_Abdul_Qais.txt: 236 tokens, 166 words [messenger, muslim, the delegation of ‚Äòabdul qais: this tribe had two a]\n",
      "   ‚úÖ part_260_Daws_Delegation.txt: 158 tokens, 112 words [daws delegation: the arrival of this tribe was in t, tufail, amr ad]\n",
      "   ‚úÖ part_261_Farwah_Bani_Amr_Al_Judhamis_Messenger.txt: 152 tokens, 112 words [messenger, muslim, arabs]\n",
      "   ‚úÖ part_262_Suda_Delegation.txt: 209 tokens, 145 words [hijra, messenger, the]\n",
      "   ‚úÖ part_263_Arrival_of_Kab_bin_Zuhair_bin_Abi_Sulma.txt: 561 tokens, 390 words [messenger, yes., bin]\n",
      "   ‚úÖ part_264_Udharah_Delegation.txt: 177 tokens, 115 words [bani bakr, udharah delegation, messenger]\n",
      "   ‚úÖ part_265_Bali_Delegation.txt: 146 tokens, 97 words [265, bali delegation, yes]\n",
      "   ‚úÖ part_266_Thaqif_Delegation_Conversion_of_Urwah.txt: 161 tokens, 105 words [messenger, muslim, delegation]\n",
      "   ‚úÖ part_267_Thaqif_Delegation_Negotiations_and_Conversion.txt: 395 tokens, 300 words [messenger, abi al, delegation]\n",
      "   ‚úÖ part_268_Thaqif_Delegation_Uthman_bin_Abi_Al_As_and_Idol_Demolition.txt: 736 tokens, 531 words [messenger, bin, abi al]\n",
      "   ‚úÖ part_269_Message_of_the_Yemeni_Kings.txt: 168 tokens, 108 words [messenger, malik, part]\n",
      "   ‚úÖ part_270_Hamdan_Delegation.txt: 224 tokens, 171 words [messenger, malik, delegation]\n",
      "   ‚úÖ part_271_Delegation_of_Bani_Fazarah.txt: 211 tokens, 160 words [bani fazarah, hijra, they]\n",
      "   ‚úÖ part_272_Najrans_Delegation_Introduction_and_Initial_Refusal.txt: 447 tokens, 303 words [aqib, initial, come]\n",
      "   ‚úÖ part_273_Najrans_Delegation_Mubahala_and_Peace_Treaty.txt: 531 tokens, 389 words [aqib, messenger, isa]\n",
      "   ‚úÖ part_274_Bani_Haneefa_Delegation_Musailima_the_Liar_Part1.txt: 544 tokens, 394 words [messenger, habeeb, muslim]\n",
      "   ‚úÖ part_275_Bani_Haneefa_Delegation_Musailima_the_Liar_Part2.txt: 482 tokens, 312 words [yamama, messenger, delegation]\n",
      "   ‚úÖ part_276_Bani_Haneefa_Delegation_Musailima_the_Liar_Part3.txt: 65 tokens, 49 words [fath al, bani, haneefa]\n",
      "   ‚úÖ part_277_Delegation_of_Bani_Amir_bin_Sa_sa_a.txt: 435 tokens, 310 words [aslam, tufail, bin]\n",
      "   ‚úÖ part_278_Tujeeb_Delegation.txt: 309 tokens, 221 words [tujeeb, messenger, muslim]\n",
      "   ‚úÖ part_279_Tai_Delegation_and_Other_Tribes.txt: 344 tokens, 193 words [messenger, khaulan, then]\n",
      "   ‚úÖ part_280_Acceptance_of_Islam_and_Bedouin_Character.txt: 465 tokens, 324 words [the qur, messenger, arabs]\n",
      "   ‚úÖ part_281_Success_and_Impact_of_the_Call.txt: 683 tokens, 519 words [messenger, part, the companions]\n",
      "   ‚úÖ part_282_Transformation_of_Arabia_by_Islam.txt: 653 tokens, 484 words [part, scattered, mankind]\n",
      "   ‚úÖ part_283_Farewell_Pilgrimage_Premonitions_and_Preparations.txt: 481 tokens, 351 words [messenger, 283, part]\n",
      "   ‚úÖ part_284_Farewell_Pilgrimage_Rituals_in_Makkah.txt: 358 tokens, 220 words [part, sunday, farewell]\n",
      "   ‚úÖ part_285_Farewell_Pilgrimage_Sermon_at_Arafah.txt: 877 tokens, 623 words [messenger, arafah, listen]\n",
      "   ‚úÖ part_286_Farewell_Pilgrimage_Completion_and_Return.txt: 1788 tokens, 1185 words [sacred months, sheikh, then]\n",
      "   ‚úÖ part_287_The_Last_Expeditions.txt: 457 tokens, 337 words [because, messenger, safar]\n",
      "   ‚úÖ part_288_Journey_to_Allah_Symptoms_of_Farewell.txt: 440 tokens, 322 words [journey, safar, wida]\n",
      "   ‚úÖ part_289_Journey_to_Allah_Onset_of_Illness_and_Last_Week.txt: 154 tokens, 119 words [journey, safar, the last week \n",
      "when]\n",
      "   ‚úÖ part_290_Journey_to_Allah_Five_Days_Before_Death.txt: 555 tokens, 420 words [journey, you owe me three dirhams., then]\n",
      "   ‚úÖ part_291_Journey_to_Allah_Abu_Bakr_as_Imam.txt: 861 tokens, 617 words [others, journey, messenger]\n",
      "   ‚úÖ part_292_Journey_to_Allah_Days_Immediately_Before_Death.txt: 348 tokens, 246 words [journey, joseph, part]\n",
      "   ‚úÖ part_293_Journey_to_Allah_The_Last_Day_Alive.txt: 555 tokens, 394 words [293, journey, alive]\n",
      "   ‚úÖ part_294_Journey_to_Allah_The_Prophet_Breathes_His_Last.txt: 504 tokens, 366 words [journey, messenger, would]\n",
      "   ‚úÖ part_295_Companions_Reactions_Umar_and_Abu_Bakr.txt: 751 tokens, 522 words [death \n",
      "the, umar, messenger]\n",
      "   ‚úÖ part_296_Burial_and_Farewell_Preparations.txt: 483 tokens, 330 words [messenger, abbas, then]\n",
      "   ‚úÖ part_297_Prophet_Household_Daughters_and_Initial_Wives.txt: 507 tokens, 318 words [initial, the prophet household, ruqaiya]\n",
      "   ‚úÖ part_298_Prophet_Household_Hafsah_to_Juwairiyah.txt: 528 tokens, 307 words [juwairiyah bint al-harith: al-harith was the head o, umar, sahmi]\n",
      "   ‚úÖ part_299_Prophet_Household_Umm_Habibah_to_Maimunah.txt: 290 tokens, 167 words [abu sufyan, fadl lubabah, umm habibah: ramlah, the daughter of abu sufyan]\n",
      "   ‚úÖ part_300_Prophet_Household_Concubines_and_General_Reasons_for_Marriage.txt: 445 tokens, 292 words [messenger, masakeen, part]\n",
      "   ‚úÖ part_301_Prophet_Household_Strategic_Alliances_through_Marriage.txt: 404 tokens, 289 words [umm salamah, messenger, 301]\n",
      "   ‚úÖ part_302_Prophet_Household_Educational_and_Societal_Reasons.txt: 999 tokens, 724 words [messenger, confederates, muslim]\n",
      "   ‚úÖ part_303_Prophet_Household_Eradicating_Adoption_through_Action.txt: 503 tokens, 355 words [hudaibiyah peace treaty, umm salamah, messenger]\n",
      "   ‚úÖ part_304_Prophet_Household_Wives_Resilience_and_Polygamy_Justification.txt: 734 tokens, 512 words [muhsinat, messenger, discussing]\n",
      "   ‚úÖ part_305_Prophets_Attributes_and_Manners_Beauty_of_Creation.txt: 2256 tokens, 1583 words [abu huraira, then, umm ma]\n",
      "   ‚úÖ part_306_Prophets_Attributes_and_Manners_Perfection_of_Soul_and_Nobility.txt: 3395 tokens, 2540 words [verses, abu sufyan, abu talhah]\n",
      "   üìä Summary: 303 files processed\n",
      "   üè∑Ô∏è  Keywords: 4399 unique, avg 47.7 per document\n",
      "   üìù Content Summary (based on flags): 300 technical, 62 structured, 148 with definitions\n",
      "\n",
      "======================================================================\n",
      "üìä DOCUMENT PROCESSING SUMMARY (NO ADDITIONAL CHUNKING - v2)\n",
      "======================================================================\n",
      "üìà Overall Results:\n",
      "   Files processed: 568\n",
      "   Documents created: 568\n",
      "   Average tokens per document: 581.5 ¬± 449.8\n",
      "   Average words per document: 415.3 ¬± 307.2\n",
      "   Token range: 17 - 3395\n",
      "\n",
      "‚≠ê Content Types (derived string):\n",
      "   - Technical, Definitions: 161 documents\n",
      "   - Technical, Structured, Definitions: 66 documents\n",
      "   - Technical, Structured: 80 documents\n",
      "   - Technical: 245 documents\n",
      "   - General: 16 documents\n",
      "\n",
      "üìÇ By Category:\n",
      "--------------------------------------------------\n",
      "ICC                   87 files ‚Üí   87 documents\n",
      "                     avg:   537 tokens, 2590 keywords\n",
      "                     content (flags): 87 tech, 71 structured\n",
      "NLP                   58 files ‚Üí   58 documents\n",
      "                     avg:   597 tokens, 1474 keywords\n",
      "                     content (flags): 55 tech, 13 structured\n",
      "Pakistan Studies     120 files ‚Üí  120 documents\n",
      "                     avg:   320 tokens, 1946 keywords\n",
      "                     content (flags): 110 tech, 0 structured\n",
      "Sealed Nectar        303 files ‚Üí  303 documents\n",
      "                     avg:   695 tokens, 4399 keywords\n",
      "                     content (flags): 300 tech, 62 structured\n",
      "\n",
      "‚≠ê Content Quality (flags):\n",
      "   Average readability score: 100.0\n",
      "   Technical documents (flag): 552/568\n",
      "   Structured documents (flag): 146/568\n",
      "   Documents with definitions (flag): 227/568\n",
      "   Documents with content keywords: 568/568\n",
      "\n",
      "‚úÖ Document processing completed!\n",
      "üî• Ready for RAG pipeline with original 568 files as:\n",
      "   ‚Ä¢ Individual document units (main text now in 'content' field)\n",
      "   ‚Ä¢ Enhanced metadata extraction (includes 'content_type' string)\n",
      "   ‚Ä¢ Category-specific optimization\n",
      "   ‚Ä¢ Comprehensive keyword analysis\n",
      "üíæ Documents saved to: processed_documents_568/\n",
      "   ‚Ä¢ All documents: processed_documents.json (568 documents)\n",
      "   ‚Ä¢ Statistics: processing_statistics.json\n",
      "   ‚Ä¢ By category: documents_[category].json\n",
      "   ‚Ä¢ Summary: summary_report.txt\n",
      "\n",
      "======================================================================\n",
      "üìù SAMPLE PROCESSED DOCUMENTS (v2)\n",
      "======================================================================\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc01_API_Evolution_Data_Formats_&_The_Emergence_of_Standards_SOAP_&_REST.txt\n",
      "üî§ Size: 676 tokens | 489 words | 20 sentences\n",
      "üÜï Content Type: Technical, Definitions\n",
      "üè∑Ô∏è  Keywords: issue, acid, wsdl, post, soap\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.72\n",
      "üîç Flags: Technical=True, Structured=False, Definitions=True\n",
      "üìú Content preview: The limitations of earlier inter-system communication methods, particularly their struggles with interoperability, \n",
      "scalability, and ease of use, became glaringly apparent as the internet expanded. Th...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc02_Advantages_r_Disadvantages_of_Private_Cloud_&_Hybrid_Cloud_Introduction.txt\n",
      "üî§ Size: 563 tokens | 445 words | 17 sentences\n",
      "üÜï Content Type: Technical, Structured, Definitions\n",
      "üè∑Ô∏è  Keywords: because, compared, public, disadvantages, on one side (e\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.79\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=True\n",
      "üìú Content preview: Advantages of the Private Cloud Model: \n",
      "The private cloud model offers distinct benefits, primarily centered around control and security, \n",
      "which are highly valued by certain organizations: \n",
      "‚Ä¢ Better C...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc03_Advantages_r_Disadvantages_of_Public_Cloud_&_Private_Cloud_Introduction.txt\n",
      "üî§ Size: 535 tokens | 429 words | 17 sentences\n",
      "üÜï Content Type: Technical, Structured, Definitions\n",
      "üè∑Ô∏è  Keywords: because, public, minimal investment, perceived, no maintenance (by user): the ongoing maintenance w\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.80\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=True\n",
      "üìú Content preview: Advantages of the Public Cloud Model: \n",
      "The public cloud offers several compelling benefits, making it an attractive option for many \n",
      "organizations and individuals: \n",
      "‚Ä¢ Minimal Investment: Because it ty...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc04_Alternatives_to_VMs_Containers_Introduction.txt\n",
      "üî§ Size: 552 tokens | 441 words | 21 sentences\n",
      "üÜï Content Type: Technical, Structured, Definitions\n",
      "üè∑Ô∏è  Keywords: to resource, portability, because, an alternatives, alternative\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.80\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=True\n",
      "üìú Content preview: Do We Have an Alternative? (To Resource-Heavy VMs) \n",
      "The preceding discussions on the drawbacks of virtualization, particularly the significant resource \n",
      "consumption by each Virtual Machine (VM) due to...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc05_Basic_Security_Terms_Continued_&_Threat_Agents.txt\n",
      "üî§ Size: 512 tokens | 422 words | 22 sentences\n",
      "üÜï Content Type: Technical, Structured\n",
      "üè∑Ô∏è  Keywords: the probability of a threat successfully occurring, continued, authentication, availability:, vulnerability:\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.82\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=False\n",
      "üìú Content preview: Basic Terms and Concepts in Security (Continued): \n",
      "This section continues defining fundamental security terms relevant to cloud environments: \n",
      "‚Ä¢ Authentication: \n",
      "o Definition: Authentication is the pr...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc06_Benefits_and_Steps_of_Cloud_Threat_Modeling.txt\n",
      "üî§ Size: 564 tokens | 396 words | 28 sentences\n",
      "üÜï Content Type: Technical, Structured\n",
      "üè∑Ô∏è  Keywords: clearly, define mitigation strategies:, privilege, elevation, benefits\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.70\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=False\n",
      "üìú Content preview: Benefits of Cloud Threat Modeling: \n",
      "Proactively engaging in cloud threat modeling offers several significant advantages for enhancing \n",
      "the security posture of cloud-based systems: \n",
      "1. Proactive Threat...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc07_Big_Data_Definition_Sources_Examples_&_Four_Dimensions.txt\n",
      "üî§ Size: 633 tokens | 442 words | 26 sentences\n",
      "üÜï Content Type: Technical, Structured, Definitions\n",
      "üè∑Ô∏è  Keywords: examples, volume, big data, describes, variety\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.70\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=True\n",
      "üìú Content preview: Big Data (Definition and Scope) \n",
      "This section elaborates on the concept of Big Data, defining it as datasets and data flows that \n",
      "are so large and complex that traditional data processing applications...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc08_Big_Data_Market_Driving_Factors_&_Data_Analytical_Tools.txt\n",
      "üî§ Size: 610 tokens | 451 words | 23 sentences\n",
      "üÜï Content Type: Technical, Structured\n",
      "üè∑Ô∏è  Keywords: apache spark: a fast and general-purpose cluster co, key, cassandra, other logos might include tools like apache flink (, apache cassandra: a highly scalable, distributed no\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.74\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=False\n",
      "üìú Content preview: Big Data Market Driving Factors \n",
      "This section outlines key developments and trends that have fueled the rapid expansion and \n",
      "adoption of Big Data technologies. Several factors have converged to create...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc09_CAP_Theorem_Introduction_and_Core_Properties.txt\n",
      "üî§ Size: 480 tokens | 387 words | 19 sentences\n",
      "üÜï Content Type: Technical, Structured\n",
      "üè∑Ô∏è  Keywords: icc09, brewer, a venn diagram is often used to illustrate this, wi, consistency, a venn\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.81\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=False\n",
      "üìú Content preview: CAP Theorem: Core Concepts \n",
      "The CAP theorem, also known as Brewer's theorem (after computer scientist Eric Brewer), is a \n",
      "fundamental principle in distributed computing systems. It states that it is i...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc10_CAP_Theorem_Cases_Importance_of_Partition_Tolerance_&_Database_as_a_Service_DynamoDB_Intro.txt\n",
      "üî§ Size: 622 tokens | 473 words | 19 sentences\n",
      "üÜï Content Type: Technical, Structured\n",
      "üè∑Ô∏è  Keywords: therefore, purpose: dynamodb is specifically used to design hi, continued, database, service\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.76\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=False\n",
      "üìú Content preview: CAP Theorem: Illustrative Cases (Continued) \n",
      "Building on the CAP theorem's principle that a distributed system can only guarantee two of the \n",
      "three properties (Consistency, Availability, Partition Tol...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc11_Classic_Consistent_Hashing_Pros_r_Cons_&_Consistent_Hashing_with_T_Tokens_per_Node.txt\n",
      "üî§ Size: 587 tokens | 455 words | 21 sentences\n",
      "üÜï Content Type: Technical, Structured\n",
      "üè∑Ô∏è  Keywords: improvement, multiple tokens, reduced rehashing on node changes: when a new physi, continued, how\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.78\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=False\n",
      "üìú Content preview: 2. Classic Consistent Hashing (Continued): \n",
      "‚Ä¢ Advantages of Classic Consistent Hashing: \n",
      "o Scalability: Significantly reduces the number of keys that need to be \n",
      "reassigned when nodes are added to or ...\n",
      "--------------------------------------------------\n",
      "\n",
      "üè∑Ô∏è  Category: ICC\n",
      "üìÑ File: icc12_Cloud_Bursting_Architecture.txt\n",
      "üî§ Size: 489 tokens | 375 words | 18 sentences\n",
      "üÜï Content Type: Technical, Structured, Definitions\n",
      "üè∑Ô∏è  Keywords: cloud bursting architecture \n",
      "cloud bursting, how it works, threshold, key components, bursting out\n",
      "‚öñÔ∏è  Weights: Tech=1.5, Narrative=1.0\n",
      "üìä Quality: Readability=100.0, Density=0.77\n",
      "üîç Flags: Technical=True, Structured=True, Definitions=True\n",
      "üìú Content preview: Cloud Bursting Architecture \n",
      "Cloud Bursting is a hybrid cloud strategy that allows an organization's on-premise IT environment \n",
      "to dynamically extend its capacity by \"bursting\" into a public or privat...\n",
      "--------------------------------------------------\n",
      "\n",
      "üéâ Document processing complete! (v2)\n",
      "Processed 568 documents from original files.\n",
      "Output 'processed_documents.json' now uses 'content' for main text and includes 'content_type' string.\n",
      "Ready for embedding generation and vector database indexing!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, \n",
    "                 model_name: str = \"intfloat/e5-base\"):\n",
    "        \"\"\"\n",
    "        Document Processor that treats each file as a single unit without additional chunking\n",
    "        \n",
    "        Args:\n",
    "            model_name: Embedding model for tokenization\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.document_counter = 0\n",
    "        \n",
    "        # Category-specific settings for better domain handling\n",
    "        self.category_configs = {\n",
    "            'ICC': {'technical_weight': 1.5, 'context_boost': True},\n",
    "            'NLP': {'technical_weight': 1.8, 'context_boost': True},\n",
    "            'Pakistan Studies': {'narrative_weight': 1.2, 'context_boost': False},\n",
    "            'Sealed Nectar': {'narrative_weight': 1.0, 'context_boost': False}\n",
    "        }\n",
    "        \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Accurate token estimation\"\"\"\n",
    "        return len(self.tokenizer.tokenize(text))\n",
    "    \n",
    "    def extract_enhanced_keywords(self, filename: str, text: str = \"\") -> Dict[str, any]:\n",
    "        \"\"\"Enhanced keyword extraction with context awareness\"\"\"\n",
    "        name_without_ext = filename.replace('.txt', '')\n",
    "        separators = ['_', '-', ' ', '.', '(', ')', '[', ']']\n",
    "        filename_keywords = [name_without_ext]\n",
    "        \n",
    "        for sep in separators:\n",
    "            new_keywords = []\n",
    "            for keyword in filename_keywords:\n",
    "                new_keywords.extend(keyword.split(sep))\n",
    "            filename_keywords = new_keywords\n",
    "        \n",
    "        filename_keywords = [\n",
    "            re.sub(r'\\s+', ' ', kw.strip().lower()) \n",
    "            for kw in filename_keywords \n",
    "            if len(kw.strip()) > 2 and kw.strip().lower() not in {'txt', 'file', 'doc', 'the', 'and', 'for', 'with'}\n",
    "        ]\n",
    "        \n",
    "        content_keywords = []\n",
    "        technical_terms_from_content = [] # Explicitly define to avoid confusion with 'technical_terms' in embedding code\n",
    "        if text:\n",
    "            capitalized_phrases = re.findall(r'\\b[A-Z][A-Za-z]*(?:\\s+[A-Z][A-Za-z]*)*\\b', text)\n",
    "            acronyms = re.findall(r'\\b[A-Z]{2,}\\b', text)\n",
    "            quoted_terms = re.findall(r'[\"\\']([^\"\\']{3,30})[\"\\']', text)\n",
    "            numbered_points = re.findall(r'(?:^|\\n)\\s*(?:\\d+\\.|\\‚Ä¢|\\-)\\s*([A-Z][^.\\n]{10,50})', text, re.MULTILINE)\n",
    "            \n",
    "            # Combine all potential content keywords\n",
    "            # 'technical_terms_from_content' will be used for 'has_technical_terms' flag later\n",
    "            technical_terms_from_content = capitalized_phrases + acronyms \n",
    "            content_keywords = technical_terms_from_content + quoted_terms + numbered_points\n",
    "            content_keywords = [kw.lower().strip() for kw in content_keywords if len(kw.strip()) > 2]\n",
    "            technical_terms_from_content = [kw.lower().strip() for kw in technical_terms_from_content if len(kw.strip()) > 2]\n",
    "\n",
    "        return {\n",
    "            'filename_keywords': list(set(filename_keywords)),\n",
    "            'content_keywords': list(set(content_keywords)), # All derived keywords from content\n",
    "            'technical_terms': list(set(technical_terms_from_content)), # Specifically technical terms for embedding prefix\n",
    "            'all_keywords': list(set(filename_keywords + content_keywords)),\n",
    "            'keyword_string': ' '.join(set(filename_keywords + content_keywords))\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text content\"\"\"\n",
    "        text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    \n",
    "    def create_document_metadata(self, text_content: str, source_info: Dict, \n",
    "                               keyword_data: Dict, category_config: Dict) -> Dict: # Renamed 'text' to 'text_content' for clarity\n",
    "        \"\"\"Create comprehensive document metadata for better retrieval\"\"\"\n",
    "        \n",
    "        doc_hash = hashlib.md5(text_content.encode()).hexdigest()[:12]\n",
    "        \n",
    "        # The 'enhanced_text' was for a previous embedding strategy, might not be directly used\n",
    "        # by the new Original568EmbeddingGenerator's prepare_text_for_embedding if metadata is used as prefix.\n",
    "        # Keeping it for now as it doesn't harm.\n",
    "        enhanced_text_for_embedding = text_content \n",
    "        if category_config.get('context_boost'):\n",
    "            enhanced_text_for_embedding = f\"[{source_info['category']}] {text_content}\"\n",
    "        \n",
    "        word_count = len(text_content.split())\n",
    "        try:\n",
    "            sentence_count = len(nltk.sent_tokenize(text_content))\n",
    "        except:\n",
    "            sentence_count = len(re.split(r'[.!?]+', text_content))\n",
    "        \n",
    "        token_count = self.estimate_tokens(text_content)\n",
    "        paragraphs = len([p for p in text_content.split('\\n\\n') if p.strip()])\n",
    "        avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0\n",
    "        \n",
    "        # --- MODIFICATION 1: Define boolean flags ---\n",
    "        has_technical_terms_flag = len(keyword_data['technical_terms']) > 5 # Use the specific technical_terms list\n",
    "        is_structured_flag = bool(re.search(r'(?:^|\\n)\\s*(?:\\d+\\.|\\‚Ä¢|\\-)', text_content, re.MULTILINE))\n",
    "        has_definitions_flag = bool(re.search(r'[\"\\']([^\"\\']{10,50})[\"\\']', text_content))\n",
    "\n",
    "        # --- MODIFICATION 2: Create content_type string ---\n",
    "        doc_content_types_list = []\n",
    "        if has_technical_terms_flag: doc_content_types_list.append(\"Technical\")\n",
    "        if is_structured_flag: doc_content_types_list.append(\"Structured\")\n",
    "        if has_definitions_flag: doc_content_types_list.append(\"Definitions\")\n",
    "        content_type_str = \", \".join(doc_content_types_list) if doc_content_types_list else \"General\"\n",
    "\n",
    "        document_data = {\n",
    "            'document_id': self.document_counter,\n",
    "            'document_hash': doc_hash,\n",
    "            \n",
    "            # --- MODIFICATION 3: Main text content now under 'content' key ---\n",
    "            'content': text_content, \n",
    "            'enhanced_text': enhanced_text_for_embedding, \n",
    "            \n",
    "            'category': source_info['category'],\n",
    "            'source_file': source_info['file_path'],\n",
    "            'file_name': source_info['file_name'],\n",
    "            \n",
    "            'filename_keywords': keyword_data['filename_keywords'],\n",
    "            'content_keywords': keyword_data['content_keywords'], # General content keywords\n",
    "            'technical_terms': keyword_data['technical_terms'], # Specific technical terms for embedding prefix\n",
    "            'all_keywords': keyword_data['all_keywords'],\n",
    "            'keyword_string': keyword_data['keyword_string'],\n",
    "            \n",
    "            'token_count': token_count,\n",
    "            'word_count': word_count,\n",
    "            'sentence_count': sentence_count,\n",
    "            'paragraph_count': paragraphs,\n",
    "            \n",
    "            'technical_weight': category_config.get('technical_weight', 1.0),\n",
    "            'narrative_weight': category_config.get('narrative_weight', 1.0),\n",
    "            \n",
    "            'document_density': word_count / token_count if token_count > 0 else 0,\n",
    "            'avg_sentence_length': avg_words_per_sentence,\n",
    "            'readability_score': min(100, max(0, 206.835 - 1.015 * avg_words_per_sentence)) if avg_words_per_sentence > 0 else 0,\n",
    "            \n",
    "            # Store the boolean flags as before (can be useful for other things)\n",
    "            'has_technical_terms': has_technical_terms_flag,\n",
    "            'is_structured': is_structured_flag,\n",
    "            'has_definitions': has_definitions_flag,\n",
    "\n",
    "            # --- MODIFICATION 4: Add the new 'content_type' string field ---\n",
    "            'content_type': content_type_str\n",
    "        }\n",
    "        \n",
    "        self.document_counter += 1\n",
    "        return document_data\n",
    "    \n",
    "    def process_document_folders(self, base_path: str) -> Tuple[List[Dict], Dict]:\n",
    "        folder_categories = {\n",
    "            'icc_text_files': 'ICC',\n",
    "            'NLP_text_files': 'NLP',\n",
    "            'pakSt_text_files': 'Pakistan Studies',\n",
    "            'Sealed_nectar_text_files': 'Sealed Nectar'\n",
    "        }\n",
    "        \n",
    "        all_documents = []\n",
    "        detailed_stats = {\n",
    "            'processing_stats': {}, 'keyword_stats': {}, 'quality_metrics': {},\n",
    "            'document_size_distribution': [], 'content_analysis': {}\n",
    "        }\n",
    "        \n",
    "        print(\"üöÄ Document Processing Started (No Additional Chunking - v2 with 'content' & 'content_type')\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for folder_name, category in folder_categories.items():\n",
    "            folder_path = os.path.join(base_path, folder_name)\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"‚ö†Ô∏è  Folder not found: {folder_name}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"üìÅ Processing: {category} ({folder_name})\")\n",
    "            folder_documents, folder_stats = [], {'files_processed': 0, 'empty_files': 0, 'error_files': 0}\n",
    "            folder_keywords, document_sizes = set(), []\n",
    "            content_types_summary = {'technical': 0, 'structured': 0, 'with_definitions': 0} # based on boolean flags\n",
    "            \n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith('.txt'):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            raw_content = f.read() # Changed variable name\n",
    "                        \n",
    "                        if not raw_content.strip():\n",
    "                            folder_stats['empty_files'] += 1\n",
    "                            continue\n",
    "                        \n",
    "                        cleaned_content = self.clean_text(raw_content) # Use cleaned content\n",
    "                        \n",
    "                        source_info = {'category': category, 'file_path': file_path, 'file_name': file_name}\n",
    "                        keyword_data = self.extract_enhanced_keywords(file_name, cleaned_content) # Pass cleaned_content\n",
    "                        category_config = self.category_configs.get(category, {})\n",
    "                        \n",
    "                        document_data = self.create_document_metadata(\n",
    "                            cleaned_content, source_info, keyword_data, category_config\n",
    "                        )\n",
    "                        \n",
    "                        folder_documents.append(document_data)\n",
    "                        folder_stats['files_processed'] += 1\n",
    "                        \n",
    "                        folder_keywords.update(document_data['all_keywords'])\n",
    "                        document_sizes.append(document_data['token_count'])\n",
    "                        \n",
    "                        if document_data['has_technical_terms']: content_types_summary['technical'] += 1\n",
    "                        if document_data['is_structured']: content_types_summary['structured'] += 1\n",
    "                        if document_data['has_definitions']: content_types_summary['with_definitions'] += 1\n",
    "                        \n",
    "                        sample_keywords = ', '.join(list(document_data['all_keywords'])[:3]) # Ensure it's a list\n",
    "                        print(f\"   ‚úÖ {file_name}: {document_data['token_count']} tokens, {document_data['word_count']} words [{sample_keywords}]\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        folder_stats['error_files'] += 1\n",
    "                        print(f\"   ‚ùå Error: {file_name} - {str(e)[:50]}...\")\n",
    "            \n",
    "            detailed_stats['processing_stats'][category] = folder_stats\n",
    "            detailed_stats['processing_stats'][category]['documents_processed'] = len(folder_documents)\n",
    "            detailed_stats['processing_stats'][category]['avg_tokens_per_document'] = np.mean(document_sizes) if document_sizes else 0\n",
    "            detailed_stats['processing_stats'][category]['token_std'] = np.std(document_sizes) if document_sizes else 0\n",
    "            \n",
    "            detailed_stats['keyword_stats'][category] = {\n",
    "                'unique_keywords': len(folder_keywords),\n",
    "                'sample_keywords': list(folder_keywords)[:10],\n",
    "                'avg_keywords_per_document': np.mean([len(d['all_keywords']) for d in folder_documents]) if folder_documents else 0\n",
    "            }\n",
    "            \n",
    "            detailed_stats['content_analysis'][category] = content_types_summary\n",
    "            detailed_stats['document_size_distribution'].extend(document_sizes)\n",
    "            all_documents.extend(folder_documents)\n",
    "            \n",
    "            print(f\"   üìä Summary: {folder_stats['files_processed']} files processed\")\n",
    "            print(f\"   üè∑Ô∏è  Keywords: {len(folder_keywords)} unique, avg {detailed_stats['keyword_stats'][category]['avg_keywords_per_document']:.1f} per document\")\n",
    "            print(f\"   üìù Content Summary (based on flags): {content_types_summary['technical']} technical, {content_types_summary['structured']} structured, {content_types_summary['with_definitions']} with definitions\")\n",
    "            print()\n",
    "        \n",
    "        detailed_stats['quality_metrics'] = self.calculate_quality_metrics(all_documents)\n",
    "        self.print_enhanced_summary(detailed_stats, all_documents)\n",
    "        return all_documents, detailed_stats\n",
    "    \n",
    "    def calculate_quality_metrics(self, documents: List[Dict]) -> Dict:\n",
    "        if not documents: return {}\n",
    "        token_counts = [d['token_count'] for d in documents]\n",
    "        word_counts = [d['word_count'] for d in documents]\n",
    "        readability_scores = [d['readability_score'] for d in documents]\n",
    "        \n",
    "        return {\n",
    "            'total_documents': len(documents),\n",
    "            'token_distribution': {'mean': np.mean(token_counts), 'std': np.std(token_counts), 'min': np.min(token_counts), 'max': np.max(token_counts), 'median': np.median(token_counts)},\n",
    "            'word_distribution': {'mean': np.mean(word_counts), 'std': np.std(word_counts), 'min': np.min(word_counts), 'max': np.max(word_counts), 'median': np.median(word_counts)},\n",
    "            'content_quality': {\n",
    "                'avg_readability': np.mean(readability_scores),\n",
    "                'documents_with_technical_terms': sum(1 for d in documents if d['has_technical_terms']),\n",
    "                'structured_documents': sum(1 for d in documents if d['is_structured']),\n",
    "                'documents_with_definitions': sum(1 for d in documents if d['has_definitions'])\n",
    "            },\n",
    "            'keyword_coverage': {\n",
    "                'avg_keywords_per_document': np.mean([len(d['all_keywords']) for d in documents]),\n",
    "                'documents_with_content_keywords': sum(1 for d in documents if len(d['content_keywords']) > 0)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def print_enhanced_summary(self, stats: Dict, all_documents: List[Dict]):\n",
    "        print(\"=\" * 70)\n",
    "        print(\"üìä DOCUMENT PROCESSING SUMMARY (NO ADDITIONAL CHUNKING - v2)\")\n",
    "        print(\"=\" * 70)\n",
    "        processing_stats, quality_metrics = stats['processing_stats'], stats['quality_metrics']\n",
    "        total_files, total_documents = sum(s['files_processed'] for s in processing_stats.values()), len(all_documents)\n",
    "        avg_tokens, avg_words = quality_metrics['token_distribution']['mean'], quality_metrics['word_distribution']['mean']\n",
    "        \n",
    "        print(f\"üìà Overall Results:\")\n",
    "        print(f\"   Files processed: {total_files}\")\n",
    "        print(f\"   Documents created: {total_documents}\")\n",
    "        print(f\"   Average tokens per document: {avg_tokens:.1f} ¬± {quality_metrics['token_distribution']['std']:.1f}\")\n",
    "        print(f\"   Average words per document: {avg_words:.1f} ¬± {quality_metrics['word_distribution']['std']:.1f}\")\n",
    "        print(f\"   Token range: {quality_metrics['token_distribution']['min']:.0f} - {quality_metrics['token_distribution']['max']:.0f}\")\n",
    "        \n",
    "        # Count documents by new 'content_type' string\n",
    "        content_type_counts = Counter(doc.get('content_type', 'Unknown') for doc in all_documents)\n",
    "        print(f\"\\n‚≠ê Content Types (derived string):\")\n",
    "        for c_type, count in content_type_counts.items():\n",
    "            print(f\"   - {c_type}: {count} documents\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"üìÇ By Category:\")\n",
    "        print(\"-\" * 50)\n",
    "        for category, stat in processing_stats.items():\n",
    "            kw_stat = stats['keyword_stats'][category]\n",
    "            content_stat = stats['content_analysis'][category] # This uses boolean flags for summary\n",
    "            print(f\"{category:<20} {stat['files_processed']:>3} files ‚Üí {stat['documents_processed']:>4} documents\")\n",
    "            print(f\"{'':>20} avg: {stat['avg_tokens_per_document']:>5.0f} tokens, {kw_stat['unique_keywords']:>3} keywords\")\n",
    "            print(f\"{'':>20} content (flags): {content_stat['technical']} tech, {content_stat['structured']} structured\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"‚≠ê Content Quality (flags):\")\n",
    "        print(f\"   Average readability score: {quality_metrics['content_quality']['avg_readability']:.1f}\")\n",
    "        print(f\"   Technical documents (flag): {quality_metrics['content_quality']['documents_with_technical_terms']}/{total_documents}\")\n",
    "        print(f\"   Structured documents (flag): {quality_metrics['content_quality']['structured_documents']}/{total_documents}\")\n",
    "        print(f\"   Documents with definitions (flag): {quality_metrics['content_quality']['documents_with_definitions']}/{total_documents}\")\n",
    "        print(f\"   Documents with content keywords: {quality_metrics['keyword_coverage']['documents_with_content_keywords']}/{total_documents}\")\n",
    "        print()\n",
    "        print(\"‚úÖ Document processing completed!\")\n",
    "        print(\"üî• Ready for RAG pipeline with original 568 files as:\")\n",
    "        print(\"   ‚Ä¢ Individual document units (main text now in 'content' field)\")\n",
    "        print(\"   ‚Ä¢ Enhanced metadata extraction (includes 'content_type' string)\")\n",
    "        print(\"   ‚Ä¢ Category-specific optimization\")\n",
    "        print(\"   ‚Ä¢ Comprehensive keyword analysis\")\n",
    "    \n",
    "    def save_documents(self, documents: List[Dict], stats: Dict, output_dir: str = \"processed_documents_568\"):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        documents_file = os.path.join(output_dir, \"processed_documents.json\")\n",
    "        with open(documents_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(documents, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        stats_file = os.path.join(output_dir, \"processing_statistics.json\")\n",
    "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "            def convert_numpy(obj):\n",
    "                if isinstance(obj, np.integer): return int(obj)\n",
    "                if isinstance(obj, np.floating): return float(obj)\n",
    "                if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "                return obj\n",
    "            json.dump(json.loads(json.dumps(stats, default=convert_numpy)), f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        category_counts = {}\n",
    "        for document in documents:\n",
    "            category = document['category'].replace(' ', '_').lower()\n",
    "            category_file = os.path.join(output_dir, f\"documents_{category}.json\")\n",
    "            category_documents = []\n",
    "            if os.path.exists(category_file):\n",
    "                with open(category_file, 'r', encoding='utf-8') as f: category_documents = json.load(f)\n",
    "            category_documents.append(document)\n",
    "            category_counts[category] = len(category_documents)\n",
    "            with open(category_file, 'w', encoding='utf-8') as f: json.dump(category_documents, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        summary_file = os.path.join(output_dir, \"summary_report.txt\")\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"DOCUMENT PROCESSING SUMMARY (v2 - 'content' & 'content_type')\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Total documents processed: {len(documents)}\\n\")\n",
    "            f.write(\"Processing approach: No additional chunking (original files as units)\\n\\n\")\n",
    "            f.write(\"Documents by category:\\n\")\n",
    "            for category, count in category_counts.items():\n",
    "                f.write(f\"  - {category.replace('_', ' ').title()}: {count} documents\\n\")\n",
    "            f.write(f\"\\nAverage document size: {stats['quality_metrics']['token_distribution']['mean']:.0f} tokens\\n\")\n",
    "            f.write(f\"Size range: {stats['quality_metrics']['token_distribution']['min']:.0f} - {stats['quality_metrics']['token_distribution']['max']:.0f} tokens\\n\")\n",
    "            content_type_counts_report = Counter(doc.get('content_type', 'Unknown') for doc in documents)\n",
    "            f.write(\"\\nContent Types (derived string):\\n\")\n",
    "            for c_type, count in content_type_counts_report.items():\n",
    "                 f.write(f\"  - {c_type}: {count} documents\\n\")\n",
    "        \n",
    "        print(f\"üíæ Documents saved to: {output_dir}/\")\n",
    "        print(f\"   ‚Ä¢ All documents: processed_documents.json ({len(documents)} documents)\")\n",
    "        print(f\"   ‚Ä¢ Statistics: processing_statistics.json\") \n",
    "        print(f\"   ‚Ä¢ By category: documents_[category].json\")\n",
    "        print(f\"   ‚Ä¢ Summary: summary_report.txt\")\n",
    "\n",
    "def main():\n",
    "    processor = DocumentProcessor(model_name=\"intfloat/e5-base\")\n",
    "    documents_path = \"documents\" # Ensure this path points to your source text files\n",
    "    all_documents, stats = processor.process_document_folders(documents_path)\n",
    "    processor.save_documents(all_documents, stats)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìù SAMPLE PROCESSED DOCUMENTS (v2)\")\n",
    "    print(\"=\" * 70)\n",
    "    categories_shown = set()\n",
    "    for doc in all_documents[:12]: # Show more samples if needed\n",
    "        if doc['category'] not in categories_shown or len(categories_shown) < 4:\n",
    "            print(f\"\\nüè∑Ô∏è  Category: {doc['category']}\")\n",
    "            print(f\"üìÑ File: {doc['file_name']}\")\n",
    "            print(f\"üî§ Size: {doc['token_count']} tokens | {doc['word_count']} words | {doc['sentence_count']} sentences\")\n",
    "            print(f\"üÜï Content Type: {doc['content_type']}\") # Display new field\n",
    "            print(f\"üè∑Ô∏è  Keywords: {', '.join(list(doc['all_keywords'])[:5])}\")\n",
    "            print(f\"‚öñÔ∏è  Weights: Tech={doc['technical_weight']}, Narrative={doc['narrative_weight']}\")\n",
    "            print(f\"üìä Quality: Readability={doc['readability_score']:.1f}, Density={doc['document_density']:.2f}\")\n",
    "            # Show boolean flags for comparison\n",
    "            print(f\"üîç Flags: Technical={doc['has_technical_terms']}, Structured={doc['is_structured']}, Definitions={doc['has_definitions']}\")\n",
    "            print(f\"üìú Content preview: {doc['content'][:200]}...\") # Preview from 'content' field\n",
    "            print(\"-\" * 50)\n",
    "            categories_shown.add(doc['category'])\n",
    "            if len(categories_shown) >= 4 and len(all_documents) > 4: # Ensure we don't go out of bounds if few docs\n",
    "                break\n",
    "    return all_documents, stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents, statistics = main()\n",
    "    print(f\"\\nüéâ Document processing complete! (v2)\")\n",
    "    print(f\"Processed {len(documents)} documents from original files.\")\n",
    "    print(\"Output 'processed_documents.json' now uses 'content' for main text and includes 'content_type' string.\")\n",
    "    print(\"Ready for embedding generation and vector database indexing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1b090",
   "metadata": {},
   "source": [
    "# On Google COLAB on Dataset Embeddings were applied, EMBEDDER: \"intfloat/e5-base\"\n",
    "\n",
    "###                                    AND\n",
    "\n",
    "# Vector Store (FAISS) \"Facebook AI Similarity Search\" was applied for Creation & Indexing on Embedded Dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1116c",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------\n",
    "### --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Load the Models Once In Memory. \n",
    "## This is done so you don't have to load it everytime.\n",
    "\n",
    "### --------------------------------------------------------------------------------------------------------------\n",
    "### --------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bde893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ RAG PIPELINE (568 DOCUMENTS) SETUP AND INITIALIZATION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üöÄ INITIALIZING COMPLETE RAG PIPELINE (FOR 568 ORIGINAL DOCUMENTS)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üîÑ INITIALIZING DENSE RETRIEVER (FOR 568 ORIGINAL DOCUMENTS)\n",
      "================================================================================\n",
      "üì• Loading embedding model (intfloat/e5-base)...\n",
      "‚úÖ Embedding model loaded in 3.70 seconds\n",
      "üì• Loading FAISS index from: faiss_vector_store_568\\faiss_index_568.index\n",
      "‚úÖ FAISS index loaded in 0.00 seconds\n",
      "üìä Index contains 568 vectors (should be 568).\n",
      "üì• Loading document metadata from: faiss_vector_store_568\\document_metadata_568.json\n",
      "üì• Loading vector store metadata from: faiss_vector_store_568\\vector_store_metadata_568.json\n",
      "‚úÖ Loaded metadata for 568 documents.\n",
      "üöÄ Dense Retriever (568) initialized successfully!\n",
      "\n",
      "================================================================================\n",
      "ü§ñ INITIALIZING LLM GENERATOR\n",
      "================================================================================\n",
      "üì• Loading LLaMA GGUF model from: models/llama-3.2-1b-instruct-q4_k_m.gguf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (5000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM loaded in 2.28 seconds\n",
      "üöÄ LLM Generator initialized successfully!\n",
      "\n",
      "================================================================================\n",
      "‚úÖ RAG PIPELINE (568) READY!\n",
      "================================================================================\n",
      "\n",
      "üíæ Saving RAG pipeline (568) to pipelines\\rag_pipeline_568_v2.pkl...\n",
      "‚úÖ RAG pipeline (568) saved successfully!\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SETUP COMPLETE (568 DOCUMENTS)!\n",
      "The RAG pipeline for 568 original documents has been initialized and saved.\n",
      "  Saved to: pipelines\\rag_pipeline_568.pkl\n",
      "You can now run a query session using this pipeline.\n",
      "================================================================================\n",
      "\n",
      "Running a sample query with the initialized 568 pipeline...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'What are the advantages of private cloud?'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 132.1ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 3 relevant documents (min threshold: 0.75)\n",
      "\n",
      "üìä TOP 3 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: icc02_Advantages_r_Disadvantages_of_Private_Cloud_&_Hybrid_Cloud_Introduction.txt (Score: 0.8706)\n",
      "  2. File: icc03_Advantages_r_Disadvantages_of_Public_Cloud_&_Private_Cloud_Introduction.txt (Score: 0.8589)\n",
      "  3. File: icc42_Hybrid_Cloud_Use_Cases_Advantages_&_Disadvantages.txt (Score: 0.8561)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 73.21 seconds. Tokens: 230\n",
      "\n",
      "================================================================================\n",
      "Sample Query: What are the advantages of private cloud?\n",
      "Response:\n",
      "Based on the provided documents, the advantages of the private cloud model are:\n",
      "\n",
      "‚Ä¢ Better Control: In a private cloud, the organization is often the sole owner or exclusive user of the infrastructure. This grants complete command over service integration, IT operations, security policies, and user behavior. Organizations can fine-tune the environment to precisely match their operational workflows and governance requirements. [1]\n",
      "‚Ä¢ Data Security and Privacy: Private clouds are highly suitable for storing sensitive corporate information to which only authorized staff should have access. By segmenting resources within the same dedicated infrastructure (e.g., creating different virtual networks or storage partitions for different departments or sensitivity levels), improved access control and enhanced security can be achieved. [1]\n",
      "‚Ä¢ Customization: Unlike a public cloud deployment which offers standardized services, a private cloud allows a company to tailor its solution extensively to meet its specific needs. This includes customizing hardware configurations, network architectures, software stacks, and security measures to align perfectly with unique business processes or regulatory obligations. [1]\n",
      "\n",
      "These advantages highlight the benefits of the private cloud model, emphasizing control, data security, and customization, which are highly valued by certain organizations.\n",
      "\n",
      "Sources used: ['icc02_Advantages_r_Disadvantages_of_Private_Cloud_&_Hybrid_Cloud_Introduction.txt', 'icc03_Advantages_r_Disadvantages_of_Public_Cloud_&_Private_Cloud_Introduction.txt', 'icc42_Hybrid_Cloud_Use_Cases_Advantages_&_Disadvantages.txt']\n",
      "Total time: 73.35s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to import llama-cpp-python\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "    LLAMA_CPP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LLAMA_CPP_AVAILABLE = False\n",
    "    print(\"Warning: llama-cpp-python not installed. LLM generation will be simulated.\")\n",
    "    print(\"         Install with: pip install llama-cpp-python (or llama-cpp-python[cuda] for GPU)\")\n",
    "\n",
    "# Configuration for RAG pipeline targeting the 568 original documents\n",
    "@dataclass\n",
    "class RAGConfig568:\n",
    "    \"\"\"Configuration class for RAG pipeline using 568 original documents\"\"\"\n",
    "    # Paths pointing to the output of FAISSVectorStore568\n",
    "    # Assumes 'faiss_vector_store_568' is in the same directory as the script\n",
    "    base_faiss_path: str = \"faiss_vector_store_568\" # Base directory for the 568 FAISS store\n",
    "    \n",
    "    # Derived paths\n",
    "    faiss_index_path: str = field(init=False)\n",
    "    document_metadata_path: str = field(init=False) # Changed from chunk_metadata_path\n",
    "    vector_store_metadata_path: str = field(init=False)\n",
    "    \n",
    "    # Model paths\n",
    "    embedding_model_name: str = \"intfloat/e5-base\"\n",
    "    llm_model_path: str = \"models/llama-3.2-1b-instruct-q4_k_m.gguf\"  # IMPORTANT: Update this path if different\n",
    "    \n",
    "    # Retrieval parameters\n",
    "    top_k_dense: int = 3\n",
    "    similarity_threshold: float = 0.75 # Adjust based on testing\n",
    "    \n",
    "    # LLM parameters\n",
    "    max_tokens: int = 1200\n",
    "    temperature: float = 0.32 # Slightly increased for a bit more variability if desired\n",
    "    top_p: float = 0.92\n",
    "    context_length: int = 5000 # Max context for Llama 3.2 1B seems to be 8k, but conservative is fine\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.faiss_index_path = os.path.join(self.base_faiss_path, \"faiss_index_568.index\")\n",
    "        self.document_metadata_path = os.path.join(self.base_faiss_path, \"document_metadata_568.json\")\n",
    "        self.vector_store_metadata_path = os.path.join(self.base_faiss_path, \"vector_store_metadata_568.json\")\n",
    "\n",
    "class DenseRetriever568:\n",
    "    \"\"\"Handles dense retrieval for the 568 original documents\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig568):\n",
    "        self.config = config\n",
    "        self.embedding_model = None\n",
    "        self.faiss_index = None\n",
    "        self.document_metadata = None # Changed from chunk_metadata\n",
    "        self.vector_store_metadata = None\n",
    "        \n",
    "    def initialize(self):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üîÑ INITIALIZING DENSE RETRIEVER (FOR 568 ORIGINAL DOCUMENTS)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"üì• Loading embedding model (intfloat/e5-base)...\")\n",
    "        start_time = time.time()\n",
    "        self.embedding_model = SentenceTransformer(self.config.embedding_model_name)\n",
    "        print(f\"‚úÖ Embedding model loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        print(f\"üì• Loading FAISS index from: {self.config.faiss_index_path}\")\n",
    "        start_time = time.time()\n",
    "        if not os.path.exists(self.config.faiss_index_path):\n",
    "            raise FileNotFoundError(f\"FAISS index not found at: {self.config.faiss_index_path}\\n\"\n",
    "                                    f\"Ensure 'faiss_vector_store_568' folder is correctly placed.\")\n",
    "        self.faiss_index = faiss.read_index(self.config.faiss_index_path)\n",
    "        print(f\"‚úÖ FAISS index loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"üìä Index contains {self.faiss_index.ntotal} vectors (should be 568).\")\n",
    "        \n",
    "        print(f\"üì• Loading document metadata from: {self.config.document_metadata_path}\")\n",
    "        if not os.path.exists(self.config.document_metadata_path):\n",
    "            raise FileNotFoundError(f\"Document metadata not found at: {self.config.document_metadata_path}\\n\"\n",
    "                                    f\"Ensure 'faiss_vector_store_568' folder is correctly placed.\")\n",
    "        with open(self.config.document_metadata_path, 'r', encoding='utf-8') as f:\n",
    "            self.document_metadata = json.load(f)\n",
    "        \n",
    "        print(f\"üì• Loading vector store metadata from: {self.config.vector_store_metadata_path}\")\n",
    "        if not os.path.exists(self.config.vector_store_metadata_path):\n",
    "            print(f\"‚ö†Ô∏è Vector store metadata not found at: {self.config.vector_store_metadata_path}. Proceeding...\")\n",
    "            self.vector_store_metadata = {}\n",
    "        else:\n",
    "            with open(self.config.vector_store_metadata_path, 'r', encoding='utf-8') as f:\n",
    "                self.vector_store_metadata = json.load(f)\n",
    "            \n",
    "        print(f\"‚úÖ Loaded metadata for {len(self.document_metadata)} documents.\")\n",
    "        print(\"üöÄ Dense Retriever (568) initialized successfully!\")\n",
    "        \n",
    "    def embed_query(self, query: str) -> np.ndarray:\n",
    "        prefixed_query = f\"query: {query}\" # E5 specific prefix for queries\n",
    "        start_time = time.time()\n",
    "        embedding = self.embedding_model.encode([prefixed_query], normalize_embeddings=True).astype(np.float32)\n",
    "        embed_time_ms = (time.time() - start_time) * 1000\n",
    "        print(f\"‚ö° Query embedded in {embed_time_ms:.1f}ms\")\n",
    "        return embedding[0]\n",
    "    \n",
    "    def search_similar_documents(self, query_embedding: np.ndarray) -> List[Dict[str, Any]]: # Renamed\n",
    "        start_time = time.time()\n",
    "        scores, indices = self.faiss_index.search(\n",
    "            query_embedding.reshape(1, -1), \n",
    "            self.config.top_k_dense\n",
    "        )\n",
    "        search_time_ms = (time.time() - start_time) * 1000\n",
    "        print(f\"üîç FAISS search completed in {search_time_ms:.1f}ms\")\n",
    "        \n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if 0 <= idx < len(self.document_metadata) and score >= self.config.similarity_threshold:\n",
    "                doc_data = self.document_metadata[idx].copy()\n",
    "                doc_data['similarity_score'] = float(score)\n",
    "                doc_data['retrieval_rank'] = i + 1\n",
    "                results.append(doc_data)\n",
    "            elif score < self.config.similarity_threshold:\n",
    "                break\n",
    "        \n",
    "        print(f\"üìã Retrieved {len(results)} relevant documents (min threshold: {self.config.similarity_threshold:.2f})\")\n",
    "        return results\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Dict[str, Any]]:\n",
    "        print(f\"\\n\" + \"-\"*80)\n",
    "        print(f\"üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: '{query[:100]}{'...' if len(query) > 100 else ''}'\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        query_embedding = self.embed_query(query)\n",
    "        results = self.search_similar_documents(query_embedding) # Renamed\n",
    "        \n",
    "        if results:\n",
    "            print(f\"\\nüìä TOP {len(results)} RETRIEVED DOCUMENTS (Summary):\")\n",
    "            for i, result_doc in enumerate(results):\n",
    "                # Use 'file_name' as it's present in your document_metadata_568.json\n",
    "                file_name = result_doc.get('file_name', Path(result_doc.get('source_file', 'Unknown')).name)\n",
    "                print(f\"  {result_doc['retrieval_rank']}. File: {file_name} (Score: {result_doc['similarity_score']:.4f})\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è No documents retrieved for this query based on set parameters.\")\n",
    "        return results\n",
    "\n",
    "class LLMGenerator568: # Renamed for clarity, though functionality is general\n",
    "    \"\"\"Handles LLM generation using LLaMA 3.2-1B\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig568):\n",
    "        self.config = config\n",
    "        self.llm = None\n",
    "        \n",
    "    def initialize(self):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ü§ñ INITIALIZING LLM GENERATOR\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if not LLAMA_CPP_AVAILABLE:\n",
    "            print(\"‚ö†Ô∏è  llama-cpp-python not available. LLM generation will be simulated.\")\n",
    "            return\n",
    "        if not os.path.exists(self.config.llm_model_path):\n",
    "            print(f\"‚ö†Ô∏è  LLM model not found at: {self.config.llm_model_path}\")\n",
    "            print(\"üìù Please download a LLaMA GGUF model and update RAGConfig568.llm_model_path.\")\n",
    "            print(\"   Using simulation mode for LLM generation.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üì• Loading LLaMA GGUF model from: {self.config.llm_model_path}...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            self.llm = Llama(\n",
    "                model_path=self.config.llm_model_path,\n",
    "                n_ctx=self.config.context_length,\n",
    "                n_threads=os.cpu_count(),\n",
    "                verbose=False,\n",
    "                n_gpu_layers=-1 \n",
    "            )\n",
    "            print(f\"‚úÖ LLM loaded in {time.time() - start_time:.2f} seconds\")\n",
    "            print(\"üöÄ LLM Generator initialized successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading LLM: {e}. Using simulation mode.\")\n",
    "            self.llm = None # Ensure LLM is None if loading failed\n",
    "    \n",
    "    def format_prompt(self, query: str, retrieved_documents: List[Dict[str, Any]]) -> str: # Renamed\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(retrieved_documents): # Renamed\n",
    "            # Use 'file_name' for source, and 'content' for the text, as per your 568 doc structure\n",
    "            source_file_name = doc.get('file_name', Path(doc.get('source_file', 'Unknown Document')).name)\n",
    "            text = doc.get('content', '') # Key change: use 'content'\n",
    "            score = doc.get('similarity_score', 0.0)\n",
    "            context_parts.append(\n",
    "                f\"<document id={i+1} source=\\\"{source_file_name}\\\" relevance={score:.3f}>\\n{text}\\n</document>\"\n",
    "            )\n",
    "        context_string = \"\\n\".join(context_parts)\n",
    "        \n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an intelligent AI assistant for course materials. Your primary task is to answer questions accurately using ONLY the provided context documents.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. **BASE YOUR ANSWER ENTIRELY ON THE PROVIDED DOCUMENTS.** Do not use any external knowledge.\n",
    "2. **CITE SOURCES METICULOUSLY**: When you use information from a document, cite it immediately using the format [Document X: source_filename.ext].\n",
    "3. **BE SPECIFIC**: If a document provides a detail, include it.\n",
    "4. **NO RELEVANT INFORMATION**: If NO documents contain relevant information for the query, respond EXACTLY with: \"Based on the provided documents, I could not find specific information to answer your query.\" Do not add any other commentary.\n",
    "5. **SYNTHESIZE**: Combine information from multiple documents if they all contribute to the answer, citing each piece of information.\n",
    "6. **DIRECT QUOTES**: If a direct quote is useful and concise, you can use it, but always attribute it.\n",
    "7. **DO NOT HALLUCINATE**: If the information isn't in the documents, you don't know it for the purpose of this task.\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Context Documents:\n",
    "{context_string}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Based ONLY on the provided documents, answer the question. Cite your sources.\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def generate_response(self, query: str, retrieved_documents: List[Dict[str, Any]]) -> Dict[str, Any]: # Renamed\n",
    "        print(f\"\\n\" + \"-\"*80)\n",
    "        print(f\"ü§ñ GENERATION PHASE (568 Docs)\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        if not retrieved_documents:\n",
    "            print(\"‚ö†Ô∏è No relevant documents were retrieved. Rigid response will be generated.\")\n",
    "            return {\n",
    "                'response': \"Based on the provided documents, I could not find specific information to answer your query.\",\n",
    "                'sources': [], 'generation_time': 0, 'token_count': 0, 'simulated': True, 'no_context': True\n",
    "            }\n",
    "        \n",
    "        prompt = self.format_prompt(query, retrieved_documents)\n",
    "        \n",
    "        if self.llm is None:\n",
    "            print(\"üîÑ Simulating LLM response...\")\n",
    "            time.sleep(1.0)\n",
    "            sources_list = sorted(list(set(doc.get('file_name', 'Unknown') for doc in retrieved_documents)))\n",
    "            simulated_response = (f\"SIMULATED RESPONSE: Based on documents like {', '.join(sources_list[:2])}..., \"\n",
    "                                  f\"the answer to '{query}' would be synthesized here. \"\n",
    "                                  f\"LLM would cite sources like [Document 1: {sources_list[0] if sources_list else 'N/A'}].\")\n",
    "            print(f\"‚úÖ Simulated response generated.\")\n",
    "            return {\n",
    "                'response': simulated_response, 'sources': sources_list, 'generation_time': 1.0, \n",
    "                'token_count': len(simulated_response.split()), 'simulated': True\n",
    "            }\n",
    "        \n",
    "        print(f\"üß† Calling LLaMA GGUF model for generation...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            output = self.llm.create_completion(\n",
    "                prompt, max_tokens=self.config.max_tokens, temperature=self.config.temperature,\n",
    "                top_p=self.config.top_p, stop=[\"<|eot_id|>\"], echo=False\n",
    "            )\n",
    "            generation_time = time.time() - start_time\n",
    "            response_text = output['choices'][0]['text'].strip()\n",
    "            sources_list = sorted(list(set(doc.get('file_name', 'Unknown') for doc in retrieved_documents)))\n",
    "            print(f\"‚úÖ LLM Response generated in {generation_time:.2f} seconds. Tokens: {output['usage']['completion_tokens']}\")\n",
    "            return {\n",
    "                'response': response_text, 'sources': sources_list, 'generation_time': generation_time,\n",
    "                'token_count': output['usage']['completion_tokens'], 'simulated': False\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during LLM generation: {e}\")\n",
    "            return {\n",
    "                'response': f\"Error during LLM generation: {str(e)}\", 'sources': [], \n",
    "                'generation_time': 0, 'token_count': 0, 'error': True, 'simulated': False\n",
    "            }\n",
    "\n",
    "class RAGPipeline568: # Renamed\n",
    "    \"\"\"Complete RAG Pipeline for 568 original documents\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig568 = None):\n",
    "        self.config = config or RAGConfig568()\n",
    "        self.retriever = DenseRetriever568(self.config)\n",
    "        self.generator = LLMGenerator568(self.config) # Using the (potentially renamed) LLMGenerator\n",
    "        \n",
    "    def initialize(self):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üöÄ INITIALIZING COMPLETE RAG PIPELINE (FOR 568 ORIGINAL DOCUMENTS)\")\n",
    "        print(\"=\"*80)\n",
    "        self.retriever.initialize()\n",
    "        self.generator.initialize()\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ RAG PIPELINE (568) READY!\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def save_pipeline(self, filepath: str = \"rag_pipeline_568_v2.pkl\"): # Changed default filename\n",
    "        print(f\"\\nüíæ Saving RAG pipeline (568) to {filepath}...\")\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(filepath) or '.', exist_ok=True)\n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(self, f)\n",
    "            print(f\"‚úÖ RAG pipeline (568) saved successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving pipeline: {e}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_pipeline(cls, filepath: str = \"rag_pipeline_568_v2.pkl\"): # Changed default filename\n",
    "        print(f\"\\nüì• Loading RAG pipeline (568) from {filepath}...\")\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"‚ùå Pipeline file not found at {filepath}. Please run setup first.\")\n",
    "            return None\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                pipeline = pickle.load(f)\n",
    "            print(f\"‚úÖ RAG pipeline (568) loaded successfully!\")\n",
    "            return pipeline\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading pipeline: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        start_time = time.time()\n",
    "        retrieved_documents = self.retriever.retrieve(question) # Renamed\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        generation_start = time.time()\n",
    "        result = self.generator.generate_response(question, retrieved_documents) # Renamed\n",
    "        generation_time = time.time() - generation_start\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        final_result = {\n",
    "            'query': question,\n",
    "            'retrieved_documents_count': len(retrieved_documents), # Renamed\n",
    "            'retrieval_time': retrieval_time,\n",
    "            'generation_time': generation_time,\n",
    "            'total_time': total_time,\n",
    "            'response': result['response'],\n",
    "            'sources': result['sources'],\n",
    "            'document_details': retrieved_documents, # Renamed\n",
    "            'no_context': result.get('no_context', False)\n",
    "        }\n",
    "        return final_result\n",
    "\n",
    "def setup_rag_pipeline_568(): # Renamed\n",
    "    \"\"\"Setup and initialize the RAG pipeline for 568 original documents\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ RAG PIPELINE (568 DOCUMENTS) SETUP AND INITIALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # --- IMPORTANT LOCAL SETUP STEPS for 568 ---\n",
    "    # 1. Ensure required libraries are installed.\n",
    "    # 2. Download FAISS vector store folder:\n",
    "    #    Download 'faiss_vector_store_568' from your Google Drive and place it\n",
    "    #    in the same directory as this script (or update RAGConfig568.base_faiss_path).\n",
    "    # 3. Download LLaMA GGUF model and update RAGConfig568.llm_model_path.\n",
    "    # ---\n",
    "    \n",
    "    config = RAGConfig568() # Use the new config\n",
    "    rag = RAGPipeline568(config) # Use the new pipeline class\n",
    "    \n",
    "    try:\n",
    "        rag.initialize()\n",
    "        # Default save path for 568 pipeline\n",
    "        rag.save_pipeline(filepath=os.path.join(\"pipelines\", \"rag_pipeline_568_v2.pkl\")) # Save in a subfolder\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ SETUP COMPLETE (568 DOCUMENTS)!\")\n",
    "        print(\"The RAG pipeline for 568 original documents has been initialized and saved.\")\n",
    "        print(f\"  Saved to: {os.path.join('pipelines', 'rag_pipeline_568.pkl')}\")\n",
    "        print(\"You can now run a query session using this pipeline.\")\n",
    "        print(\"=\"*80)\n",
    "        return rag\n",
    "        \n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"\\n‚ùå CRITICAL FILE NOT FOUND during setup: {fnf_error}\")\n",
    "        print(\"   Please ensure all required FAISS store files are in the correct location:\")\n",
    "        print(f\"   Expected FAISS base path: {config.base_faiss_path}\")\n",
    "        print(f\"   - Index: {config.faiss_index_path}\")\n",
    "        print(f\"   - Document Metadata: {config.document_metadata_path}\")\n",
    "        print(\"   Also check your LLM model path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå An error occurred during setup: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This will set up and save the pipeline for the 568 original documents\n",
    "    pipeline_568 = setup_rag_pipeline_568()\n",
    "    \n",
    "    # Example of how you might run a query if setup was successful\n",
    "    if pipeline_568:\n",
    "        print(\"\\nRunning a sample query with the initialized 568 pipeline...\")\n",
    "        sample_query = \"What are the advantages of private cloud?\"\n",
    "        response = pipeline_568.query(sample_query)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Sample Query: {response['query']}\")\n",
    "        print(f\"Response:\\n{response['response']}\")\n",
    "        print(f\"\\nSources used: {response['sources']}\")\n",
    "        print(f\"Total time: {response['total_time']:.2f}s\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # To load and use later (in a different script or session):\n",
    "        # loaded_pipeline = RAGPipeline568.load_pipeline(filepath=os.path.join(\"pipelines\", \"rag_pipeline_568.pkl\"))\n",
    "        # if loaded_pipeline:\n",
    "        #     response = loaded_pipeline.query(\"Another query\")\n",
    "        #     print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33612195",
   "metadata": {},
   "source": [
    "## NOW write you query it will take generally 50 seconds to respond\n",
    "\n",
    "### Ask as many queries you want the output will properly display all the information, make sure to click scrollable element in output so that you able to see all the output parts \n",
    "\n",
    "### or copy the output of the cell and paste it to word if you donot know what scrollabel element is \n",
    "\n",
    "### Write exit when you want to stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb303f97",
   "metadata": {},
   "source": [
    "# max tokens 800 and collect 4 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b5cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: For this query script to successfully unpickle 'rag_pipeline_568.pkl',\n",
      "      the class definitions (RAGPipeline568, RAGConfig568, etc.) must be accessible\n",
      "      in this script's environment (e.g., defined here or imported from the setup script's module).\n",
      "      Proceeding with main_query_568 assuming they are available.\n",
      "\n",
      "================================================================================\n",
      "üöÄ RAG PIPELINE (568 DOCUMENTS) QUERY SESSION\n",
      "================================================================================\n",
      "\n",
      "üì• Loading RAG pipeline (568) from pipelines\\rag_pipeline_568.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4500) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG pipeline (568) loaded successfully!\n",
      "\n",
      "‚úÖ RAG pipeline (568) loaded and ready!\n",
      "\n",
      "================================================================================\n",
      "üöÄ RAG PIPELINE (568 DOCUMENTS) INTERACTIVE SESSION\n",
      "Type 'exit' or 'quit' to end the session.\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'tell me about the confusion matrix, accuracy, recall, precision and F1 score also explain with examp...'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 2033.3ms\n",
      "üîç FAISS search completed in 1.0ms\n",
      "üìã Retrieved 5 relevant documents (min threshold: 0.38)\n",
      "\n",
      "üìä TOP 5 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: lec3-(f) Evaluation measure.txt (Score: 0.8532)\n",
      "  2. File: lec3-(k) F-measure.txt (Score: 0.8378)\n",
      "  3. File: lec3-(g) Confusion Matrix.txt (Score: 0.8341)\n",
      "  4. File: lec3-(i) Precision vs recall.txt (Score: 0.8334)\n",
      "  5. File: lec3-(m) Multi-class Confusion Matrix.txt (Score: 0.8260)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 119.62 seconds. Tokens: 919\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Based on the provided documents, here's an explanation of the confusion matrix, accuracy, recall, precision, and F1 score, along with examples and their main purposes:\n",
      "\n",
      "**Confusion Matrix:**\n",
      "A confusion matrix is a table that helps identify where a classification model is making mistakes, also known as \"confusion\" between different classes. It has four parts:\n",
      "\n",
      "* **True Positive (TP):** The model correctly predicted the positive class when the actual class is positive.\n",
      "* **False Positive (FP):** The model predicted the positive class but the actual class is negative.\n",
      "* **False Negative (FN):** The model predicted the negative class but the actual class is positive.\n",
      "* **True Negative (TN):** The model correctly predicted the negative class when the actual class is negative.\n",
      "\n",
      "**Accuracy:**\n",
      "Accuracy is the sum of all correct predictions (true positives across classes) divided by total items. It's a measure of how well the model performs overall.\n",
      "\n",
      "**Recall:**\n",
      "Recall is the percentage of correct items that are successfully selected. It's a measure of how well the model identifies the positive class.\n",
      "\n",
      "**Precision:**\n",
      "Precision is the percentage of selected items that are actually correct. It's a measure of how well the model avoids false positives.\n",
      "\n",
      "**F1 Score (F1-Score):**\n",
      "The F1 score is a harmonic mean of precision and recall, balancing both metrics. It's particularly useful when there's an uneven class distribution.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Let's say we have a dataset with the following counts:\n",
      "\n",
      "* Actual urgent items predicted as urgent: 96\n",
      "* Actual urgent items predicted as normal: 3\n",
      "* Actual urgent items predicted as spam: 1\n",
      "* Actual normal items predicted as urgent: 4\n",
      "* Actual normal items predicted as normal: 89\n",
      "* Actual normal items predicted as spam: 7\n",
      "* Actual spam items predicted as urgent: 2\n",
      "* Actual spam items predicted as normal: 3\n",
      "* Actual spam items predicted as spam: 95\n",
      "\n",
      "To calculate precision and recall for each class:\n",
      "\n",
      "* Urgent precision: true positives are 96; false positives are predictions of urgent that were actually normal or spam (4 + 2). So, urgent precision = 96 / (96 + 4 + 2) = 96 / 102 = 0.936.\n",
      "* Urgent recall: false negatives are actual urgent items predicted as normal or spam (3 + 1). So, urgent recall = 3 / (3 + 1 + 4) = 3 / 8 = 0.375.\n",
      "* Normal precision: true positives are 89; false positives are predictions of normal that were actually urgent or spam (4 + 3). So, normal precision = 89 / (89 + 4 + 3) = 89 / 96 = 0.921.\n",
      "* Normal recall: false negatives are actual normal items predicted as urgent or spam (4 + 7). So, normal recall = 4 / (4 + 7 + 4) = 4 / 15 = 0.267.\n",
      "* Spam precision: true positives are 2; false positives are predictions of spam that were actually urgent or normal (1 + 3). So, spam precision = 2 / (2 + 1 + 3) = 2 / 6 = 0.333.\n",
      "* Spam recall: false negatives are actual spam items predicted as normal or urgent (3 + 2). So, spam recall = 3 / (3 + 2 + 3) = 3 / 8 = 0.375.\n",
      "\n",
      "**Main purpose of each:**\n",
      "\n",
      "* **Confusion Matrix:** To identify where the model is making mistakes and to understand the class distribution.\n",
      "* **Accuracy:** To evaluate the overall performance of the model.\n",
      "* **Recall:** To identify the positive class and to ensure that the model is not missing any actual positive instances.\n",
      "* **Precision:** To avoid false positives and to ensure that the model is not misclassifying any actual positive instances as negative.\n",
      "* **F1 Score (F1-Score):** To balance precision and recall and to provide a more accurate overall performance measure.\n",
      "\n",
      "In summary, the confusion matrix helps identify where the model is making mistakes, while accuracy and recall provide a more detailed understanding of the model's performance. Precision and F1 score provide a balanced view of the model's performance, taking into account both the ability to avoid false positives and false negatives.\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** lec3-(f) Evaluation measure.txt, lec3-(g) Confusion Matrix.txt, lec3-(i) Precision vs recall.txt, lec3-(k) F-measure.txt, lec3-(m) Multi-class Confusion Matrix.txt\n",
      "‚è±Ô∏è **Response time:** 121.65 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'tell me about 2 wars that muammad peace be upon him was a part of in his life and write a breif summ...'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 1746.4ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 5 relevant documents (min threshold: 0.38)\n",
      "\n",
      "üìä TOP 5 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: part_108_Life_in_Madinah_Three_Phases_of_the_Madinese_Era.txt (Score: 0.8049)\n",
      "  2. File: part_213_The_Second_Stage_New_Phase_of_Islamic_Action.txt (Score: 0.7979)\n",
      "  3. File: part_125_Pre_Badr_Missions_and_Early_Invasions_Part_1.txt (Score: 0.7929)\n",
      "  4. File: part_237_The_Third_Stage_and_Hunain_Ghazwah_Part1.txt (Score: 0.7921)\n",
      "  5. File: part_017_Rulership_in_Pan_Arabia_and_Political_Situation_Overview.txt (Score: 0.7896)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 121.77 seconds. Tokens: 518\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Based on the provided documents, Muhammad (peace be upon him) was involved in two wars:\n",
      "\n",
      "1. **The First War: Saif Al-Bahr Platoon**\n",
      "The first war mentioned in the documents is the Saif Al-Bahr Platoon, which took place in Ramadan 1 A.H. (623 A.D.). The platoon was sent by Hamzah bin 'Abdullah, and its task was to intercept a caravan belonging to Quraish. The caravan was led by Abu Jahl bin Hisham, and the platoon encountered each other in preparation for fighting. The Prophet (peace be upon him) accredited the first flag in the history of Muslims, which was white in color, and entrusted it to Kinaz bin Husain Al-Ghanawi.\n",
      "\n",
      "2. **The Second War: Hunain Ghazwah**\n",
      "The second war mentioned in the documents is Hunain Ghazwah, which took place in Safar 2 A.H. (623 A.D.). The war was led by Malik bin 'Awf An-Nasri, and the enemy was the Hawazin and Thaqif tribes. The enemy's march and encampment at Awtas was a strategic location, and Malik bin 'Awf decided to march and fight the Muslims. He made his people take their wealth, women, and children with them to Awtas, which is a valley in Hawazin land. The war was a decisive victory for the Muslims, and it affected the course of events in Arabia.\n",
      "\n",
      "Important parts of these wars include:\n",
      "\n",
      "* The Prophet (peace be upon him) placed a decisive war with Quraish as a first priority on his agenda shortly after the endorsement of the Hudaibiyah Peace Treaty.\n",
      "* Malik bin 'Awf An-Nasri's decision to march and fight the Muslims at Awtas, which was a strategic location, and the decision to make his people take their wealth, women, and children with them.\n",
      "* The Prophet (peace be upon him) having a non-aggression pact with 'Amr bin Makhshi Ad-Damari, which was a document from Muhammad (peace be upon him) concerning Bani Damrah in which he established them safe and secure in their wealth and lives.\n",
      "* The Prophet (peace be upon him) having a war experienced man, Duraid bin As-Simmah, who was well-known as a war-experienced man, and who was among those who gathered round Malik bin 'Awf An-Nasri at Awtas.\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** part_017_Rulership_in_Pan_Arabia_and_Political_Situation_Overview.txt, part_108_Life_in_Madinah_Three_Phases_of_the_Madinese_Era.txt, part_125_Pre_Badr_Missions_and_Early_Invasions_Part_1.txt, part_213_The_Second_Stage_New_Phase_of_Islamic_Action.txt, part_237_The_Third_Stage_and_Hunain_Ghazwah_Part1.txt\n",
      "‚è±Ô∏è **Response time:** 123.53 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'tell me all about the vector model, its steps and exaplin with example'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 78.5ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 5 relevant documents (min threshold: 0.38)\n",
      "\n",
      "üìä TOP 5 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: lec6-(f) Vector Space Models (VSMs).txt (Score: 0.8254)\n",
      "  2. File: icc39_High_Availability_for_Writes_and_Recovering_from_Permanent_Failures.txt (Score: 0.8093)\n",
      "  3. File: lec1-(a) Natural Language Processing.txt (Score: 0.8068)\n",
      "  4. File: lec4-(e) N-gram Model.txt (Score: 0.8049)\n",
      "  5. File: icc45_Introduction_to_Web_Services_&_Early_Evolution.txt (Score: 0.8042)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚ùå Error during LLM generation: Requested tokens (5068) exceed context window of 4512\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Error during LLM generation: Requested tokens (5068) exceed context window of 4512\n",
      "--------------------------------------------------\n",
      "‚è±Ô∏è **Response time:** 0.09 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'tell me abou soap and rest'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 101.3ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 5 relevant documents (min threshold: 0.38)\n",
      "\n",
      "üìä TOP 5 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: icc77_Soap_vs_rest.txt (Score: 0.8226)\n",
      "  2. File: icc45_Introduction_to_Web_Services_&_Early_Evolution.txt (Score: 0.8159)\n",
      "  3. File: icc01_API_Evolution_Data_Formats_&_The_Emergence_of_Standards_SOAP_&_REST.txt (Score: 0.8036)\n",
      "  4. File: icc66_RESTful_Architecture_Principles_Verbs_and_Resource_Design.txt (Score: 0.7997)\n",
      "  5. File: icc46_Key_Technologies_in_SOA_SOAP_WSDL_JSON_HTTP_&_SOAP_vs_JSON.txt (Score: 0.7984)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 106.34 seconds. Tokens: 261\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Based on the provided documents, here's an answer to your question:\n",
      "\n",
      "SOAP (Simple Object Access Protocol) and REST (Representational State Transfer) are two architectural styles for designing networked applications.\n",
      "\n",
      "SOAP is a protocol for exchanging structured data via XML, introduced in 1999. It provides a standardized messaging framework for enterprise-level applications requiring strict security, reliability, and transaction management. SOAP is considered heavyweight due to its complexity and reliance on XML.\n",
      "\n",
      "REST, on the other hand, is an architectural style for designing networked applications, introduced by Roy Fielding in 2000. It is a lightweight, flexible, and scalable alternative to SOAP, prioritizing simplicity and broad interoperability. REST is suitable for modern web and mobile applications, particularly those requiring ease of use and scalability.\n",
      "\n",
      "SOAP and REST have different design principles, such as client-server separation, statelessness, cacheability, and uniform interface. SOAP is more complex and heavyweight, while REST is simpler and more scalable.\n",
      "\n",
      "SOAP is often used in enterprise environments with stringent security, reliability, and transaction requirements, whereas REST is preferred for applications requiring simplicity and broad interoperability.\n",
      "\n",
      "In summary, SOAP is a protocol for structured data exchange via XML, while REST is an architectural style for designing networked applications, prioritizing simplicity, scalability, and interoperability.\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** icc01_API_Evolution_Data_Formats_&_The_Emergence_of_Standards_SOAP_&_REST.txt, icc45_Introduction_to_Web_Services_&_Early_Evolution.txt, icc46_Key_Technologies_in_SOA_SOAP_WSDL_JSON_HTTP_&_SOAP_vs_JSON.txt, icc66_RESTful_Architecture_Principles_Verbs_and_Resource_Design.txt, icc77_Soap_vs_rest.txt\n",
      "‚è±Ô∏è **Response time:** 106.45 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'tell me some ahadees mention in and tell thier summary.'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 116.3ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 5 relevant documents (min threshold: 0.38)\n",
      "\n",
      "üìä TOP 5 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: lec2-(f) Summarization.txt (Score: 0.8031)\n",
      "  2. File: lec2-(d) Question Answering.txt (Score: 0.7975)\n",
      "  3. File: lec1-(c) Common Applications.txt (Score: 0.7958)\n",
      "  4. File: lec2-(c) Information Retrieval.txt (Score: 0.7887)\n",
      "  5. File: part_017_Rulership_in_Pan_Arabia_and_Political_Situation_Overview.txt (Score: 0.7834)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 65.94 seconds. Tokens: 291\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Based on the provided documents, here are some ahadees (Prophetic traditions) that mention the topic of rship in Pan-Arabia:\n",
      "\n",
      "1. The ahadees mention that the Arabian tribes were divided into two groups: the Qahtanide and 'Adnanide emigrations, and division of Arabia between these two tribes. This division was formal rather than actual, and those living in the Syrian semi-desert were under the domain of the Arabian Ghassanide king. (Source: \"rship in Pan-Arabia\" section)\n",
      "2. The ahadees also mention that the heads of tribes enjoyed dictatorial privileges similar to those of kings, and were rendered full obedience and subordination in both war and peace. This is similar to the system of autocratic rulership that was prevalent in Arabia. (Source: \"rship in Pan-Arabia\" section)\n",
      "3. The ahadees also mention that the rulers of Hijaz were considered as rulers and servants of the religious centre. They ruled among the Arabs in the name of religious leadership and always monopolized the custodianship of the Holy Sanctuary and its neighbourhood. (Source: \"rship in Pan-Arabia\" section)\n",
      "\n",
      "These ahadees provide a summary of the topic of rship in Pan-Arabia, which is characterized by autocratic rulership, division between tribes, and the dominance of certain tribes over others.\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** lec1-(c) Common Applications.txt, lec2-(c) Information Retrieval.txt, lec2-(d) Question Answering.txt, lec2-(f) Summarization.txt, part_017_Rulership_in_Pan_Arabia_and_Political_Situation_Overview.txt\n",
      "‚è±Ô∏è **Response time:** 66.06 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Goodbye! üëã\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import os # Added for os.path.join\n",
    "\n",
    "# Define the expected path for the 568 pipeline\n",
    "# Assuming the query script is in the same root directory as the 'pipelines' folder\n",
    "DEFAULT_PIPELINE_568_PATH = os.path.join(\"pipelines\", \"rag_pipeline_568.pkl\")\n",
    "\n",
    "def load_rag_pipeline_568(filepath: str = DEFAULT_PIPELINE_568_PATH): # Renamed and updated default\n",
    "    \"\"\"Load a pre-initialized RAG pipeline (568 version) from file\"\"\"\n",
    "    print(f\"\\nüì• Loading RAG pipeline (568) from {filepath}...\")\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            # This will require RAGPipeline568 and its dependent classes to be defined\n",
    "            # or importable in the environment where this script is run.\n",
    "            # If running as a standalone script, you might need to include class definitions\n",
    "            # or ensure they are in an importable module.\n",
    "            pipeline = pickle.load(f)\n",
    "        print(f\"‚úÖ RAG pipeline (568) loaded successfully!\")\n",
    "        return pipeline\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Pipeline file not found at {filepath}\")\n",
    "        print(\"   Please run the setup script (e.g., rag_setup_568.py) first to initialize and save the pipeline.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # More details on error\n",
    "        return None\n",
    "\n",
    "def display_query_result_568(result: Dict[str, Any]): # Renamed\n",
    "    \"\"\"Display the results of a RAG query (568 version) in a formatted way\"\"\"\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"FINAL RAG RESPONSE FOR: '{result['query']}'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìù Generated Response:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['response'])\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Display details of the retrieved documents\n",
    "    # The key in 'result' should be 'document_details' as per your updated RAGPipeline568\n",
    "    if result.get('document_details'):\n",
    "        print(f\"\\nüìö Retrieved Documents (Top {result.get('retrieved_documents_count', 0)}):\") # Updated key\n",
    "        print(\"-\" * 70)\n",
    "        for j, doc in enumerate(result['document_details']): # Updated variable name\n",
    "            # Use 'file_name' directly as it's expected in the document metadata\n",
    "            file_name = doc.get('file_name', 'Unknown_File') \n",
    "            print(f\"  {j+1}. Document: {file_name}\")\n",
    "            print(f\"     Category: {doc.get('category', 'N/A')}\")\n",
    "            print(f\"     Relevance Score: {doc.get('similarity_score', 0):.4f}\")\n",
    "            # Preview of the text used for embedding, if available\n",
    "            if 'text_for_embedding' in doc:\n",
    "                preview_text = doc['text_for_embedding']\n",
    "            elif 'content' in doc: # Fallback to 'content'\n",
    "                preview_text = doc['content']\n",
    "            else:\n",
    "                preview_text = doc.get('text', '') # Last fallback\n",
    "            # print(f\"     Preview: {preview_text[:150]}...\")\n",
    "            print(\"-\" * 70)\n",
    "    elif result.get('no_context'):\n",
    "        print(\"\\n‚ö†Ô∏è No relevant documents were retrieved to answer this query based on the provided context.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No document details found in the result.\")\n",
    "\n",
    "    \n",
    "    print(f\"\\nüìä Pipeline Metrics:\")\n",
    "    print(f\"  ‚Ä¢ Retrieval Time: {result.get('retrieval_time', 0):.3f} seconds\")\n",
    "    print(f\"  ‚Ä¢ Generation Time: {result.get('generation_time', 0):.3f} seconds\")\n",
    "    print(f\"  ‚Ä¢ Total Time: {result.get('total_time', 0):.3f} seconds\")\n",
    "    if result.get('simulated_llm_response'): # If your pipeline adds this flag\n",
    "        print(\"  ‚ö†Ô∏è LLM Response was SIMULATED.\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "def run_interactive_session_568(rag_pipeline): # Renamed\n",
    "    \"\"\"Run an interactive query session with the loaded RAG pipeline (568 version)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ RAG PIPELINE (568 DOCUMENTS) INTERACTIVE SESSION\")\n",
    "    print(\"Type 'exit' or 'quit' to end the session.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        user_query = input(\"ü§î Your question: \").strip()\n",
    "        \n",
    "        if user_query.lower() in ['exit', 'quit']:\n",
    "            print(\"\\nGoodbye! üëã\")\n",
    "            break\n",
    "        if not user_query:\n",
    "            print(\"‚ö†Ô∏è Please enter a question.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(\"\\nüîÑ Processing your question...\")\n",
    "            result = rag_pipeline.query(user_query) # This calls RAGPipeline568.query\n",
    "            \n",
    "            print(f\"\\nüí° **Answer:**\")\n",
    "            print(\"-\" * 50)\n",
    "            print(result['response'])\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            if result.get('sources'):\n",
    "                print(f\"\\nüìö **Sources cited (original files):** {', '.join(result['sources'])}\")\n",
    "            elif result.get('no_context'):\n",
    "                 print(f\"\\nüìö No specific documents found to answer the query based on provided context.\")\n",
    "\n",
    "\n",
    "            print(f\"‚è±Ô∏è **Response time:** {result.get('total_time', 0):.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during query processing: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def run_single_query_568(rag_pipeline, query: str): # Renamed\n",
    "    \"\"\"Run a single query (568 version) without interactive mode\"\"\"\n",
    "    print(f\"\\nüîç Processing single query: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        result = rag_pipeline.query(query)\n",
    "        display_query_result_568(result) # Use the 568-specific display\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing query: {e}\")\n",
    "        return None\n",
    "\n",
    "def main_query_568(): # Renamed\n",
    "    \"\"\"Main function for the query session (568 version)\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ RAG PIPELINE (568 DOCUMENTS) QUERY SESSION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # --- Ensure class definitions are available ---\n",
    "    # If RAGPipeline568 and its dependencies (RAGConfig568, DenseRetriever568, LLMGenerator568)\n",
    "    # are not in this file, they need to be imported from the setup script.\n",
    "    # e.g., from rag_setup_568_script import RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568\n",
    "    # For simplicity if running as a standalone file after setup, you might copy those class defs here\n",
    "    # OR ensure the setup script creates a module you can import.\n",
    "\n",
    "    # Load the pre-initialized 568 pipeline\n",
    "    rag_pipeline_568 = load_rag_pipeline_568() # Use the 568-specific loader\n",
    "    \n",
    "    if rag_pipeline_568 is None:\n",
    "        print(\"\\n‚ùå Could not load RAG pipeline for 568 documents.\")\n",
    "        print(\"   Please ensure you have run the setup script for the 568-document pipeline,\")\n",
    "        print(f\"   and the file '{DEFAULT_PIPELINE_568_PATH}' exists.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n‚úÖ RAG pipeline (568) loaded and ready!\")\n",
    "    \n",
    "    # Example: Run a few predefined queries or start interactive session\n",
    "    # run_single_query_568(rag_pipeline_568, \"What is cloud computing security?\")\n",
    "    # run_single_query_568(rag_pipeline_568, \"Explain the concept of APIs in NLP.\")\n",
    "    \n",
    "    run_interactive_session_568(rag_pipeline_568) # Use the 568-specific session runner\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Crucial for unpickling custom objects:\n",
    "    # The definitions of RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568\n",
    "    # must be known in the main scope when pickle.load() is called.\n",
    "    # If these are in rag_setup_script.py, you would typically do:\n",
    "    # from rag_setup_script import RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568 # (Adjust script name)\n",
    "    # For this example, assuming these classes are either in this file or correctly imported.\n",
    "    \n",
    "    # If you are running this as a separate script and the classes are in another file (e.g. rag_setup_568.py)\n",
    "    # you would need to import them. For example:\n",
    "    # from rag_setup_568 import RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568\n",
    "    # If they are not imported, pickle will fail to reconstruct the objects.\n",
    "    \n",
    "    # For testing, if this script is run standalone AFTER the setup script has created the .pkl file,\n",
    "    # and the class definitions are present in this file (or an imported module), it should work.\n",
    "    # For now, I'll assume the class definitions would be made available (e.g. by copying them here or importing)\n",
    "    print(\"NOTE: For this query script to successfully unpickle 'rag_pipeline_568.pkl',\")\n",
    "    print(\"      the class definitions (RAGPipeline568, RAGConfig568, etc.) must be accessible\")\n",
    "    print(\"      in this script's environment (e.g., defined here or imported from the setup script's module).\")\n",
    "    print(\"      Proceeding with main_query_568 assuming they are available.\\n\")\n",
    "\n",
    "    # To actually run this, you'd make sure the class definitions are here or imported.\n",
    "    # For now, this call will likely fail unless you've set up imports or copied classes.\n",
    "    # --- Placeholder for demonstrating the call ---\n",
    "    # To make it runnable as a standalone demo after setup, you'd typically copy the class definitions\n",
    "    # from the setup script into this query script, or ensure the setup script can be imported as a module.\n",
    "\n",
    "    # Let's assume for a moment you've copied the class definitions from the setup script into this file\n",
    "    # (RAGConfig568, DenseRetriever568, LLMGenerator568, RAGPipeline568)\n",
    "    # Then the following would be the intended execution:\n",
    "    main_query_568()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc00ccd",
   "metadata": {},
   "source": [
    "# MAX token 800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01393c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: For this query script to successfully unpickle 'rag_pipeline_568.pkl',\n",
      "      the class definitions (RAGPipeline568, RAGConfig568, etc.) must be accessible\n",
      "      in this script's environment (e.g., defined here or imported from the setup script's module).\n",
      "      Proceeding with main_query_568 assuming they are available.\n",
      "\n",
      "================================================================================\n",
      "üöÄ RAG PIPELINE (568 DOCUMENTS) QUERY SESSION\n",
      "================================================================================\n",
      "\n",
      "üì• Loading RAG pipeline (568) from pipelines\\rag_pipeline_568_v2.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (5000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG pipeline (568) loaded successfully!\n",
      "\n",
      "‚úÖ RAG pipeline (568) loaded and ready!\n",
      "\n",
      "================================================================================\n",
      "üöÄ RAG PIPELINE (568 DOCUMENTS) INTERACTIVE SESSION\n",
      "Type 'exit' or 'quit' to end the session.\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'Define Vector Model, its steps and explain with example''\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 197.6ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 3 relevant documents (min threshold: 0.75)\n",
      "\n",
      "üìä TOP 3 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: lec6-(f) Vector Space Models (VSMs).txt (Score: 0.8376)\n",
      "  2. File: lec4-(e) N-gram Model.txt (Score: 0.8220)\n",
      "  3. File: lec1-(a) Natural Language Processing.txt (Score: 0.8141)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 220.50 seconds. Tokens: 800\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Vector Space Model (VSM) is a way to represent text documents as mathematical vectors, which are essentially lists of numbers. It is a fundamental concept in Natural Language Processing (NLP) and Information Retrieval (IR) tasks.\n",
      "\n",
      "**Vector Space Model (VSM) Steps:**\n",
      "\n",
      "1. **Collect the text data**: Gather a large corpus of text documents.\n",
      "2. **Clean the text**: Preprocess the text by tokenizing (breaking down) it into individual words or tokens, lowercasing them, and removing common words that don't carry significant meaning.\n",
      "3. **Build a vocabulary**: Create a comprehensive list of unique words that remain after the cleaning process across all documents in the corpus.\n",
      "4. **Create vectors**: Represent each document as a numerical vector, where each element in the vector corresponds to a word in the vocabulary. The value of each element can be determined by:\n",
      "\t* **Binary**: 1 if the word is present in the document, 0 if absent.\n",
      "\t* **Term Frequency (TF)**: The count of how often a word appears in the document, possibly normalized by the total number of words in that document.\n",
      "\t* **Term Frequency-Inverse Document Frequency (TF-IDF)**: A weighted score that gives higher importance to words that are frequent in a specific document but rare across the entire corpus.\n",
      "5. **Calculate similarity**: Use a similarity measure (like cosine similarity) to quantify how close or related document vectors are to each other in the vector space.\n",
      "6. **Search/query**: For search applications, a user's query is converted into a vector, and this vector is compared against all document vectors in the corpus using the chosen similarity measure to find the most relevant documents.\n",
      "\n",
      "**Vector Space Model (VSM) Strengths:**\n",
      "\n",
      "* Flexible: Can be applied to various text analysis tasks and data types.\n",
      "* Scalable: Handles large collections of documents and extensive vocabularies.\n",
      "* Effective at measuring similarity: Good at determining the degree of relatedness between two pieces of text based on shared terms.\n",
      "* Language independent: The core mechanism of counting words and creating vectors doesn't rely on deep grammatical rules of a specific language, making it broadly applicable.\n",
      "\n",
      "**Vector Space Model (VSM) Weaknesses:**\n",
      "\n",
      "* No context understanding (Bag-of-Words assumption): Treats words as independent units, ignoring word order and grammatical structure.\n",
      "* Struggles with phrases or sentence meaning: Doesn't inherently capture multi-word expressions, idioms, or overall semantic meaning of a sentence beyond the sum of its words.\n",
      "* Semantic drift and polysemy problems: May struggle with changes in word meanings over time or with polysemy (words with multiple meanings).\n",
      "\n",
      "**Example: Vector Space Model with 3 documents**\n",
      "\n",
      "Suppose we have three documents:\n",
      "\n",
      "* Doc1: \"I love Pakistan\"\n",
      "* Doc2: \"Pakistan Zindabad\"\n",
      "* Doc3: \"I am Pakistan\"\n",
      "\n",
      "Using the vocabulary and TF-IDF, we can create vectors for each document:\n",
      "\n",
      "V_Doc1 = [1/3, 1/3, 1/3, 0, 0]\n",
      "V_Doc2 = [0, 0, 1/2, 1/2, 0]\n",
      "V_Doc3 = [1/3, 0, 1/3, 0, 1/3]\n",
      "\n",
      "We can calculate the cosine similarity between Doc1 and Doc3:\n",
      "\n",
      "Cosine Similarity(V_Doc1, V_Doc3) = (1/3 * 1/3) / (sqrt(1/3^2 + 1/3^2 + 1/3^2) * sqrt(1/3^2))\n",
      "\n",
      "= (1/9) / (sqrt(3/9) * sqrt(1/3))\n",
      "\n",
      "= 1/9 / (sqrt(3/9) * sqrt(1/3))\n",
      "\n",
      "=\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** lec1-(a) Natural Language Processing.txt, lec4-(e) N-gram Model.txt, lec6-(f) Vector Space Models (VSMs).txt\n",
      "‚è±Ô∏è **Response time:** 220.70 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'Tell me some interesting Facts about the lIfe of Prophet Muhammad peace be upon him'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 185.8ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 3 relevant documents (min threshold: 0.75)\n",
      "\n",
      "üìä TOP 3 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: part_045_Prophet_Muhammad_Pre_Revelation_Meditation_and_First_Encounter_with_Gabriel.txt (Score: 0.8113)\n",
      "  2. File: part_082_Early_Individual_Converts_Tufail_bin_Amr_Ad_Dausi.txt (Score: 0.8073)\n",
      "  3. File: part_033_The_Prophetic_Family_Hashim.txt (Score: 0.8049)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 95.58 seconds. Tokens: 625\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Based on the provided documents, here are some interesting facts about the life of Prophet Muhammad (peace be upon him):\n",
      "\n",
      "1. **Prophet Muhammad's love for solitude**: Prophet Muhammad (peace be upon him) was known to spend long hours meditating and speculating over creation around him, which helped him to widen the mental gap between him and his compatriots. (Source: [part_045_Prophet_Muhammad_Pre_Revelation_Meditation_and_First_Encounter_with_Gabriel.txt](https://example.com/document1) [part_082_Early_Individual_Converts_Tufail_bin_Amr_Ad_Dausi.txt](https://example.com/document2) [part_033_The_Prophetic_Family_Hashim.txt) [Rahmat Al-lil-alameen 1/47; Ibn Hisham 1/235,236; Fi Zilal Al-Qur'an 29/166])\n",
      "2. **His love for sharing with others**: Prophet Muhammad (peace be upon him) used to provide himself with Sawiq (barley porridge) and water and then head for the hills and ravines in the neighborhood of Makkah to invite wayfarers to share him his modest provision. (Source: [part_045_Prophet_Muhammad_Pre_Revelation_Meditation_and_First_Encounter_with_Gabriel.txt](https://example.com/document1) [part_082_Early_Individual_Converts_Tufail_bin_Amr_Ad_Dausi.txt](https://example.com/document2) [part_033_The_Prophetic_Family_Hashim.txt))\n",
      "3. **His concern for the moral evils**: Prophet Muhammad (peace be upon him) was restless about the moral evils and idolatry that were rampant among his people, and he used to invite wayfarers to share his modest provision and worship and meditation on the universe around him. (Source: [part_045_Prophet_Muhammad_Pre_Revelation_Meditation_and_First_Encounter_with_Gabriel.txt](https://example.com/document1) [part_082_Early_Individual_Converts_Tufail_bin_Amr_Ad_Dausi.txt](https://example.com/document2) [part_033_The_Prophetic_Family_Hashim.txt])\n",
      "4. **His influence on his people**: Prophet Muhammad (peace be upon him) was an influential man among his people, and he called his father and wife to embrace Islam, and they did respond. His people lagged a little but he exhorted them fervently and was fully successful. (Source: [part_082_Early_Individual_Converts_Tufail_bin_Amr_Ad_Dausi.txt](https://example.com/document2) [part_033_The_Prophetic_Family_Hashim.txt] [Rahmat Al-lil-alameen 1/47; Ibn Hisham 1/235,236; Fi Zilal Al-Qur'an 29/166])\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** part_033_The_Prophetic_Family_Hashim.txt, part_045_Prophet_Muhammad_Pre_Revelation_Meditation_and_First_Encounter_with_Gabriel.txt, part_082_Early_Individual_Converts_Tufail_bin_Amr_Ad_Dausi.txt\n",
      "‚è±Ô∏è **Response time:** 95.77 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Goodbye! üëã\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import os # Added for os.path.join\n",
    "\n",
    "# Define the expected path for the 568 pipeline\n",
    "# Assuming the query script is in the same root directory as the 'pipelines' folder\n",
    "DEFAULT_PIPELINE_568_PATH = os.path.join(\"pipelines\", \"rag_pipeline_568_v2.pkl\")\n",
    "\n",
    "def load_rag_pipeline_568(filepath: str = DEFAULT_PIPELINE_568_PATH): # Renamed and updated default\n",
    "    \"\"\"Load a pre-initialized RAG pipeline (568 version) from file\"\"\"\n",
    "    print(f\"\\nüì• Loading RAG pipeline (568) from {filepath}...\")\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            # This will require RAGPipeline568 and its dependent classes to be defined\n",
    "            # or importable in the environment where this script is run.\n",
    "            # If running as a standalone script, you might need to include class definitions\n",
    "            # or ensure they are in an importable module.\n",
    "            pipeline = pickle.load(f)\n",
    "        print(f\"‚úÖ RAG pipeline (568) loaded successfully!\")\n",
    "        return pipeline\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Pipeline file not found at {filepath}\")\n",
    "        print(\"   Please run the setup script (e.g., rag_setup_568.py) first to initialize and save the pipeline.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # More details on error\n",
    "        return None\n",
    "\n",
    "def display_query_result_568(result: Dict[str, Any]): # Renamed\n",
    "    \"\"\"Display the results of a RAG query (568 version) in a formatted way\"\"\"\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"FINAL RAG RESPONSE FOR: '{result['query']}'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìù Generated Response:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['response'])\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Display details of the retrieved documents\n",
    "    # The key in 'result' should be 'document_details' as per your updated RAGPipeline568\n",
    "    if result.get('document_details'):\n",
    "        print(f\"\\nüìö Retrieved Documents (Top {result.get('retrieved_documents_count', 0)}):\") # Updated key\n",
    "        print(\"-\" * 70)\n",
    "        for j, doc in enumerate(result['document_details']): # Updated variable name\n",
    "            # Use 'file_name' directly as it's expected in the document metadata\n",
    "            file_name = doc.get('file_name', 'Unknown_File') \n",
    "            print(f\"  {j+1}. Document: {file_name}\")\n",
    "            print(f\"     Category: {doc.get('category', 'N/A')}\")\n",
    "            print(f\"     Relevance Score: {doc.get('similarity_score', 0):.4f}\")\n",
    "            # Preview of the text used for embedding, if available\n",
    "            if 'text_for_embedding' in doc:\n",
    "                preview_text = doc['text_for_embedding']\n",
    "            elif 'content' in doc: # Fallback to 'content'\n",
    "                preview_text = doc['content']\n",
    "            else:\n",
    "                preview_text = doc.get('text', '') # Last fallback\n",
    "            # print(f\"     Preview: {preview_text[:150]}...\")\n",
    "            print(\"-\" * 70)\n",
    "    elif result.get('no_context'):\n",
    "        print(\"\\n‚ö†Ô∏è No relevant documents were retrieved to answer this query based on the provided context.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No document details found in the result.\")\n",
    "\n",
    "    \n",
    "    print(f\"\\nüìä Pipeline Metrics:\")\n",
    "    print(f\"  ‚Ä¢ Retrieval Time: {result.get('retrieval_time', 0):.3f} seconds\")\n",
    "    print(f\"  ‚Ä¢ Generation Time: {result.get('generation_time', 0):.3f} seconds\")\n",
    "    print(f\"  ‚Ä¢ Total Time: {result.get('total_time', 0):.3f} seconds\")\n",
    "    if result.get('simulated_llm_response'): # If your pipeline adds this flag\n",
    "        print(\"  ‚ö†Ô∏è LLM Response was SIMULATED.\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "def run_interactive_session_568(rag_pipeline): # Renamed\n",
    "    \"\"\"Run an interactive query session with the loaded RAG pipeline (568 version)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ RAG PIPELINE (568 DOCUMENTS) INTERACTIVE SESSION\")\n",
    "    print(\"Type 'exit' or 'quit' to end the session.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        user_query = input(\"ü§î Your question: \").strip()\n",
    "        \n",
    "        if user_query.lower() in ['exit', 'quit']:\n",
    "            print(\"\\nGoodbye! üëã\")\n",
    "            break\n",
    "        if not user_query:\n",
    "            print(\"‚ö†Ô∏è Please enter a question.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(\"\\nüîÑ Processing your question...\")\n",
    "            result = rag_pipeline.query(user_query) # This calls RAGPipeline568.query\n",
    "            \n",
    "            print(f\"\\nüí° **Answer:**\")\n",
    "            print(\"-\" * 50)\n",
    "            print(result['response'])\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            if result.get('sources'):\n",
    "                print(f\"\\nüìö **Sources cited (original files):** {', '.join(result['sources'])}\")\n",
    "            elif result.get('no_context'):\n",
    "                 print(f\"\\nüìö No specific documents found to answer the query based on provided context.\")\n",
    "\n",
    "\n",
    "            print(f\"‚è±Ô∏è **Response time:** {result.get('total_time', 0):.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during query processing: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def run_single_query_568(rag_pipeline, query: str): # Renamed\n",
    "    \"\"\"Run a single query (568 version) without interactive mode\"\"\"\n",
    "    print(f\"\\nüîç Processing single query: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        result = rag_pipeline.query(query)\n",
    "        display_query_result_568(result) # Use the 568-specific display\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing query: {e}\")\n",
    "        return None\n",
    "\n",
    "def main_query_568(): # Renamed\n",
    "    \"\"\"Main function for the query session (568 version)\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ RAG PIPELINE (568 DOCUMENTS) QUERY SESSION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # --- Ensure class definitions are available ---\n",
    "    # If RAGPipeline568 and its dependencies (RAGConfig568, DenseRetriever568, LLMGenerator568)\n",
    "    # are not in this file, they need to be imported from the setup script.\n",
    "    # e.g., from rag_setup_568_script import RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568\n",
    "    # For simplicity if running as a standalone file after setup, you might copy those class defs here\n",
    "    # OR ensure the setup script creates a module you can import.\n",
    "\n",
    "    # Load the pre-initialized 568 pipeline\n",
    "    rag_pipeline_568 = load_rag_pipeline_568() # Use the 568-specific loader\n",
    "    \n",
    "    if rag_pipeline_568 is None:\n",
    "        print(\"\\n‚ùå Could not load RAG pipeline for 568 documents.\")\n",
    "        print(\"   Please ensure you have run the setup script for the 568-document pipeline,\")\n",
    "        print(f\"   and the file '{DEFAULT_PIPELINE_568_PATH}' exists.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n‚úÖ RAG pipeline (568) loaded and ready!\")\n",
    "    \n",
    "    # Example: Run a few predefined queries or start interactive session\n",
    "    # run_single_query_568(rag_pipeline_568, \"What is cloud computing security?\")\n",
    "    # run_single_query_568(rag_pipeline_568, \"Explain the concept of APIs in NLP.\")\n",
    "    \n",
    "    run_interactive_session_568(rag_pipeline_568) # Use the 568-specific session runner\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Crucial for unpickling custom objects:\n",
    "    # The definitions of RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568\n",
    "    # must be known in the main scope when pickle.load() is called.\n",
    "    # If these are in rag_setup_script.py, you would typically do:\n",
    "    # from rag_setup_script import RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568 # (Adjust script name)\n",
    "    # For this example, assuming these classes are either in this file or correctly imported.\n",
    "    \n",
    "    # If you are running this as a separate script and the classes are in another file (e.g. rag_setup_568.py)\n",
    "    # you would need to import them. For example:\n",
    "    # from rag_setup_568 import RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568\n",
    "    # If they are not imported, pickle will fail to reconstruct the objects.\n",
    "    \n",
    "    # For testing, if this script is run standalone AFTER the setup script has created the .pkl file,\n",
    "    # and the class definitions are present in this file (or an imported module), it should work.\n",
    "    # For now, I'll assume the class definitions would be made available (e.g. by copying them here or importing)\n",
    "    print(\"NOTE: For this query script to successfully unpickle 'rag_pipeline_568.pkl',\")\n",
    "    print(\"      the class definitions (RAGPipeline568, RAGConfig568, etc.) must be accessible\")\n",
    "    print(\"      in this script's environment (e.g., defined here or imported from the setup script's module).\")\n",
    "    print(\"      Proceeding with main_query_568 assuming they are available.\\n\")\n",
    "\n",
    "    # To actually run this, you'd make sure the class definitions are here or imported.\n",
    "    # For now, this call will likely fail unless you've set up imports or copied classes.\n",
    "    # --- Placeholder for demonstrating the call ---\n",
    "    # To make it runnable as a standalone demo after setup, you'd typically copy the class definitions\n",
    "    # from the setup script into this query script, or ensure the setup script can be imported as a module. \n",
    "\n",
    "    # Let's assume for a moment you've copied the class definitions from the setup script into this file\n",
    "    # (RAGConfig568, DenseRetriever568, LLMGenerator568, RAGPipeline568)\n",
    "    # Then the following would be the intended execution:\n",
    "    main_query_568()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada9d0d",
   "metadata": {},
   "source": [
    "# max token 1200 and 3 files collectedd results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc20603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: For this query script to successfully unpickle 'rag_pipeline_568.pkl',\n",
      "      the class definitions (RAGPipeline568, RAGConfig568, etc.) must be accessible\n",
      "      in this script's environment (e.g., defined here or imported from the setup script's module).\n",
      "      Proceeding with main_query_568 assuming they are available.\n",
      "\n",
      "================================================================================\n",
      "üöÄ RAG PIPELINE (568 DOCUMENTS) QUERY SESSION\n",
      "================================================================================\n",
      "\n",
      "üì• Loading RAG pipeline (568) from pipelines\\rag_pipeline_568_v2.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (5000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG pipeline (568) loaded successfully!\n",
      "\n",
      "‚úÖ RAG pipeline (568) loaded and ready!\n",
      "\n",
      "================================================================================\n",
      "üöÄ RAG PIPELINE (568 DOCUMENTS) INTERACTIVE SESSION\n",
      "Type 'exit' or 'quit' to end the session.\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'Define Vector Model, its steps and explain with example'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 268.2ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 3 relevant documents (min threshold: 0.75)\n",
      "\n",
      "üìä TOP 3 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: lec6-(f) Vector Space Models (VSMs).txt (Score: 0.8453)\n",
      "  2. File: lec4-(e) N-gram Model.txt (Score: 0.8299)\n",
      "  3. File: lec1-(a) Natural Language Processing.txt (Score: 0.8217)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 177.79 seconds. Tokens: 689\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Vector Space Model (VSM) is a way to represent text documents as mathematical vectors, which are essentially lists of numbers.\n",
      "\n",
      "Vector Space Models (VSMs) are a type of Natural Language Processing (NLP) technique used to represent text documents as numerical vectors, allowing for comparing, analyzing, or manipulating text using mathematical operations and algorithms.\n",
      "\n",
      "The steps to build a VSM are:\n",
      "\n",
      "1. **Collect the text data**: Gather the documents, sentences, or text snippets you want to model.\n",
      "2. **Clean the text**: Preprocess the text by tokenizing (breaking down) it into individual words or tokens, lowercasing, removing stop words (common words like \"is\", \"a\", etc.), and stemming (reducing words to their base form).\n",
      "3. **Build a vocabulary**: Create a comprehensive list of unique words (terms) that remain after cleaning the text.\n",
      "4. **Create vectors**: Represent each document as a numerical vector, where each element corresponds to a word in the vocabulary. The value of each element can be determined using one of the following methods:\n",
      "\t* **Binary**: 1 if the word is present in the document, 0 if absent.\n",
      "\t* **Term Frequency (TF)**: The count of how often a word appears in the document, possibly normalized by the total number of words in that document.\n",
      "\t* **Term Frequency-Inverse Document Frequency (TF-IDF)**: A weighted score that gives higher importance to words that are frequent in a specific document but rare across the entire corpus.\n",
      "5. **Calculate similarity**: Use a similarity measure (like cosine similarity) to quantify how close or related document vectors are to each other.\n",
      "6. **Search/query**: For search applications, a user's query is also converted into a vector. This query vector is then compared against all document vectors in the corpus using the chosen similarity measure to find the most relevant documents.\n",
      "\n",
      "Vector Space Models are used in various NLP tasks, including:\n",
      "\n",
      "* Finding similar documents or texts\n",
      "* Classifying documents into categories\n",
      "* Summarizing content\n",
      "* Identifying topics\n",
      "* Translating languages\n",
      "* Any task where understanding the semantic relationship or comparing text content is important.\n",
      "\n",
      "The strengths of VSMs include:\n",
      "\n",
      "* Flexibility: Can be applied to a wide range of text analysis tasks and different types of textual data.\n",
      "* Scalability: Can handle large collections of documents and extensive vocabularies.\n",
      "* Effectiveness at measuring similarity: Generally good at determining the degree of relatedness between two pieces of text based on shared terms.\n",
      "* Language independence: The core mechanism of counting words and creating vectors doesn't rely on deep grammatical rules of a specific language, making it broadly applicable.\n",
      "\n",
      "However, VSMs also have some weaknesses:\n",
      "\n",
      "* No context understanding (Bag-of-Words assumption): Treats words as independent units, ignoring the order of words and grammatical structure.\n",
      "* Struggles with phrases or sentence meaning: Doesn't capture the meaning of multi-word expressions, idioms, or the overall semantic meaning of a sentence beyond the sum of its words.\n",
      "* Semantic drift and polysemy problems: Semantic drift (words changing meaning over time) and polysemy (words with multiple meanings) can lead to ambiguous or muddled vector representations.\n",
      "\n",
      "Vector Space Models are widely used in various applications, such as speech recognition, machine translation, spelling correction, text generation, and more.\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** lec1-(a) Natural Language Processing.txt, lec4-(e) N-gram Model.txt, lec6-(f) Vector Space Models (VSMs).txt\n",
      "‚è±Ô∏è **Response time:** 178.06 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'what is private cliud and What are the advantages of private cloud?''\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 2133.9ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 3 relevant documents (min threshold: 0.75)\n",
      "\n",
      "üìä TOP 3 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: icc02_Advantages_r_Disadvantages_of_Private_Cloud_&_Hybrid_Cloud_Introduction.txt (Score: 0.8619)\n",
      "  2. File: icc62_Private_Cloud_Details_&_Use_Cases.txt (Score: 0.8577)\n",
      "  3. File: icc03_Advantages_r_Disadvantages_of_Public_Cloud_&_Private_Cloud_Introduction.txt (Score: 0.8567)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 69.63 seconds. Tokens: 338\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Based on the provided documents, a **private cloud** is a computing environment that is owned and operated by a single business or organization, providing **distinct benefits**, particularly centered around **control and security**. The private cloud model offers several advantages, including:\n",
      "\n",
      "* Better Control: The organization is often the sole owner or exclusive user of the infrastructure, granting complete command over service integration, IT operations, security policies, and user behavior. (Document 1)\n",
      "* Data Security and Privacy: Private clouds are suitable for storing sensitive corporate information, and by segmenting resources within the same dedicated infrastructure, improved access control and enhanced security can be achieved. (Document 1)\n",
      "* Customization: Unlike a public cloud deployment, a private cloud allows a company to tailor its solution extensively to meet its specific needs, including customizing hardware configurations, network architectures, software stacks, and security measures. (Document 2)\n",
      "* Scalability: Private clouds are generally more scalable than public clouds, with fewer overall resources available, and scaling often requires manual intervention or longer procurement cycles for new hardware. (Document 2)\n",
      "\n",
      "However, the private cloud model also has some disadvantages, including:\n",
      "\n",
      "* Less Scalable (Compared to Public): Private clouds are typically scaled within a predefined range of resources, which can limit their elasticity. (Document 1)\n",
      "* Costly: Private clouds are generally more costly to implement and maintain than public cloud services, especially for smaller organizations. (Document 1)\n",
      "\n",
      "The private cloud model is often preferred by organizations with strict security, compliance, or control requirements, or those handling highly sensitive data. Examples of use cases include large hospitals, banks, government agencies, and multinational corporations.\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** icc02_Advantages_r_Disadvantages_of_Private_Cloud_&_Hybrid_Cloud_Introduction.txt, icc03_Advantages_r_Disadvantages_of_Public_Cloud_&_Private_Cloud_Introduction.txt, icc62_Private_Cloud_Details_&_Use_Cases.txt\n",
      "‚è±Ô∏è **Response time:** 71.76 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'tell me some interesting facts about the lif eof prophet muhamad peace be upon him'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 126.1ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 3 relevant documents (min threshold: 0.75)\n",
      "\n",
      "üìä TOP 3 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: part_184_An_Errand_led_by_Abdullah_bin_Unais.txt (Score: 0.8024)\n",
      "  2. File: part_306_Prophets_Attributes_and_Manners_Perfection_of_Soul_and_Nobility.txt (Score: 0.8007)\n",
      "  3. File: part_045_Prophet_Muhammad_Pre_Revelation_Meditation_and_First_Encounter_with_Gabriel.txt (Score: 0.8002)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 180.81 seconds. Tokens: 499\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Based on the provided documents, here are some interesting facts about the life of Prophet Muhammad (peace be upon him):\n",
      "\n",
      "1. **He was a skilled and generous host**: When Prophet Muhammad (peace be upon him) was traveling with his companions, he would often slaughter a sheep for them, even if he didn't have to. (Document 1: Part 1, Source: \"The Perfection of Soul and Nobility\")\n",
      "2. **He was a skilled speaker and communicator**: Prophet Muhammad (peace be upon him) was known for his eloquence and fluency in Arabic, and he was often sought out for advice and consultation by his companions. (Document 2: \"The Perfection of Soul and Nobility\")\n",
      "3. **He was modest and humble**: Prophet Muhammad (peace be upon him) was known for his humility and modesty, and he never sought to draw attention to himself or seek to elevate himself above others. (Document 3: \"In the Shade of the Message and Prophethood\")\n",
      "4. **He was a skilled mediator and peacemaker**: Prophet Muhammad (peace be upon him) was known for his ability to bring people together and resolve conflicts, and he often used his calm and gentle demeanor to soothe disputes. (Document 2: \"The Perfection of Soul and Nobility\")\n",
      "5. **He was a devoted student of the Quran**: Prophet Muhammad (peace be upon him) was a diligent student of the Quran and spent many hours meditating and reflecting on its teachings. (Document 1: \"The Perfection of Soul and Nobility\")\n",
      "6. **He was a skilled teacher and mentor**: Prophet Muhammad (peace be upon him) was known for his ability to teach and mentor others, and he often shared his knowledge and wisdom with his companions. (Document 3: \"In the Shade of the Message and Prophethood\")\n",
      "7. **He was a kind and compassionate person**: Prophet Muhammad (peace be upon him) was known for his kindness and compassion, and he was often sought out for help and support by his companions. (Document 2: \"The Perfection of Soul and Nobility\")\n",
      "8. **He was a skilled storyteller**: Prophet Muhammad (peace be upon him) was known for his ability to tell engaging and entertaining stories, and he often used his storytelling skills to teach and educate others. (Document 1: \"The Perfection of Soul and Nobility\")\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** part_045_Prophet_Muhammad_Pre_Revelation_Meditation_and_First_Encounter_with_Gabriel.txt, part_184_An_Errand_led_by_Abdullah_bin_Unais.txt, part_306_Prophets_Attributes_and_Manners_Perfection_of_Soul_and_Nobility.txt\n",
      "‚è±Ô∏è **Response time:** 180.94 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ Processing your question...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üîç RETRIEVAL PHASE (568 Docs) FOR QUERY: 'what is semantic analysis'\n",
      "--------------------------------------------------------------------------------\n",
      "‚ö° Query embedded in 125.9ms\n",
      "üîç FAISS search completed in 0.0ms\n",
      "üìã Retrieved 3 relevant documents (min threshold: 0.75)\n",
      "\n",
      "üìä TOP 3 RETRIEVED DOCUMENTS (Summary):\n",
      "  1. File: lec2-(j) Discourse Analysis.txt (Score: 0.8379)\n",
      "  2. File: lec2-(k) Pragmatic Analysis.txt (Score: 0.8156)\n",
      "  3. File: lec2-(i) Sentiment Analysis.txt (Score: 0.8098)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ü§ñ GENERATION PHASE (568 Docs)\n",
      "--------------------------------------------------------------------------------\n",
      "üß† Calling LLaMA GGUF model for generation...\n",
      "‚úÖ LLM Response generated in 17.41 seconds. Tokens: 20\n",
      "\n",
      "üí° **Answer:**\n",
      "--------------------------------------------------\n",
      "Based on the provided documents, I could not find specific information to answer your question about Semantic Analysis.\n",
      "--------------------------------------------------\n",
      "\n",
      "üìö **Sources cited (original files):** lec2-(i) Sentiment Analysis.txt, lec2-(j) Discourse Analysis.txt, lec2-(k) Pragmatic Analysis.txt\n",
      "‚è±Ô∏è **Response time:** 17.55 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Goodbye! üëã\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import os # Added for os.path.join\n",
    "\n",
    "# Define the expected path for the 568 pipeline\n",
    "# Assuming the query script is in the same root directory as the 'pipelines' folder\n",
    "DEFAULT_PIPELINE_568_PATH = os.path.join(\"pipelines\", \"rag_pipeline_568_v2.pkl\")\n",
    "\n",
    "def load_rag_pipeline_568(filepath: str = DEFAULT_PIPELINE_568_PATH): # Renamed and updated default\n",
    "    \"\"\"Load a pre-initialized RAG pipeline (568 version) from file\"\"\"\n",
    "    print(f\"\\nüì• Loading RAG pipeline (568) from {filepath}...\")\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            # This will require RAGPipeline568 and its dependent classes to be defined\n",
    "            # or importable in the environment where this script is run.\n",
    "            # If running as a standalone script, you might need to include class definitions\n",
    "            # or ensure they are in an importable module.\n",
    "            pipeline = pickle.load(f)\n",
    "        print(f\"‚úÖ RAG pipeline (568) loaded successfully!\")\n",
    "        return pipeline\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Pipeline file not found at {filepath}\")\n",
    "        print(\"   Please run the setup script (e.g., rag_setup_568.py) first to initialize and save the pipeline.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # More details on error\n",
    "        return None\n",
    "\n",
    "def display_query_result_568(result: Dict[str, Any]): # Renamed\n",
    "    \"\"\"Display the results of a RAG query (568 version) in a formatted way\"\"\"\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"FINAL RAG RESPONSE FOR: '{result['query']}'\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìù Generated Response:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(result['response'])\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Display details of the retrieved documents\n",
    "    # The key in 'result' should be 'document_details' as per your updated RAGPipeline568\n",
    "    if result.get('document_details'):\n",
    "        print(f\"\\nüìö Retrieved Documents (Top {result.get('retrieved_documents_count', 0)}):\") # Updated key\n",
    "        print(\"-\" * 70)\n",
    "        for j, doc in enumerate(result['document_details']): # Updated variable name\n",
    "            # Use 'file_name' directly as it's expected in the document metadata\n",
    "            file_name = doc.get('file_name', 'Unknown_File') \n",
    "            print(f\"  {j+1}. Document: {file_name}\")\n",
    "            print(f\"     Category: {doc.get('category', 'N/A')}\")\n",
    "            print(f\"     Relevance Score: {doc.get('similarity_score', 0):.4f}\")\n",
    "            # Preview of the text used for embedding, if available\n",
    "            if 'text_for_embedding' in doc:\n",
    "                preview_text = doc['text_for_embedding']\n",
    "            elif 'content' in doc: # Fallback to 'content'\n",
    "                preview_text = doc['content']\n",
    "            else:\n",
    "                preview_text = doc.get('text', '') # Last fallback\n",
    "            # print(f\"     Preview: {preview_text[:150]}...\")\n",
    "            print(\"-\" * 70)\n",
    "    elif result.get('no_context'):\n",
    "        print(\"\\n‚ö†Ô∏è No relevant documents were retrieved to answer this query based on the provided context.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No document details found in the result.\")\n",
    "\n",
    "    \n",
    "    print(f\"\\nüìä Pipeline Metrics:\")\n",
    "    print(f\"  ‚Ä¢ Retrieval Time: {result.get('retrieval_time', 0):.3f} seconds\")\n",
    "    print(f\"  ‚Ä¢ Generation Time: {result.get('generation_time', 0):.3f} seconds\")\n",
    "    print(f\"  ‚Ä¢ Total Time: {result.get('total_time', 0):.3f} seconds\")\n",
    "    if result.get('simulated_llm_response'): # If your pipeline adds this flag\n",
    "        print(\"  ‚ö†Ô∏è LLM Response was SIMULATED.\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "def run_interactive_session_568(rag_pipeline): # Renamed\n",
    "    \"\"\"Run an interactive query session with the loaded RAG pipeline (568 version)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ RAG PIPELINE (568 DOCUMENTS) INTERACTIVE SESSION\")\n",
    "    print(\"Type 'exit' or 'quit' to end the session.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        user_query = input(\"ü§î Your question: \").strip()\n",
    "        \n",
    "        if user_query.lower() in ['exit', 'quit']:\n",
    "            print(\"\\nGoodbye! üëã\")\n",
    "            break\n",
    "        if not user_query:\n",
    "            print(\"‚ö†Ô∏è Please enter a question.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            print(\"\\nüîÑ Processing your question...\")\n",
    "            result = rag_pipeline.query(user_query) # This calls RAGPipeline568.query\n",
    "            \n",
    "            print(f\"\\nüí° **Answer:**\")\n",
    "            print(\"-\" * 50)\n",
    "            print(result['response'])\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            if result.get('sources'):\n",
    "                print(f\"\\nüìö **Sources cited (original files):** {', '.join(result['sources'])}\")\n",
    "            elif result.get('no_context'):\n",
    "                 print(f\"\\nüìö No specific documents found to answer the query based on provided context.\")\n",
    "\n",
    "\n",
    "            print(f\"‚è±Ô∏è **Response time:** {result.get('total_time', 0):.2f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during query processing: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def run_single_query_568(rag_pipeline, query: str): # Renamed\n",
    "    \"\"\"Run a single query (568 version) without interactive mode\"\"\"\n",
    "    print(f\"\\nüîç Processing single query: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        result = rag_pipeline.query(query)\n",
    "        display_query_result_568(result) # Use the 568-specific display\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing query: {e}\")\n",
    "        return None\n",
    "\n",
    "def main_query_568(): # Renamed\n",
    "    \"\"\"Main function for the query session (568 version)\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ RAG PIPELINE (568 DOCUMENTS) QUERY SESSION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # --- Ensure class definitions are available ---\n",
    "    # If RAGPipeline568 and its dependencies (RAGConfig568, DenseRetriever568, LLMGenerator568)\n",
    "    # are not in this file, they need to be imported from the setup script.\n",
    "    # e.g., from rag_setup_568_script import RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568\n",
    "    # For simplicity if running as a standalone file after setup, you might copy those class defs here\n",
    "    # OR ensure the setup script creates a module you can import.\n",
    "\n",
    "    # Load the pre-initialized 568 pipeline\n",
    "    rag_pipeline_568 = load_rag_pipeline_568() # Use the 568-specific loader\n",
    "    \n",
    "    if rag_pipeline_568 is None:\n",
    "        print(\"\\n‚ùå Could not load RAG pipeline for 568 documents.\")\n",
    "        print(\"   Please ensure you have run the setup script for the 568-document pipeline,\")\n",
    "        print(f\"   and the file '{DEFAULT_PIPELINE_568_PATH}' exists.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n‚úÖ RAG pipeline (568) loaded and ready!\")\n",
    "    \n",
    "    # Example: Run a few predefined queries or start interactive session\n",
    "    # run_single_query_568(rag_pipeline_568, \"What is cloud computing security?\")\n",
    "    # run_single_query_568(rag_pipeline_568, \"Explain the concept of APIs in NLP.\")\n",
    "    \n",
    "    run_interactive_session_568(rag_pipeline_568) # Use the 568-specific session runner\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Crucial for unpickling custom objects:\n",
    "    # The definitions of RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568\n",
    "    # must be known in the main scope when pickle.load() is called.\n",
    "    # If these are in rag_setup_script.py, you would typically do:\n",
    "    # from rag_setup_script import RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568 # (Adjust script name)\n",
    "    # For this example, assuming these classes are either in this file or correctly imported.\n",
    "    \n",
    "    # If you are running this as a separate script and the classes are in another file (e.g. rag_setup_568.py)\n",
    "    # you would need to import them. For example:\n",
    "    # from rag_setup_568 import RAGPipeline568, RAGConfig568, DenseRetriever568, LLMGenerator568\n",
    "    # If they are not imported, pickle will fail to reconstruct the objects.\n",
    "    \n",
    "    # For testing, if this script is run standalone AFTER the setup script has created the .pkl file,\n",
    "    # and the class definitions are present in this file (or an imported module), it should work.\n",
    "    # For now, I'll assume the class definitions would be made available (e.g. by copying them here or importing)\n",
    "    print(\"NOTE: For this query script to successfully unpickle 'rag_pipeline_568.pkl',\")\n",
    "    print(\"      the class definitions (RAGPipeline568, RAGConfig568, etc.) must be accessible\")\n",
    "    print(\"      in this script's environment (e.g., defined here or imported from the setup script's module).\")\n",
    "    print(\"      Proceeding with main_query_568 assuming they are available.\\n\")\n",
    "\n",
    "    # To actually run this, you'd make sure the class definitions are here or imported.\n",
    "    # For now, this call will likely fail unless you've set up imports or copied classes.\n",
    "    # --- Placeholder for demonstrating the call ---\n",
    "    # To make it runnable as a standalone demo after setup, you'd typically copy the class definitions\n",
    "    # from the setup script into this query script, or ensure the setup script can be imported as a module. \n",
    "\n",
    "    # Let's assume for a moment you've copied the class definitions from the setup script into this file\n",
    "    # (RAGConfig568, DenseRetriever568, LLMGenerator568, RAGPipeline568)\n",
    "    # Then the following would be the intended execution:\n",
    "    main_query_568()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
