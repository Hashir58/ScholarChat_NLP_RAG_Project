[
  {
    "document_id": 87,
    "document_hash": "7843527de633",
    "content": "What is NLP?\n\nNatural Language Processing (NLP) is a field of Artificial Intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language.\n\nIt combines linguistics (how language works) with computer science to create intelligent systems that can process natural (human) languages like English, Urdu, Chinese, etc.\n\nSimple Real-World Examples\n\nExample 1: Chatbot\nYou type: “How are you?”\nThe computer replies: “I’m doing great, thank you!”\nThis shows NLP understanding your input and generating a human-like response.\n\nExample 2: Translation\nYou enter: “Good morning”\nNLP translates it to: “صباح الخير” (Arabic)\nHere, the system understands the meaning and rewrites it in another language.\n\nExample 3: Sentiment Analysis\nYou write a review: “The product is terrible.”\nThe system reads your words and detects negative emotion.\nIt understands the feeling behind the words.\n\nExample 4: Voice Assistant\nYou say: “Play some music.”\nYour device understands your voice and starts playing songs.\nThis combines speech recognition and NLP to carry out your request.\n\nExample 5: Email Filtering\nYou receive lots of emails.\nNLP helps automatically move spam messages to the spam folder, and organize others into categories like “Promotions,” “Updates,” etc.\n\nExample 6: Search Engine\nYou type: “Best pizza in Islamabad”\nThe system understands what you're looking for and shows nearby pizza places.\nIt doesn’t just look for keywords — it understands the intent.",
    "enhanced_text": "[NLP] What is NLP?\n\nNatural Language Processing (NLP) is a field of Artificial Intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language.\n\nIt combines linguistics (how language works) with computer science to create intelligent systems that can process natural (human) languages like English, Urdu, Chinese, etc.\n\nSimple Real-World Examples\n\nExample 1: Chatbot\nYou type: “How are you?”\nThe computer replies: “I’m doing great, thank you!”\nThis shows NLP understanding your input and generating a human-like response.\n\nExample 2: Translation\nYou enter: “Good morning”\nNLP translates it to: “صباح الخير” (Arabic)\nHere, the system understands the meaning and rewrites it in another language.\n\nExample 3: Sentiment Analysis\nYou write a review: “The product is terrible.”\nThe system reads your words and detects negative emotion.\nIt understands the feeling behind the words.\n\nExample 4: Voice Assistant\nYou say: “Play some music.”\nYour device understands your voice and starts playing songs.\nThis combines speech recognition and NLP to carry out your request.\n\nExample 5: Email Filtering\nYou receive lots of emails.\nNLP helps automatically move spam messages to the spam folder, and organize others into categories like “Promotions,” “Updates,” etc.\n\nExample 6: Search Engine\nYou type: “Best pizza in Islamabad”\nThe system understands what you're looking for and shows nearby pizza places.\nIt doesn’t just look for keywords — it understands the intent.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(a) Natural Language Processing.txt",
    "file_name": "lec1-(a) Natural Language Processing.txt",
    "filename_keywords": [
      "language",
      "natural",
      "processing",
      "lec1"
    ],
    "content_keywords": [
      "chatbot\nyou",
      "search engine\nyou",
      "islamabad",
      "play",
      "how",
      "voice assistant\nyou",
      "email filtering\nyou",
      "best",
      "world examples\n\nexample",
      "example",
      "updates",
      "chinese",
      "natural language processing",
      "simple real",
      "this",
      "nlp",
      "good",
      "arabic",
      "urdu",
      "english",
      "your",
      "the",
      "promotions",
      "sentiment analysis\nyou",
      "here",
      "translation\nyou",
      "what",
      "artificial intelligence"
    ],
    "technical_terms": [
      "chatbot\nyou",
      "search engine\nyou",
      "islamabad",
      "play",
      "how",
      "voice assistant\nyou",
      "email filtering\nyou",
      "best",
      "world examples\n\nexample",
      "example",
      "updates",
      "chinese",
      "natural language processing",
      "simple real",
      "this",
      "nlp",
      "good",
      "arabic",
      "urdu",
      "english",
      "your",
      "the",
      "promotions",
      "sentiment analysis\nyou",
      "here",
      "translation\nyou",
      "what",
      "artificial intelligence"
    ],
    "all_keywords": [
      "language",
      "chatbot\nyou",
      "search engine\nyou",
      "islamabad",
      "play",
      "how",
      "natural",
      "voice assistant\nyou",
      "processing",
      "email filtering\nyou",
      "best",
      "world examples\n\nexample",
      "example",
      "updates",
      "chinese",
      "natural language processing",
      "simple real",
      "this",
      "nlp",
      "good",
      "lec1",
      "arabic",
      "urdu",
      "english",
      "your",
      "the",
      "promotions",
      "sentiment analysis\nyou",
      "here",
      "translation\nyou",
      "what",
      "artificial intelligence"
    ],
    "keyword_string": "language chatbot\nyou search engine\nyou islamabad play how natural voice assistant\nyou processing email filtering\nyou best world examples\n\nexample example updates chinese natural language processing simple real this nlp good lec1 arabic urdu english your the promotions sentiment analysis\nyou here translation\nyou what artificial intelligence",
    "token_count": 322,
    "word_count": 224,
    "sentence_count": 13,
    "paragraph_count": 10,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6956521739130435,
    "avg_sentence_length": 17.23076923076923,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 88,
    "document_hash": "95de7d2b4e7a",
    "content": "Why is NLP Confusing (Even for Computers)?\n\nNatural Language (NL), or human language, is incredibly complex, messy, and full of exceptions. Even humans sometimes struggle to understand it—especially when learning a new language. So, for computers that don’t think or feel like humans, understanding language becomes even more challenging.\n\nKey Reasons Why NLP is Confusing for Computers:\n\n1. Language is Ambiguous\nWords or sentences can have multiple meanings, depending on how and where they’re used.\n\nExample:\n\n“He saw the man with the telescope.”\nDoes this mean he used a telescope to see the man or the man had a telescope?\nA computer can’t easily decide without context.\n\n2. Same Words, Different Meanings (Polysemy)\nMany words in English have multiple meanings.\n\nExample:\n\n“Bank” can mean a financial institution or the side of a river.\nThe sentence “I’m going to the bank” isn’t clear unless the full context is known.\n\n3. Different Words, Same Meaning (Synonyms)\nDifferent words can mean the same thing, but not always in the exact same way.\n\nExample:\n\n“Begin” and “Start” are similar, but sometimes only one fits naturally in a sentence.\n\n“Buy” and “Purchase” are similar, but “purchase” sounds more formal.\n\n4. Language Depends on Context\nThe meaning of a sentence often depends on what was said before, who is speaking, or where they are.\n\nExample:\n\n“Can you pass me the salt?”\nLiterally, it’s a question about ability, but really, it’s a polite request.\n\n5. Idioms and Expressions\nPhrases don’t always mean what the words say.\n\nExample:\n\n“Kick the bucket” means to die, not to literally kick something.\n\nComputers struggle with this because they analyze words literally.\n\n6. Word Order Matters\nA small change in word order can change the entire meaning.\n\nExample:\n\n“The dog bit the man.”\n\n“The man bit the dog.”\nSame words, but completely different meanings.\n\n7. Grammar Rules Have Many Exceptions\nThere are rules in language, but there are lots of exceptions.\n\nExample:\n\n“Go → Went” is an irregular past tense.\nComputers have to learn these exceptions separately — they’re not obvious.\n\nFunny Example of Language Confusion:\n\n“Time flies like an arrow.”\n“Fruit flies like a banana.”\n\nThe first one means time moves quickly, but the second sounds like a joke — suggesting fruit flies enjoy bananas. Same structure, totally different meaning. This is very confusing for a machine!",
    "enhanced_text": "[NLP] Why is NLP Confusing (Even for Computers)?\n\nNatural Language (NL), or human language, is incredibly complex, messy, and full of exceptions. Even humans sometimes struggle to understand it—especially when learning a new language. So, for computers that don’t think or feel like humans, understanding language becomes even more challenging.\n\nKey Reasons Why NLP is Confusing for Computers:\n\n1. Language is Ambiguous\nWords or sentences can have multiple meanings, depending on how and where they’re used.\n\nExample:\n\n“He saw the man with the telescope.”\nDoes this mean he used a telescope to see the man or the man had a telescope?\nA computer can’t easily decide without context.\n\n2. Same Words, Different Meanings (Polysemy)\nMany words in English have multiple meanings.\n\nExample:\n\n“Bank” can mean a financial institution or the side of a river.\nThe sentence “I’m going to the bank” isn’t clear unless the full context is known.\n\n3. Different Words, Same Meaning (Synonyms)\nDifferent words can mean the same thing, but not always in the exact same way.\n\nExample:\n\n“Begin” and “Start” are similar, but sometimes only one fits naturally in a sentence.\n\n“Buy” and “Purchase” are similar, but “purchase” sounds more formal.\n\n4. Language Depends on Context\nThe meaning of a sentence often depends on what was said before, who is speaking, or where they are.\n\nExample:\n\n“Can you pass me the salt?”\nLiterally, it’s a question about ability, but really, it’s a polite request.\n\n5. Idioms and Expressions\nPhrases don’t always mean what the words say.\n\nExample:\n\n“Kick the bucket” means to die, not to literally kick something.\n\nComputers struggle with this because they analyze words literally.\n\n6. Word Order Matters\nA small change in word order can change the entire meaning.\n\nExample:\n\n“The dog bit the man.”\n\n“The man bit the dog.”\nSame words, but completely different meanings.\n\n7. Grammar Rules Have Many Exceptions\nThere are rules in language, but there are lots of exceptions.\n\nExample:\n\n“Go → Went” is an irregular past tense.\nComputers have to learn these exceptions separately — they’re not obvious.\n\nFunny Example of Language Confusion:\n\n“Time flies like an arrow.”\n“Fruit flies like a banana.”\n\nThe first one means time moves quickly, but the second sounds like a joke — suggesting fruit flies enjoy bananas. Same structure, totally different meaning. This is very confusing for a machine!",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(b) NL is Confusing for Computers.txt",
    "file_name": "lec1-(b) NL is Confusing for Computers.txt",
    "filename_keywords": [
      "confusing",
      "lec1",
      "computers"
    ],
    "content_keywords": [
      "language",
      "does",
      "idioms",
      "grammar rules have many exceptions\nthere",
      "expressions\nphrases",
      "kick",
      "language is ambiguous",
      "different words, same meaning (synonyms)",
      "synonyms",
      "context\nthe",
      "even",
      "word order matters",
      "funny example",
      "many",
      "same",
      "time",
      "grammar rules have many exceptions",
      "language depends on context",
      "went",
      "confusing",
      "begin",
      "ambiguous\nwords",
      "different meanings",
      "example",
      "language confusion",
      "literally",
      "polysemy",
      "buy",
      "natural language",
      "different",
      "this",
      "different words",
      "language depends",
      "nlp",
      "bank",
      "same meaning",
      "english",
      "start",
      "fruit",
      "computers",
      "nlp confusing",
      "purchase",
      "idioms and expressions",
      "the",
      "same words",
      "why",
      "same words, different meanings (polysemy)",
      "can",
      "word order matters\na",
      "key reasons why nlp"
    ],
    "technical_terms": [
      "language",
      "does",
      "idioms",
      "grammar rules have many exceptions\nthere",
      "expressions\nphrases",
      "kick",
      "synonyms",
      "context\nthe",
      "even",
      "funny example",
      "many",
      "same",
      "time",
      "went",
      "confusing",
      "begin",
      "ambiguous\nwords",
      "different meanings",
      "example",
      "language confusion",
      "literally",
      "polysemy",
      "buy",
      "natural language",
      "different",
      "this",
      "different words",
      "language depends",
      "nlp",
      "bank",
      "same meaning",
      "english",
      "start",
      "fruit",
      "computers",
      "nlp confusing",
      "purchase",
      "the",
      "same words",
      "why",
      "can",
      "word order matters\na",
      "key reasons why nlp"
    ],
    "all_keywords": [
      "language",
      "does",
      "idioms",
      "grammar rules have many exceptions\nthere",
      "expressions\nphrases",
      "kick",
      "language is ambiguous",
      "different words, same meaning (synonyms)",
      "synonyms",
      "context\nthe",
      "even",
      "word order matters",
      "funny example",
      "many",
      "same",
      "time",
      "grammar rules have many exceptions",
      "language depends on context",
      "went",
      "confusing",
      "begin",
      "ambiguous\nwords",
      "different meanings",
      "example",
      "language confusion",
      "literally",
      "polysemy",
      "buy",
      "natural language",
      "different",
      "this",
      "different words",
      "language depends",
      "nlp",
      "bank",
      "same meaning",
      "lec1",
      "english",
      "start",
      "fruit",
      "computers",
      "nlp confusing",
      "purchase",
      "idioms and expressions",
      "the",
      "same words",
      "why",
      "same words, different meanings (polysemy)",
      "can",
      "word order matters\na",
      "key reasons why nlp"
    ],
    "keyword_string": "language does idioms grammar rules have many exceptions\nthere expressions\nphrases kick language is ambiguous different words, same meaning (synonyms) synonyms context\nthe even word order matters funny example many same time grammar rules have many exceptions language depends on context went confusing begin ambiguous\nwords different meanings example language confusion literally polysemy buy natural language different this different words language depends nlp bank same meaning lec1 english start fruit computers nlp confusing purchase idioms and expressions the same words why same words, different meanings (polysemy) can word order matters\na key reasons why nlp",
    "token_count": 520,
    "word_count": 386,
    "sentence_count": 33,
    "paragraph_count": 30,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7423076923076923,
    "avg_sentence_length": 11.696969696969697,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": false,
    "content_type": "Technical, Structured"
  },
  {
    "document_id": 89,
    "document_hash": "e07c9bfa055e",
    "content": "Common Applications of NLP\n\nNLP is everywhere around us. It powers many tools and technologies we use every day. Below are some of the most common applications of Natural Language Processing, with simple explanations.\n\n1. Machine Translation\n\nThis is the process of automatically translating text from one language to another.\nIt helps people understand content written in other languages.\nExample:\n\nTranslating “Good morning” from English to French → “Bonjour”\n\n2. Text Summarization\n\nNLP can shorten long documents into brief summaries while keeping the main message.\nIt is useful for news apps, research papers, and reports.\nExample:\n\nA 3-page news article gets summarized in 3 sentences.\n\n3. Searching (Information Retrieval)\n\nSearch engines like Google use NLP to understand your query and find the most relevant web pages.\nIt doesn’t just look for keywords; it understands the meaning of your question.\nExample:\n\nYou search: “Best restaurants in Lahore”\n\nGoogle shows local listings, not just pages that mention the word “restaurant.”\n\n4. Question Answering\n\nThis refers to systems that can read a text or understand a query and give a direct answer.\nUsed in search engines, voice assistants, and customer support.\nExample:\n\nYou ask: “Who is the founder of Microsoft?”\n\nThe system replies: “Bill Gates”\n\n5. Named Entity Recognition (NER)\n\nNER is used to find and label important pieces of information in text like names of people, places, organizations, dates, etc.\nExample:\n\nFrom the sentence: “Apple was founded by Steve Jobs in California,”\n\nNLP extracts:\n\nPerson: Steve Jobs\n\nOrganization: Apple\n\nLocation: California\n\n6. Parts-of-Speech Tagging (POS Tagging)\n\nThis involves identifying the grammatical role of each word in a sentence—whether it’s a noun, verb, adjective, etc.\nIt helps machines understand sentence structure and meaning.\nExample:\n\n“The cat sleeps.”\n\nPOS tags: “The (determiner) cat (noun) sleeps (verb)”\n\n7. Clustering\n\nThis is used to group similar documents or texts together automatically, without labels.\nVery useful for organizing large sets of unstructured data like customer reviews, tweets, or news articles.\nExample:\n\nAll articles about sports are grouped together, while articles about politics form another group.\n\n8. Sentiment Analysis\n\nThis is the process of identifying emotions or opinions in text—whether something is positive, negative, or neutral.\nUsed in reviews, surveys, social media, etc.\nExample:\n\n“I love this phone!” → Positive sentiment\n\n“This service is terrible.” → Negative sentiment\n\n9. Text Classification\n\nNLP systems can automatically assign a category or label to a piece of text.\nIt’s used for spam detection, topic labeling, intent detection, etc.\nExample:\n\nAn email is labeled as “Spam” or “Important”\n\nA tweet is classified as “Sports” or “Entertainment”\n\n10. Chatbots and Virtual Assistants\n\nNLP helps build chatbots that can talk to humans through text or voice.\nVirtual assistants like Alexa, Siri, and Google Assistant use NLP to understand your commands and respond accordingly.\nExample:\n\nYou say: “Set an alarm for 6 AM”\n\nThe assistant understands and sets the alarm",
    "enhanced_text": "[NLP] Common Applications of NLP\n\nNLP is everywhere around us. It powers many tools and technologies we use every day. Below are some of the most common applications of Natural Language Processing, with simple explanations.\n\n1. Machine Translation\n\nThis is the process of automatically translating text from one language to another.\nIt helps people understand content written in other languages.\nExample:\n\nTranslating “Good morning” from English to French → “Bonjour”\n\n2. Text Summarization\n\nNLP can shorten long documents into brief summaries while keeping the main message.\nIt is useful for news apps, research papers, and reports.\nExample:\n\nA 3-page news article gets summarized in 3 sentences.\n\n3. Searching (Information Retrieval)\n\nSearch engines like Google use NLP to understand your query and find the most relevant web pages.\nIt doesn’t just look for keywords; it understands the meaning of your question.\nExample:\n\nYou search: “Best restaurants in Lahore”\n\nGoogle shows local listings, not just pages that mention the word “restaurant.”\n\n4. Question Answering\n\nThis refers to systems that can read a text or understand a query and give a direct answer.\nUsed in search engines, voice assistants, and customer support.\nExample:\n\nYou ask: “Who is the founder of Microsoft?”\n\nThe system replies: “Bill Gates”\n\n5. Named Entity Recognition (NER)\n\nNER is used to find and label important pieces of information in text like names of people, places, organizations, dates, etc.\nExample:\n\nFrom the sentence: “Apple was founded by Steve Jobs in California,”\n\nNLP extracts:\n\nPerson: Steve Jobs\n\nOrganization: Apple\n\nLocation: California\n\n6. Parts-of-Speech Tagging (POS Tagging)\n\nThis involves identifying the grammatical role of each word in a sentence—whether it’s a noun, verb, adjective, etc.\nIt helps machines understand sentence structure and meaning.\nExample:\n\n“The cat sleeps.”\n\nPOS tags: “The (determiner) cat (noun) sleeps (verb)”\n\n7. Clustering\n\nThis is used to group similar documents or texts together automatically, without labels.\nVery useful for organizing large sets of unstructured data like customer reviews, tweets, or news articles.\nExample:\n\nAll articles about sports are grouped together, while articles about politics form another group.\n\n8. Sentiment Analysis\n\nThis is the process of identifying emotions or opinions in text—whether something is positive, negative, or neutral.\nUsed in reviews, surveys, social media, etc.\nExample:\n\n“I love this phone!” → Positive sentiment\n\n“This service is terrible.” → Negative sentiment\n\n9. Text Classification\n\nNLP systems can automatically assign a category or label to a piece of text.\nIt’s used for spam detection, topic labeling, intent detection, etc.\nExample:\n\nAn email is labeled as “Spam” or “Important”\n\nA tweet is classified as “Sports” or “Entertainment”\n\n10. Chatbots and Virtual Assistants\n\nNLP helps build chatbots that can talk to humans through text or voice.\nVirtual assistants like Alexa, Siri, and Google Assistant use NLP to understand your commands and respond accordingly.\nExample:\n\nYou say: “Set an alarm for 6 AM”\n\nThe assistant understands and sets the alarm",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(c) Common Applications.txt",
    "file_name": "lec1-(c) Common Applications.txt",
    "filename_keywords": [
      "common",
      "lec1",
      "applications"
    ],
    "content_keywords": [
      "information retrieval",
      "translating",
      "text summarization\n\nnlp",
      "pos",
      "searching",
      "sentiment analysis\n\nthis",
      "chatbots and virtual assistants",
      "virtual",
      "parts-of-speech tagging (pos tagging)",
      "sentiment analysis",
      "set",
      "who",
      "question answering",
      "person",
      "you",
      "french",
      "machine translation\n\nthis",
      "speech tagging",
      "named entity recognition (ner)",
      "best",
      "steve jobs\n\norganization",
      "negative",
      "searching (information retrieval)",
      "text classification",
      "very",
      "search",
      "chatbots",
      "bonjour",
      "virtual assistants\n\nnlp",
      "all",
      "example",
      "lahore",
      "microsoft",
      "important",
      "text summarization",
      "nlp\n\nnlp",
      "google",
      "parts",
      "text classification\n\nnlp",
      "google assistant",
      "common applications",
      "ner",
      "natural language processing",
      "bill gates",
      "siri",
      "used",
      "spam",
      "clustering\n\nthis",
      "steve jobs",
      "nlp",
      "this",
      "good",
      "positive",
      "entertainment",
      "pos tagging",
      "english",
      "question answering\n\nthis",
      "named entity recognition",
      "sports",
      "apple\n\nlocation",
      "from",
      "the",
      "machine translation",
      "alexa",
      "california",
      "apple",
      "below"
    ],
    "technical_terms": [
      "information retrieval",
      "translating",
      "text summarization\n\nnlp",
      "pos",
      "searching",
      "sentiment analysis\n\nthis",
      "virtual",
      "set",
      "who",
      "person",
      "you",
      "french",
      "machine translation\n\nthis",
      "speech tagging",
      "best",
      "steve jobs\n\norganization",
      "negative",
      "very",
      "search",
      "chatbots",
      "bonjour",
      "virtual assistants\n\nnlp",
      "all",
      "example",
      "lahore",
      "microsoft",
      "important",
      "nlp\n\nnlp",
      "google",
      "parts",
      "text classification\n\nnlp",
      "google assistant",
      "common applications",
      "ner",
      "natural language processing",
      "bill gates",
      "siri",
      "used",
      "spam",
      "clustering\n\nthis",
      "steve jobs",
      "nlp",
      "this",
      "good",
      "positive",
      "entertainment",
      "pos tagging",
      "english",
      "question answering\n\nthis",
      "named entity recognition",
      "sports",
      "apple\n\nlocation",
      "from",
      "the",
      "alexa",
      "california",
      "apple",
      "below"
    ],
    "all_keywords": [
      "information retrieval",
      "translating",
      "text summarization\n\nnlp",
      "pos",
      "searching",
      "sentiment analysis\n\nthis",
      "chatbots and virtual assistants",
      "virtual",
      "parts-of-speech tagging (pos tagging)",
      "sentiment analysis",
      "set",
      "who",
      "question answering",
      "person",
      "you",
      "french",
      "machine translation\n\nthis",
      "speech tagging",
      "named entity recognition (ner)",
      "best",
      "steve jobs\n\norganization",
      "negative",
      "searching (information retrieval)",
      "text classification",
      "very",
      "search",
      "chatbots",
      "bonjour",
      "virtual assistants\n\nnlp",
      "all",
      "example",
      "lahore",
      "microsoft",
      "common",
      "important",
      "text summarization",
      "nlp\n\nnlp",
      "google",
      "parts",
      "text classification\n\nnlp",
      "google assistant",
      "common applications",
      "ner",
      "natural language processing",
      "bill gates",
      "siri",
      "used",
      "spam",
      "clustering\n\nthis",
      "steve jobs",
      "nlp",
      "this",
      "good",
      "positive",
      "entertainment",
      "lec1",
      "pos tagging",
      "english",
      "question answering\n\nthis",
      "named entity recognition",
      "sports",
      "apple\n\nlocation",
      "from",
      "the",
      "machine translation",
      "alexa",
      "applications",
      "california",
      "apple",
      "below"
    ],
    "keyword_string": "information retrieval translating text summarization\n\nnlp pos searching sentiment analysis\n\nthis chatbots and virtual assistants virtual parts-of-speech tagging (pos tagging) sentiment analysis set who question answering person you french machine translation\n\nthis speech tagging named entity recognition (ner) best steve jobs\n\norganization negative searching (information retrieval) text classification very search chatbots bonjour virtual assistants\n\nnlp all example lahore microsoft common important text summarization nlp\n\nnlp google parts text classification\n\nnlp google assistant common applications ner natural language processing bill gates siri used spam clustering\n\nthis steve jobs nlp this good positive entertainment lec1 pos tagging english question answering\n\nthis named entity recognition sports apple\n\nlocation from the machine translation alexa applications california apple below",
    "token_count": 658,
    "word_count": 475,
    "sentence_count": 35,
    "paragraph_count": 42,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7218844984802432,
    "avg_sentence_length": 13.571428571428571,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": false,
    "content_type": "Technical, Structured"
  },
  {
    "document_id": 90,
    "document_hash": "a329475c8c7d",
    "content": "Job Market for NLP\n\nNatural Language Processing is one of the fastest-growing areas in the field of Artificial Intelligence and Data Science. With the rise of voice assistants, chatbots, search engines, translation tools, and social media analysis, companies around the world are investing in NLP technology.\n\nReal-World Demand\n\nMany global and local companies are already working on NLP-based products and services. These include:\n\niManage – A successful company co-founded by an American-Pakistani, known for working with intelligent document and knowledge management. Their tools use NLP to understand legal documents, contracts, and business files.\n\nTeradata – A multinational company that uses NLP in large-scale data platforms for extracting insights from unstructured text like emails, reports, and customer feedback.\n\nSystems Limited – A Pakistani IT company involved in projects using AI and NLP, offering smart solutions for businesses in banking, telecom, and healthcare.\n\n NLP is in High Demand Across Industries\n\nJobs in NLP are available in fields like:\n\nHealthcare – Reading medical records, voice-based diagnostics, patient feedback.\n\nFinance – Analyzing financial documents, reports, and fraud detection.\n\nLegal – Reading and summarizing long contracts and legal documents.\n\nCustomer Service – Chatbots, email sorting, customer feedback analysis.\n\nE-commerce – Product recommendations, review analysis, search optimization.\n\nEducation – Automated grading, summarizing lectures, student feedback analysis.\n\n In-Demand Skills for NLP Jobs\n\nTo enter the NLP field, you should be familiar with:\n\nPython programming\n\nLibraries like spaCy, NLTK, HuggingFace Transformers\n\nMachine Learning & Deep Learning (e.g., using scikit-learn, TensorFlow, PyTorch)\n\nText processing techniques (tokenization, stemming, lemmatization)\n\nUnderstanding of linguistics and statistics\n\nExplore More\n\nFor more about NLP careers, skills, and job roles, visit:\n🔗 KnowledgeHut Blog on NLP Careers",
    "enhanced_text": "[NLP] Job Market for NLP\n\nNatural Language Processing is one of the fastest-growing areas in the field of Artificial Intelligence and Data Science. With the rise of voice assistants, chatbots, search engines, translation tools, and social media analysis, companies around the world are investing in NLP technology.\n\nReal-World Demand\n\nMany global and local companies are already working on NLP-based products and services. These include:\n\niManage – A successful company co-founded by an American-Pakistani, known for working with intelligent document and knowledge management. Their tools use NLP to understand legal documents, contracts, and business files.\n\nTeradata – A multinational company that uses NLP in large-scale data platforms for extracting insights from unstructured text like emails, reports, and customer feedback.\n\nSystems Limited – A Pakistani IT company involved in projects using AI and NLP, offering smart solutions for businesses in banking, telecom, and healthcare.\n\n NLP is in High Demand Across Industries\n\nJobs in NLP are available in fields like:\n\nHealthcare – Reading medical records, voice-based diagnostics, patient feedback.\n\nFinance – Analyzing financial documents, reports, and fraud detection.\n\nLegal – Reading and summarizing long contracts and legal documents.\n\nCustomer Service – Chatbots, email sorting, customer feedback analysis.\n\nE-commerce – Product recommendations, review analysis, search optimization.\n\nEducation – Automated grading, summarizing lectures, student feedback analysis.\n\n In-Demand Skills for NLP Jobs\n\nTo enter the NLP field, you should be familiar with:\n\nPython programming\n\nLibraries like spaCy, NLTK, HuggingFace Transformers\n\nMachine Learning & Deep Learning (e.g., using scikit-learn, TensorFlow, PyTorch)\n\nText processing techniques (tokenization, stemming, lemmatization)\n\nUnderstanding of linguistics and statistics\n\nExplore More\n\nFor more about NLP careers, skills, and job roles, visit:\n🔗 KnowledgeHut Blog on NLP Careers",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(d) Job Market for NLP.txt",
    "file_name": "lec1-(d) Job Market for NLP.txt",
    "filename_keywords": [
      "market",
      "job",
      "lec1",
      "nlp"
    ],
    "content_keywords": [
      "reading",
      "healthcare",
      "tensorflow",
      "product",
      "demand skills",
      "text",
      "nlp careers",
      "with",
      "job market",
      "legal",
      "python",
      "high demand across industries\n\njobs",
      "real",
      "these",
      "pakistani",
      "their",
      "chatbots",
      "education",
      "american",
      "systems limited",
      "explore more\n\nfor",
      "pytorch",
      "nlp\n\nnatural language processing",
      "nltk",
      "world demand\n\nmany",
      "analyzing",
      "libraries",
      "nlp",
      "customer service",
      "nlp jobs\n\nto",
      "deep learning",
      "automated",
      "knowledgehut blog",
      "understanding",
      "data science",
      "teradata",
      "finance",
      "artificial intelligence",
      "huggingface transformers\n\nmachine learning",
      "a pakistani it"
    ],
    "technical_terms": [
      "reading",
      "healthcare",
      "tensorflow",
      "product",
      "demand skills",
      "text",
      "nlp careers",
      "with",
      "job market",
      "legal",
      "python",
      "high demand across industries\n\njobs",
      "real",
      "these",
      "pakistani",
      "their",
      "chatbots",
      "education",
      "american",
      "systems limited",
      "explore more\n\nfor",
      "pytorch",
      "nlp\n\nnatural language processing",
      "nltk",
      "world demand\n\nmany",
      "analyzing",
      "libraries",
      "nlp",
      "customer service",
      "nlp jobs\n\nto",
      "deep learning",
      "automated",
      "knowledgehut blog",
      "understanding",
      "data science",
      "teradata",
      "finance",
      "artificial intelligence",
      "huggingface transformers\n\nmachine learning",
      "a pakistani it"
    ],
    "all_keywords": [
      "reading",
      "healthcare",
      "tensorflow",
      "product",
      "demand skills",
      "text",
      "nlp careers",
      "with",
      "job market",
      "legal",
      "python",
      "high demand across industries\n\njobs",
      "real",
      "these",
      "pakistani",
      "their",
      "chatbots",
      "education",
      "american",
      "systems limited",
      "explore more\n\nfor",
      "pytorch",
      "nlp\n\nnatural language processing",
      "job",
      "nltk",
      "market",
      "world demand\n\nmany",
      "analyzing",
      "libraries",
      "nlp",
      "customer service",
      "nlp jobs\n\nto",
      "lec1",
      "automated",
      "deep learning",
      "knowledgehut blog",
      "understanding",
      "data science",
      "teradata",
      "finance",
      "artificial intelligence",
      "huggingface transformers\n\nmachine learning",
      "a pakistani it"
    ],
    "keyword_string": "reading healthcare tensorflow product demand skills text nlp careers with job market legal python high demand across industries\n\njobs real these pakistani their chatbots education american systems limited explore more\n\nfor pytorch nlp\n\nnatural language processing job nltk market world demand\n\nmany analyzing libraries nlp customer service nlp jobs\n\nto lec1 automated deep learning knowledgehut blog understanding data science teradata finance artificial intelligence huggingface transformers\n\nmachine learning a pakistani it",
    "token_count": 391,
    "word_count": 272,
    "sentence_count": 14,
    "paragraph_count": 24,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6956521739130435,
    "avg_sentence_length": 19.428571428571427,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 91,
    "document_hash": "9b9cf143a437",
    "content": "Basic Vocabulary in NLP\n\nBefore building any Natural Language Processing system, it's important to understand some basic concepts and terms. These terms help us describe and process human language in a structured way.\n\nExample to Begin With:\n\nLet's take a sentence:\n“The cats are chasing mice.”\n\nFrom this, we get:\n\nWords or Tokens: \"The\", \"cats\", \"are\", \"chasing\", \"mice\"\nThese are the smallest units of meaning we can extract.\n\nLexicon\n\nA lexicon is like a smart dictionary for a language. It contains not just words and their meanings, but also:\n\nTheir grammatical properties (noun, verb, etc.)\n\nSemantic relationships (synonyms, opposites)\n\nMulti-word expressions and idioms (e.g., \"kick the bucket\")\n\nSo, a lexicon isn't just a word list — it's a full guide to how those words behave in a language.\n\n Vocabulary\n\nVocabulary refers to the complete set of unique words found in a collection of documents or a dataset.\nIt does not include word meanings or grammar — just the words themselves.\n\nFor example:\nIf your dataset includes:\n\n“The cat sleeps.”\n\n“The dog sleeps.”\n\nThen your vocabulary is:\n[“the”, “cat”, “sleeps”, “dog”]\n\nEach word is counted only once, even if it appears multiple times.\n\nTokenization\n\nTokenization is the process of splitting text into small parts called tokens.\nTokens are usually words, but they can also be punctuation marks or subwords.\n\nExample:\nInput: “Cats are running.”\nAfter tokenization: [“Cats”, “are”, “running”, “.”]\n\nThis is often the first step in most NLP tasks. Special tools called tokenizers do this job.\n\nStemming\n\nStemming is the process of cutting off prefixes or suffixes from words to get to their root form.\nIt doesn't always produce a real word — just a basic stem.\n\nExamples:\n\n“Universal” → “Univers”\n\n“University” → “Univers”\n\n“Player” → “Play”\n\nThis is helpful when you want to treat similar forms of a word as one item.\nHowever, stemming is rough — it might cut too much (over-stemming) or too little (under-stemming).\n\nPopular stemming algorithms include:\n\nPorter Stemmer\n\nLovins Stemmer\n\nDawson Stemmer\n\nLemmatization\n\nLemmatization is similar to stemming but smarter.\nIt uses grammar rules and a dictionary to reduce a word to its base or dictionary form, called a lemma.\n\nExample:\nOriginal sentence: “The cats are chasing mice.”\nAfter lemmatization: “The cat be chase mouse.”\n\nUnlike stemming, lemmatization gives real words, and it understands context.\nIt knows the difference between “better” and “good” or “ran” and “run”.\n\nTools called lemmatizers perform this task. They rely on linguistics and lexical databases like WordNet.\n\n Difference: Stemming vs Lemmatization\n\nStemming is fast and simple but may create fake or broken words.\n\nLemmatization is slower but gives correct and meaningful words.\n\nStemming uses fixed rules.\n\nLemmatization uses grammar, context, and dictionary lookups.\n\nCorpus (Plural: Corpora)\n\nA corpus is a large collection of texts, often used for training or testing NLP models.\nIt can include:\n\nNews articles\n\nBooks\n\nSocial media posts\n\nLegal documents\n\nTranscripts of spoken conversations\n\nExamples of known corpora:\n\nBrown Corpus\n\nDawn News Corpus\n\nBBC News Articles Corpus\n\nYou can find datasets on:\n\nGoogle Dataset Search\n\nKaggle\n\nStop Words\n\nStop words are very common words in a language that don’t add much meaning by themselves.\nExamples in English include: is, am, are, the, a, an, in, on, at, to, etc.\n\nThese are often removed during text processing to focus on the more meaningful words.\n\nExample: Python Code to Get Stop Words Using spaCy\n\npython\n\nCopyEdit\n\nimport spacy\n\nspacy.cli.download(\"en_core_web_sm\")\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nstopwords = nlp.Defaults.stop_words\n\nprint(stopwords)\n\nText Normalization\n\nText normalization means converting text into a standard, clean, and consistent format.\nIt includes many small steps like:\n\nLowercasing (converting all text to lowercase)\n\nTokenization (splitting into words)\n\nRemoving stop words\n\nStemming and lemmatization\n\nCorrecting spelling\n\nHandling numbers, dates, and symbols\n\nExpanding abbreviations (e.g., “don’t” → “do not”)\n\nAll these steps make the text easier for a machine to read and understand.",
    "enhanced_text": "[NLP] Basic Vocabulary in NLP\n\nBefore building any Natural Language Processing system, it's important to understand some basic concepts and terms. These terms help us describe and process human language in a structured way.\n\nExample to Begin With:\n\nLet's take a sentence:\n“The cats are chasing mice.”\n\nFrom this, we get:\n\nWords or Tokens: \"The\", \"cats\", \"are\", \"chasing\", \"mice\"\nThese are the smallest units of meaning we can extract.\n\nLexicon\n\nA lexicon is like a smart dictionary for a language. It contains not just words and their meanings, but also:\n\nTheir grammatical properties (noun, verb, etc.)\n\nSemantic relationships (synonyms, opposites)\n\nMulti-word expressions and idioms (e.g., \"kick the bucket\")\n\nSo, a lexicon isn't just a word list — it's a full guide to how those words behave in a language.\n\n Vocabulary\n\nVocabulary refers to the complete set of unique words found in a collection of documents or a dataset.\nIt does not include word meanings or grammar — just the words themselves.\n\nFor example:\nIf your dataset includes:\n\n“The cat sleeps.”\n\n“The dog sleeps.”\n\nThen your vocabulary is:\n[“the”, “cat”, “sleeps”, “dog”]\n\nEach word is counted only once, even if it appears multiple times.\n\nTokenization\n\nTokenization is the process of splitting text into small parts called tokens.\nTokens are usually words, but they can also be punctuation marks or subwords.\n\nExample:\nInput: “Cats are running.”\nAfter tokenization: [“Cats”, “are”, “running”, “.”]\n\nThis is often the first step in most NLP tasks. Special tools called tokenizers do this job.\n\nStemming\n\nStemming is the process of cutting off prefixes or suffixes from words to get to their root form.\nIt doesn't always produce a real word — just a basic stem.\n\nExamples:\n\n“Universal” → “Univers”\n\n“University” → “Univers”\n\n“Player” → “Play”\n\nThis is helpful when you want to treat similar forms of a word as one item.\nHowever, stemming is rough — it might cut too much (over-stemming) or too little (under-stemming).\n\nPopular stemming algorithms include:\n\nPorter Stemmer\n\nLovins Stemmer\n\nDawson Stemmer\n\nLemmatization\n\nLemmatization is similar to stemming but smarter.\nIt uses grammar rules and a dictionary to reduce a word to its base or dictionary form, called a lemma.\n\nExample:\nOriginal sentence: “The cats are chasing mice.”\nAfter lemmatization: “The cat be chase mouse.”\n\nUnlike stemming, lemmatization gives real words, and it understands context.\nIt knows the difference between “better” and “good” or “ran” and “run”.\n\nTools called lemmatizers perform this task. They rely on linguistics and lexical databases like WordNet.\n\n Difference: Stemming vs Lemmatization\n\nStemming is fast and simple but may create fake or broken words.\n\nLemmatization is slower but gives correct and meaningful words.\n\nStemming uses fixed rules.\n\nLemmatization uses grammar, context, and dictionary lookups.\n\nCorpus (Plural: Corpora)\n\nA corpus is a large collection of texts, often used for training or testing NLP models.\nIt can include:\n\nNews articles\n\nBooks\n\nSocial media posts\n\nLegal documents\n\nTranscripts of spoken conversations\n\nExamples of known corpora:\n\nBrown Corpus\n\nDawn News Corpus\n\nBBC News Articles Corpus\n\nYou can find datasets on:\n\nGoogle Dataset Search\n\nKaggle\n\nStop Words\n\nStop words are very common words in a language that don’t add much meaning by themselves.\nExamples in English include: is, am, are, the, a, an, in, on, at, to, etc.\n\nThese are often removed during text processing to focus on the more meaningful words.\n\nExample: Python Code to Get Stop Words Using spaCy\n\npython\n\nCopyEdit\n\nimport spacy\n\nspacy.cli.download(\"en_core_web_sm\")\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nstopwords = nlp.Defaults.stop_words\n\nprint(stopwords)\n\nText Normalization\n\nText normalization means converting text into a standard, clean, and consistent format.\nIt includes many small steps like:\n\nLowercasing (converting all text to lowercase)\n\nTokenization (splitting into words)\n\nRemoving stop words\n\nStemming and lemmatization\n\nCorrecting spelling\n\nHandling numbers, dates, and symbols\n\nExpanding abbreviations (e.g., “don’t” → “do not”)\n\nAll these steps make the text easier for a machine to read and understand.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(e) Basic Vocabulary.txt",
    "file_name": "lec1-(e) Basic Vocabulary.txt",
    "filename_keywords": [
      "vocabulary",
      "lec1",
      "basic"
    ],
    "content_keywords": [
      "lexicon\n\na",
      "examples",
      "removing",
      "vocabulary\n\nvocabulary",
      "play",
      "defaults",
      "then",
      "corpora",
      "lowercasing",
      "brown corpus\n\ndawn news corpus\n\nbbc news articles corpus\n\nyou",
      "handling",
      "player",
      "legal",
      "tokenization\n\ntokenization",
      "after",
      "corpus",
      "for",
      "these",
      "let",
      "news",
      "tools",
      "their",
      "university",
      "unlike",
      "chasing",
      "universal",
      "example",
      "special",
      "tokenization",
      "python code",
      "expanding",
      "stemming",
      "google dataset search\n\nkaggle\n\nstop words\n\nstop",
      "all",
      "plural",
      "words",
      "t just a word list — it",
      "copyedit",
      "en_core_web_sm",
      "get stop words using",
      "bbc",
      "they",
      "natural language processing",
      "input",
      "porter stemmer\n\nlovins stemmer\n\ndawson stemmer\n\nlemmatization\n\nlemmatization",
      "stemming\n\nstemming",
      "semantic",
      "lemmatization",
      "this",
      "nlp",
      "univers",
      "cats",
      "begin with",
      "transcripts",
      "text normalization\n\ntext",
      "difference",
      "are",
      "lemmatization\n\nstemming",
      "english",
      "from",
      "the",
      "however",
      "books\n\nsocial",
      "multi",
      "kick the bucket",
      "tokens",
      "correcting",
      "basic vocabulary",
      "nlp\n\nbefore",
      "original",
      "popular",
      "wordnet",
      "each",
      "mice"
    ],
    "technical_terms": [
      "lexicon\n\na",
      "examples",
      "removing",
      "vocabulary\n\nvocabulary",
      "play",
      "defaults",
      "then",
      "corpora",
      "lowercasing",
      "brown corpus\n\ndawn news corpus\n\nbbc news articles corpus\n\nyou",
      "handling",
      "player",
      "legal",
      "tokenization\n\ntokenization",
      "after",
      "corpus",
      "for",
      "these",
      "let",
      "news",
      "tools",
      "their",
      "university",
      "unlike",
      "universal",
      "example",
      "special",
      "tokenization",
      "python code",
      "expanding",
      "stemming",
      "google dataset search\n\nkaggle\n\nstop words\n\nstop",
      "all",
      "plural",
      "words",
      "copyedit",
      "get stop words using",
      "bbc",
      "they",
      "natural language processing",
      "input",
      "porter stemmer\n\nlovins stemmer\n\ndawson stemmer\n\nlemmatization\n\nlemmatization",
      "stemming\n\nstemming",
      "semantic",
      "lemmatization",
      "this",
      "nlp",
      "univers",
      "cats",
      "begin with",
      "transcripts",
      "text normalization\n\ntext",
      "difference",
      "lemmatization\n\nstemming",
      "english",
      "from",
      "the",
      "however",
      "books\n\nsocial",
      "multi",
      "tokens",
      "correcting",
      "basic vocabulary",
      "nlp\n\nbefore",
      "original",
      "popular",
      "wordnet",
      "each"
    ],
    "all_keywords": [
      "play",
      "then",
      "after",
      "for",
      "these",
      "news",
      "vocabulary",
      "basic",
      "tokenization",
      "google dataset search\n\nkaggle\n\nstop words\n\nstop",
      "cats",
      "transcripts",
      "are",
      "from",
      "however",
      "lowercasing",
      "handling",
      "corpus",
      "let",
      "their",
      "example",
      "python code",
      "en_core_web_sm",
      "words",
      "copyedit",
      "porter stemmer\n\nlovins stemmer\n\ndawson stemmer\n\nlemmatization\n\nlemmatization",
      "stemming\n\nstemming",
      "univers",
      "the",
      "correcting",
      "original",
      "popular",
      "each",
      "vocabulary\n\nvocabulary",
      "defaults",
      "corpora",
      "brown corpus\n\ndawn news corpus\n\nbbc news articles corpus\n\nyou",
      "player",
      "university",
      "unlike",
      "chasing",
      "all",
      "expanding",
      "plural",
      "t just a word list — it",
      "they",
      "natural language processing",
      "this",
      "lec1",
      "difference",
      "lemmatization\n\nstemming",
      "english",
      "kick the bucket",
      "basic vocabulary",
      "nlp\n\nbefore",
      "lexicon\n\na",
      "examples",
      "removing",
      "legal",
      "tokenization\n\ntokenization",
      "tools",
      "universal",
      "special",
      "stemming",
      "get stop words using",
      "bbc",
      "input",
      "lemmatization",
      "semantic",
      "nlp",
      "begin with",
      "text normalization\n\ntext",
      "books\n\nsocial",
      "multi",
      "tokens",
      "wordnet",
      "mice"
    ],
    "keyword_string": "play then after for these news vocabulary basic tokenization google dataset search\n\nkaggle\n\nstop words\n\nstop cats transcripts are from however lowercasing handling corpus let their example python code en_core_web_sm words copyedit porter stemmer\n\nlovins stemmer\n\ndawson stemmer\n\nlemmatization\n\nlemmatization stemming\n\nstemming univers the correcting original popular each vocabulary\n\nvocabulary defaults corpora brown corpus\n\ndawn news corpus\n\nbbc news articles corpus\n\nyou player university unlike chasing all expanding plural t just a word list — it they natural language processing this lec1 difference lemmatization\n\nstemming english kick the bucket basic vocabulary nlp\n\nbefore lexicon\n\na examples removing legal tokenization\n\ntokenization tools universal special stemming get stop words using bbc input lemmatization semantic nlp begin with text normalization\n\ntext books\n\nsocial multi tokens wordnet mice",
    "token_count": 977,
    "word_count": 632,
    "sentence_count": 33,
    "paragraph_count": 79,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.646878198567042,
    "avg_sentence_length": 19.151515151515152,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 92,
    "document_hash": "af3cc204e5bd",
    "content": "A Basic Natural Language Processing (NLP) System\n\nTo understand how an NLP system works, imagine it as a four-step pipeline where human language enters one end, and machine-generated or processed language comes out the other. Each step plays a vital role in how machines handle natural language.\n\n🔹 1. Input Language\n\nThis is the starting point. A user gives some natural language input — it could be in the form of:\n\nText (e.g., “What’s the weather today?”)\n\nVoice (which is first converted to text using speech recognition)\n\nThe input is usually messy, unstructured, and full of variations, just like how people talk or write.\n\n🔹 2. Understanding the Language\n\nOnce the language enters the system, the machine tries to understand the meaning. This step is the heart of NLP.\nIt involves tasks like:\n\nTokenization (splitting the sentence into words)\n\nParsing (understanding sentence structure)\n\nPart-of-speech tagging\n\nNamed entity recognition\n\nSentiment detection\n\nSemantic analysis (what the sentence actually means)\n\nThis is the \"U\" in NLP: Understanding.\nHere, the computer is trained to read and comprehend language the way humans do.\n\n🔹 3. Computer Processing (Reasoning and Decision-Making)\n\nAfter understanding the input, the system uses logic, machine learning models, or pre-defined rules to decide what to do.\n\nExamples:\n\nA chatbot figures out how to respond.\n\nA translation model determines the equivalent sentence in another language.\n\nA search engine finds the most relevant result.\n\nThis is where the thinking or computation happens based on what the system understood.\n\n🔹 4. Language Generation (Output)\n\nFinally, the system generates language as output.\nThis output could be:\n\nA direct response (e.g., “It’s sunny and 30°C”)\n\nA translation (e.g., converting English to French)\n\nA summary or a sentiment label\n\nThis is the “G” part: Generation.\nThe computer creates or outputs text (or speech) that is clear and meaningful for a human to understand.\n\n📌 Example:\n\nUser: \"What's the weather today?\"\n\nInput Language: User speaks or types the question.\n\nUnderstanding: The system breaks the sentence into parts, identifies the question type, and understands the meaning.\n\nProcessing: It searches a weather database for today’s data.\n\nGeneration: The system replies: “The weather is sunny and 30 degrees.”",
    "enhanced_text": "[NLP] A Basic Natural Language Processing (NLP) System\n\nTo understand how an NLP system works, imagine it as a four-step pipeline where human language enters one end, and machine-generated or processed language comes out the other. Each step plays a vital role in how machines handle natural language.\n\n🔹 1. Input Language\n\nThis is the starting point. A user gives some natural language input — it could be in the form of:\n\nText (e.g., “What’s the weather today?”)\n\nVoice (which is first converted to text using speech recognition)\n\nThe input is usually messy, unstructured, and full of variations, just like how people talk or write.\n\n🔹 2. Understanding the Language\n\nOnce the language enters the system, the machine tries to understand the meaning. This step is the heart of NLP.\nIt involves tasks like:\n\nTokenization (splitting the sentence into words)\n\nParsing (understanding sentence structure)\n\nPart-of-speech tagging\n\nNamed entity recognition\n\nSentiment detection\n\nSemantic analysis (what the sentence actually means)\n\nThis is the \"U\" in NLP: Understanding.\nHere, the computer is trained to read and comprehend language the way humans do.\n\n🔹 3. Computer Processing (Reasoning and Decision-Making)\n\nAfter understanding the input, the system uses logic, machine learning models, or pre-defined rules to decide what to do.\n\nExamples:\n\nA chatbot figures out how to respond.\n\nA translation model determines the equivalent sentence in another language.\n\nA search engine finds the most relevant result.\n\nThis is where the thinking or computation happens based on what the system understood.\n\n🔹 4. Language Generation (Output)\n\nFinally, the system generates language as output.\nThis output could be:\n\nA direct response (e.g., “It’s sunny and 30°C”)\n\nA translation (e.g., converting English to French)\n\nA summary or a sentiment label\n\nThis is the “G” part: Generation.\nThe computer creates or outputs text (or speech) that is clear and meaningful for a human to understand.\n\n📌 Example:\n\nUser: \"What's the weather today?\"\n\nInput Language: User speaks or types the question.\n\nUnderstanding: The system breaks the sentence into parts, identifies the question type, and understands the meaning.\n\nProcessing: It searches a weather database for today’s data.\n\nGeneration: The system replies: “The weather is sunny and 30 degrees.”",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(f) basic Natural Language Processing (NLP) system.txt",
    "file_name": "lec1-(f) basic Natural Language Processing (NLP) system.txt",
    "filename_keywords": [
      "language",
      "natural",
      "basic",
      "nlp",
      "processing",
      "lec1",
      "system"
    ],
    "content_keywords": [
      "a basic natural language processing",
      "examples",
      "text",
      "part",
      "named",
      "finally",
      "user",
      "after",
      "french",
      "processing",
      "input language",
      "input language\n\nthis",
      "parsing",
      "voice",
      "output",
      "tokenization",
      "example",
      "language generation",
      "computer processing",
      "semantic",
      "this",
      "nlp",
      "reasoning",
      "english",
      "decision",
      "understanding",
      "the",
      "language\n\nonce",
      "system\n\nto",
      "here",
      "sentiment",
      "what",
      "generation",
      "each",
      "making"
    ],
    "technical_terms": [
      "a basic natural language processing",
      "examples",
      "text",
      "part",
      "named",
      "finally",
      "user",
      "after",
      "french",
      "processing",
      "input language",
      "input language\n\nthis",
      "parsing",
      "voice",
      "output",
      "tokenization",
      "example",
      "language generation",
      "computer processing",
      "semantic",
      "this",
      "nlp",
      "reasoning",
      "english",
      "decision",
      "understanding",
      "the",
      "language\n\nonce",
      "system\n\nto",
      "here",
      "sentiment",
      "what",
      "generation",
      "each",
      "making"
    ],
    "all_keywords": [
      "language",
      "a basic natural language processing",
      "examples",
      "text",
      "natural",
      "part",
      "named",
      "finally",
      "user",
      "processing",
      "after",
      "french",
      "input language",
      "input language\n\nthis",
      "parsing",
      "voice",
      "output",
      "basic",
      "tokenization",
      "example",
      "language generation",
      "computer processing",
      "semantic",
      "this",
      "nlp",
      "lec1",
      "system",
      "reasoning",
      "english",
      "decision",
      "understanding",
      "the",
      "language\n\nonce",
      "system\n\nto",
      "here",
      "sentiment",
      "what",
      "generation",
      "each",
      "making"
    ],
    "keyword_string": "language a basic natural language processing examples text natural part named finally user processing after french input language input language\n\nthis parsing voice output basic tokenization example language generation computer processing semantic this nlp lec1 system reasoning english decision understanding the language\n\nonce system\n\nto here sentiment what generation each making",
    "token_count": 486,
    "word_count": 356,
    "sentence_count": 25,
    "paragraph_count": 35,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7325102880658436,
    "avg_sentence_length": 14.24,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 93,
    "document_hash": "a4b713f7f02b",
    "content": "Why is NLP Important?\n\nHuman-Centered Communication\nMost human knowledge is stored and shared through language. NLP enables computers to access, analyze, and generate this information.\n\nMassive Text Data Everywhere\nWith billions of emails, social media posts, websites, and documents being created daily, NLP helps in managing and making sense of this data.\n\nBridges the Gap Between Humans and Machines\nInstead of learning programming languages to talk to machines, NLP allows us to use our own language.\n\nEssential in AI Applications\nNLP powers chatbots, translation tools, voice assistants, sentiment analysis, and more.",
    "enhanced_text": "[NLP] Why is NLP Important?\n\nHuman-Centered Communication\nMost human knowledge is stored and shared through language. NLP enables computers to access, analyze, and generate this information.\n\nMassive Text Data Everywhere\nWith billions of emails, social media posts, websites, and documents being created daily, NLP helps in managing and making sense of this data.\n\nBridges the Gap Between Humans and Machines\nInstead of learning programming languages to talk to machines, NLP allows us to use our own language.\n\nEssential in AI Applications\nNLP powers chatbots, translation tools, voice assistants, sentiment analysis, and more.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(g) importance of NLP.txt",
    "file_name": "lec1-(g) importance of NLP.txt",
    "filename_keywords": [
      "importance",
      "nlp",
      "lec1"
    ],
    "content_keywords": [
      "massive text data everywhere\nwith",
      "gap between humans",
      "human",
      "essential",
      "why",
      "centered communication\nmost",
      "machines\ninstead",
      "nlp",
      "bridges",
      "ai applications\nnlp",
      "nlp important"
    ],
    "technical_terms": [
      "massive text data everywhere\nwith",
      "gap between humans",
      "human",
      "essential",
      "why",
      "centered communication\nmost",
      "machines\ninstead",
      "nlp",
      "bridges",
      "ai applications\nnlp",
      "nlp important"
    ],
    "all_keywords": [
      "massive text data everywhere\nwith",
      "gap between humans",
      "human",
      "essential",
      "why",
      "centered communication\nmost",
      "importance",
      "machines\ninstead",
      "nlp",
      "bridges",
      "ai applications\nnlp",
      "nlp important",
      "lec1"
    ],
    "keyword_string": "massive text data everywhere\nwith gap between humans human essential why centered communication\nmost importance machines\ninstead nlp bridges ai applications\nnlp nlp important lec1",
    "token_count": 116,
    "word_count": 91,
    "sentence_count": 6,
    "paragraph_count": 5,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7844827586206896,
    "avg_sentence_length": 15.166666666666666,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 94,
    "document_hash": "ed76d7557cd2",
    "content": "Why NLP Needs Special Techniques\n\nBecause human language is full of ambiguity, context, hidden meanings, and exceptions, we can’t just use simple rules or logic to make computers understand it. We need to combine knowledge from many different fields to teach machines how to process language in a human-like way.\n\nHere’s a short explanation of the key fields that NLP depends on:\n\n1. Linguistics – Understanding how language works\n\nLinguistics helps us understand the structure of language, like grammar, syntax (sentence structure), semantics (meaning), and phonetics (sound).\nThis knowledge is essential for breaking down sentences, identifying parts of speech, and figuring out how words relate to each other.\n\n2. Mathematics and Statistics – Finding patterns in language\n\nLanguage data is huge and messy. We use mathematical models and statistical methods to analyze patterns, like how often words appear together, or what word is likely to come next in a sentence.\nFor example, we can calculate the probability that the next word after “I am” is “happy”.\n\n3. Artificial Intelligence and Machine Learning – Learning from data\n\nInstead of writing manual rules, we teach machines to learn from examples.\nMachine learning allows a system to look at thousands of sentences and figure out how language is used. Over time, it can predict answers, translate text, or understand emotions in new sentences it has never seen before.\n\n4. Psychology – Understanding how humans communicate\n\nPsychology helps us understand how people think, speak, and understand language.\nIt gives insight into meaning, emotions, memory, and behavior — all important when building NLP systems like chatbots or voice assistants that need to “think” like a human.\n\n5. Databases and Algorithms – Handling big data and smart processing\n\nLanguage data comes from the web, books, news, tweets, emails — it's massive!\nDatabases help us store and organize this data.\nAlgorithms help us search, sort, and process it quickly — for example, finding the right answer in a question-answering system or clustering similar documents together.\n\nEach of these fields plays a unique role in helping machines understand the very thing that defines us as humans: language.",
    "enhanced_text": "[NLP] Why NLP Needs Special Techniques\n\nBecause human language is full of ambiguity, context, hidden meanings, and exceptions, we can’t just use simple rules or logic to make computers understand it. We need to combine knowledge from many different fields to teach machines how to process language in a human-like way.\n\nHere’s a short explanation of the key fields that NLP depends on:\n\n1. Linguistics – Understanding how language works\n\nLinguistics helps us understand the structure of language, like grammar, syntax (sentence structure), semantics (meaning), and phonetics (sound).\nThis knowledge is essential for breaking down sentences, identifying parts of speech, and figuring out how words relate to each other.\n\n2. Mathematics and Statistics – Finding patterns in language\n\nLanguage data is huge and messy. We use mathematical models and statistical methods to analyze patterns, like how often words appear together, or what word is likely to come next in a sentence.\nFor example, we can calculate the probability that the next word after “I am” is “happy”.\n\n3. Artificial Intelligence and Machine Learning – Learning from data\n\nInstead of writing manual rules, we teach machines to learn from examples.\nMachine learning allows a system to look at thousands of sentences and figure out how language is used. Over time, it can predict answers, translate text, or understand emotions in new sentences it has never seen before.\n\n4. Psychology – Understanding how humans communicate\n\nPsychology helps us understand how people think, speak, and understand language.\nIt gives insight into meaning, emotions, memory, and behavior — all important when building NLP systems like chatbots or voice assistants that need to “think” like a human.\n\n5. Databases and Algorithms – Handling big data and smart processing\n\nLanguage data comes from the web, books, news, tweets, emails — it's massive!\nDatabases help us store and organize this data.\nAlgorithms help us search, sort, and process it quickly — for example, finding the right answer in a question-answering system or clustering similar documents together.\n\nEach of these fields plays a unique role in helping machines understand the very thing that defines us as humans: language.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec1-(h) NLP need special techniques.txt",
    "file_name": "lec1-(h) NLP need special techniques.txt",
    "filename_keywords": [
      "special",
      "nlp",
      "need",
      "techniques",
      "lec1"
    ],
    "content_keywords": [
      "language",
      "handling",
      "over",
      "linguistics – understanding how language works",
      "mathematics",
      "for",
      "instead",
      "algorithms",
      "learning",
      "psychology",
      "psychology – understanding how humans communicate",
      "machine learning",
      "linguistics",
      "databases",
      "mathematics and statistics – finding patterns in la",
      "finding",
      "artificial intelligence and machine learning – lear",
      "statistics",
      "this",
      "nlp",
      "why nlp needs special techniques\n\nbecause",
      "machine",
      "understanding",
      "here",
      "databases and algorithms – handling big data and sm",
      "artificial intelligence",
      "each"
    ],
    "technical_terms": [
      "language",
      "handling",
      "over",
      "mathematics",
      "for",
      "instead",
      "algorithms",
      "learning",
      "psychology",
      "machine learning",
      "linguistics",
      "databases",
      "finding",
      "statistics",
      "this",
      "nlp",
      "why nlp needs special techniques\n\nbecause",
      "machine",
      "understanding",
      "here",
      "artificial intelligence",
      "each"
    ],
    "all_keywords": [
      "language",
      "handling",
      "over",
      "linguistics – understanding how language works",
      "need",
      "mathematics",
      "for",
      "instead",
      "algorithms",
      "learning",
      "psychology",
      "psychology – understanding how humans communicate",
      "special",
      "machine learning",
      "linguistics",
      "databases",
      "mathematics and statistics – finding patterns in la",
      "finding",
      "artificial intelligence and machine learning – lear",
      "statistics",
      "this",
      "nlp",
      "why nlp needs special techniques\n\nbecause",
      "techniques",
      "machine",
      "lec1",
      "understanding",
      "here",
      "databases and algorithms – handling big data and sm",
      "artificial intelligence",
      "each"
    ],
    "keyword_string": "language handling over linguistics – understanding how language works need mathematics for instead algorithms learning psychology psychology – understanding how humans communicate special machine learning linguistics databases mathematics and statistics – finding patterns in la finding artificial intelligence and machine learning – lear statistics this nlp why nlp needs special techniques\n\nbecause techniques machine lec1 understanding here databases and algorithms – handling big data and sm artificial intelligence each",
    "token_count": 431,
    "word_count": 349,
    "sentence_count": 21,
    "paragraph_count": 14,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.8097447795823666,
    "avg_sentence_length": 16.61904761904762,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": false,
    "content_type": "Technical, Structured"
  },
  {
    "document_id": 95,
    "document_hash": "776ab8e6e55e",
    "content": "Information Extraction (IE)\n\nInformation Extraction is the process of pulling out specific pieces of information from unstructured text — like articles, emails, or reports — and organizing it into a structured form that computers can use.\n\nInstead of reading the entire text manually, IE helps systems automatically detect important facts, such as names, dates, locations, or events.\n\n🔍 What Does “Unstructured Text” Mean?\n\nUnstructured text means natural language, written the way people talk or write — not stored in neat rows and columns like a spreadsheet.\n\nExample of unstructured text:\n\n\"Mohammad Hassan, a 26-year-old aspiring photographer, was stuck in a dilemma when he found out that his wheat yield...”\n\nIt’s messy for a computer to read, but full of useful data for a human.\n\n🎯 Goal of Information Extraction\n\nThe main goal is to identify and extract only the relevant information, such as:\n\nNames of people\n\nPlaces or locations\n\nDates or times\n\nEvents or actions\n\nAmounts or quantities\n\nThis helps convert unstructured text into structured data, which can then be used for analysis, searching, or automation.\n\n✅ Real-Life Example\n\nYour Email → Calendar\nYou receive an email:\n\n“Let’s meet on Monday at 2 PM at the coffee shop.”\n\nYour email app detects the date and time and suggests adding a calendar event — this is Information Extraction in action.\n\n🧠 Why is Information Extraction Useful?\n\nInformation Extraction is important for:\n\nSmart search and classification\nAutomatically labeling emails or documents (e.g., \"Job Offer\", \"Travel Plan\")\n\nAutomation\nPulling details from thousands of resumes, reports, or forms without human effort\n\nTrend analysis\nMining articles or social media to find out what's trending (e.g., people talking about “floods” in “Sindh”)\n\nFinding hidden connections\nDetecting relationships between people, companies, events, or topics\n\n📝 Example Text for Extraction:\n\nMohammad Hassan, a 26-year-old aspiring photographer, was stuck in a dilemma when he found out that his wheat yield, the primary source of income for his family, had dramatically fallen from the usual five sacks to three. An acute water shortage had gripped his hometown, Skardu, located in the heart of the Karakoram mountains, bringing the lives of its residents to a halt.\n\nLet’s extract key information from this paragraph:\n\nName: Mohammad Hassan\n\nAge: 26\n\nOccupation: Aspiring Photographer\n\nLocation: Skardu, Karakoram Mountains\n\nEvent: Wheat yield dropped due to water shortage\n\nCause: Acute water shortage",
    "enhanced_text": "[NLP] Information Extraction (IE)\n\nInformation Extraction is the process of pulling out specific pieces of information from unstructured text — like articles, emails, or reports — and organizing it into a structured form that computers can use.\n\nInstead of reading the entire text manually, IE helps systems automatically detect important facts, such as names, dates, locations, or events.\n\n🔍 What Does “Unstructured Text” Mean?\n\nUnstructured text means natural language, written the way people talk or write — not stored in neat rows and columns like a spreadsheet.\n\nExample of unstructured text:\n\n\"Mohammad Hassan, a 26-year-old aspiring photographer, was stuck in a dilemma when he found out that his wheat yield...”\n\nIt’s messy for a computer to read, but full of useful data for a human.\n\n🎯 Goal of Information Extraction\n\nThe main goal is to identify and extract only the relevant information, such as:\n\nNames of people\n\nPlaces or locations\n\nDates or times\n\nEvents or actions\n\nAmounts or quantities\n\nThis helps convert unstructured text into structured data, which can then be used for analysis, searching, or automation.\n\n✅ Real-Life Example\n\nYour Email → Calendar\nYou receive an email:\n\n“Let’s meet on Monday at 2 PM at the coffee shop.”\n\nYour email app detects the date and time and suggests adding a calendar event — this is Information Extraction in action.\n\n🧠 Why is Information Extraction Useful?\n\nInformation Extraction is important for:\n\nSmart search and classification\nAutomatically labeling emails or documents (e.g., \"Job Offer\", \"Travel Plan\")\n\nAutomation\nPulling details from thousands of resumes, reports, or forms without human effort\n\nTrend analysis\nMining articles or social media to find out what's trending (e.g., people talking about “floods” in “Sindh”)\n\nFinding hidden connections\nDetecting relationships between people, companies, events, or topics\n\n📝 Example Text for Extraction:\n\nMohammad Hassan, a 26-year-old aspiring photographer, was stuck in a dilemma when he found out that his wheat yield, the primary source of income for his family, had dramatically fallen from the usual five sacks to three. An acute water shortage had gripped his hometown, Skardu, located in the heart of the Karakoram mountains, bringing the lives of its residents to a halt.\n\nLet’s extract key information from this paragraph:\n\nName: Mohammad Hassan\n\nAge: 26\n\nOccupation: Aspiring Photographer\n\nLocation: Skardu, Karakoram Mountains\n\nEvent: Wheat yield dropped due to water shortage\n\nCause: Acute water shortage",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(a) Information Extraction.txt",
    "file_name": "lec2-(a) Information Extraction.txt",
    "filename_keywords": [
      "extraction",
      "information",
      "lec2"
    ],
    "content_keywords": [
      "what does",
      "unstructured text",
      "automation\npulling",
      "smart",
      "sindh",
      "trend",
      "example text",
      "events",
      "places",
      "instead",
      "real",
      "mohammad hassan",
      "let",
      "travel plan",
      "wheat",
      "mean",
      "mohammad hassan\n\nage",
      "dates",
      "example",
      "job offer",
      "information extraction",
      "karakoram",
      "detecting",
      "automatically",
      "occupation",
      "finding",
      "information extraction useful",
      "acute",
      "this",
      "life example\n\nyour email",
      "calendar\nyou",
      "unstructured",
      "cause",
      "information extraction\n\nthe",
      "goal",
      "names",
      "your",
      "mining",
      "aspiring photographer\n\nlocation",
      "monday",
      "why",
      "skardu",
      "karakoram mountains\n\nevent",
      "name",
      "extraction",
      "amounts"
    ],
    "technical_terms": [
      "what does",
      "unstructured text",
      "automation\npulling",
      "smart",
      "sindh",
      "trend",
      "example text",
      "events",
      "places",
      "instead",
      "real",
      "mohammad hassan",
      "let",
      "travel plan",
      "wheat",
      "mean",
      "mohammad hassan\n\nage",
      "dates",
      "example",
      "job offer",
      "information extraction",
      "karakoram",
      "detecting",
      "automatically",
      "occupation",
      "finding",
      "information extraction useful",
      "acute",
      "this",
      "life example\n\nyour email",
      "calendar\nyou",
      "unstructured",
      "cause",
      "information extraction\n\nthe",
      "goal",
      "names",
      "your",
      "mining",
      "aspiring photographer\n\nlocation",
      "monday",
      "why",
      "skardu",
      "karakoram mountains\n\nevent",
      "name",
      "extraction",
      "amounts"
    ],
    "all_keywords": [
      "information",
      "what does",
      "unstructured text",
      "automation\npulling",
      "smart",
      "sindh",
      "trend",
      "example text",
      "events",
      "places",
      "instead",
      "real",
      "mohammad hassan",
      "let",
      "travel plan",
      "wheat",
      "mean",
      "mohammad hassan\n\nage",
      "dates",
      "example",
      "job offer",
      "information extraction",
      "karakoram",
      "detecting",
      "automatically",
      "occupation",
      "finding",
      "lec2",
      "information extraction useful",
      "acute",
      "this",
      "life example\n\nyour email",
      "calendar\nyou",
      "unstructured",
      "cause",
      "information extraction\n\nthe",
      "goal",
      "names",
      "your",
      "mining",
      "aspiring photographer\n\nlocation",
      "monday",
      "why",
      "skardu",
      "karakoram mountains\n\nevent",
      "name",
      "extraction",
      "amounts"
    ],
    "keyword_string": "information what does unstructured text automation\npulling smart sindh trend example text events places instead real mohammad hassan let travel plan wheat mean mohammad hassan\n\nage dates example job offer information extraction karakoram detecting automatically occupation finding lec2 information extraction useful acute this life example\n\nyour email calendar\nyou unstructured cause information extraction\n\nthe goal names your mining aspiring photographer\n\nlocation monday why skardu karakoram mountains\n\nevent name extraction amounts",
    "token_count": 510,
    "word_count": 386,
    "sentence_count": 11,
    "paragraph_count": 35,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7568627450980392,
    "avg_sentence_length": 35.09090909090909,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 96,
    "document_hash": "119b53facd25",
    "content": "Typical Information Extraction Applications\n\nBusiness Intelligence\nDescription:\nExtracting structured data from large volumes of text such as reports, emails, news articles, and social media to help business analysts make informed decisions. The goal is to identify key facts like company names, product launches, market trends, or financial figures automatically.\n\nExample:\nInput Text:\n“ABC Corp announced a 10% increase in revenue this quarter and plans to launch a new product next month.”\nExtracted Information:\n\nCompany: ABC Corp\n\nRevenue Growth: 10%\n\nEvent: Product launch\n\nTime: Next month\n\n2. Financial Investigation\nDescription:\nUsing NLP to find hidden connections and patterns in financial documents, transaction logs, or emails that could indicate fraud, money laundering, or other financial crimes.\n\nExample:\nInput Text:\n“John transferred $5 million to XYZ Ltd., which is linked to offshore accounts.”\nExtracted Information:\n\nPerson: John\n\nAmount: $5 million\n\nCompany: XYZ Ltd.\n\nSuspicious Link: Offshore accounts\n\n3. Scientific Research\nDescription:\nAutomatically extracting references, key findings, or related work from scientific papers to assist researchers in literature review or paper recommendation systems.\n\nExample:\nInput Text:\n“The method proposed by Smith et al. (2020) improves neural network accuracy by 15%.”\nExtracted Information:\n\nAuthors: Smith et al.\n\nYear: 2020\n\nContribution: Improved neural network accuracy\n\nImprovement Percentage: 15%\n\n4. Media Monitoring\nDescription:\nTracking mentions of companies, products, or people across news articles, blogs, or social media to analyze public sentiment or reputation.\n\nExample:\nInput Text:\n“Brand X received positive feedback after their latest product release.”\nExtracted Information:\n\nBrand: Brand X\n\nSentiment: Positive feedback\n\nEvent: Product release\n\n5. Healthcare Records Management & Pharma Research\nDescription:\nExtracting patient information, diagnosis, medication, and treatment plans from unstructured clinical notes or research papers to improve healthcare delivery and pharmaceutical studies.\n\nExample:\nInput Text:\n“Patient diagnosed with diabetes type 2 was prescribed Metformin 500mg daily.”\nExtracted Information:\n\nPatient Condition: Diabetes type 2\n\nMedication: Metformin\n\nDosage: 500mg daily",
    "enhanced_text": "[NLP] Typical Information Extraction Applications\n\nBusiness Intelligence\nDescription:\nExtracting structured data from large volumes of text such as reports, emails, news articles, and social media to help business analysts make informed decisions. The goal is to identify key facts like company names, product launches, market trends, or financial figures automatically.\n\nExample:\nInput Text:\n“ABC Corp announced a 10% increase in revenue this quarter and plans to launch a new product next month.”\nExtracted Information:\n\nCompany: ABC Corp\n\nRevenue Growth: 10%\n\nEvent: Product launch\n\nTime: Next month\n\n2. Financial Investigation\nDescription:\nUsing NLP to find hidden connections and patterns in financial documents, transaction logs, or emails that could indicate fraud, money laundering, or other financial crimes.\n\nExample:\nInput Text:\n“John transferred $5 million to XYZ Ltd., which is linked to offshore accounts.”\nExtracted Information:\n\nPerson: John\n\nAmount: $5 million\n\nCompany: XYZ Ltd.\n\nSuspicious Link: Offshore accounts\n\n3. Scientific Research\nDescription:\nAutomatically extracting references, key findings, or related work from scientific papers to assist researchers in literature review or paper recommendation systems.\n\nExample:\nInput Text:\n“The method proposed by Smith et al. (2020) improves neural network accuracy by 15%.”\nExtracted Information:\n\nAuthors: Smith et al.\n\nYear: 2020\n\nContribution: Improved neural network accuracy\n\nImprovement Percentage: 15%\n\n4. Media Monitoring\nDescription:\nTracking mentions of companies, products, or people across news articles, blogs, or social media to analyze public sentiment or reputation.\n\nExample:\nInput Text:\n“Brand X received positive feedback after their latest product release.”\nExtracted Information:\n\nBrand: Brand X\n\nSentiment: Positive feedback\n\nEvent: Product release\n\n5. Healthcare Records Management & Pharma Research\nDescription:\nExtracting patient information, diagnosis, medication, and treatment plans from unstructured clinical notes or research papers to improve healthcare delivery and pharmaceutical studies.\n\nExample:\nInput Text:\n“Patient diagnosed with diabetes type 2 was prescribed Metformin 500mg daily.”\nExtracted Information:\n\nPatient Condition: Diabetes type 2\n\nMedication: Metformin\n\nDosage: 500mg daily",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(b) Typical Information Extraction Applications.txt",
    "file_name": "lec2-(b) Typical Information Extraction Applications.txt",
    "filename_keywords": [
      "information",
      "lec2",
      "applications",
      "typical",
      "extraction"
    ],
    "content_keywords": [
      "scientific research",
      "product",
      "xyz",
      "brand x",
      "scientific research\ndescription",
      "event",
      "media monitoring\ndescription",
      "patient condition",
      "person",
      "diabetes",
      "healthcare records management & pharma research",
      "year",
      "suspicious link",
      "metformin\n\ndosage",
      "extracted information",
      "time",
      "brand",
      "company",
      "abc corp",
      "example",
      "media monitoring",
      "improvement percentage",
      "financial investigation",
      "offshore",
      "input text",
      "smith",
      "healthcare records management",
      "automatically",
      "typical information extraction applications\n\nbusiness intelligence\ndescription",
      "authors",
      "next",
      "nlp",
      "contribution",
      "john",
      "financial investigation\ndescription",
      "positive",
      "brand x\n\nsentiment",
      "xyz ltd",
      "medication",
      "pharma research\ndescription",
      "abc corp\n\nrevenue growth",
      "patient",
      "metformin",
      "the",
      "extracting",
      "tracking",
      "using nlp",
      "abc",
      "john\n\namount",
      "improved"
    ],
    "technical_terms": [
      "product",
      "xyz",
      "brand x",
      "scientific research\ndescription",
      "event",
      "media monitoring\ndescription",
      "patient condition",
      "person",
      "diabetes",
      "year",
      "suspicious link",
      "metformin\n\ndosage",
      "extracted information",
      "time",
      "brand",
      "company",
      "abc corp",
      "example",
      "improvement percentage",
      "offshore",
      "input text",
      "smith",
      "healthcare records management",
      "automatically",
      "typical information extraction applications\n\nbusiness intelligence\ndescription",
      "authors",
      "next",
      "nlp",
      "contribution",
      "john",
      "financial investigation\ndescription",
      "positive",
      "brand x\n\nsentiment",
      "xyz ltd",
      "medication",
      "pharma research\ndescription",
      "abc corp\n\nrevenue growth",
      "patient",
      "metformin",
      "the",
      "extracting",
      "tracking",
      "using nlp",
      "abc",
      "john\n\namount",
      "improved"
    ],
    "all_keywords": [
      "information",
      "scientific research",
      "product",
      "xyz",
      "brand x",
      "scientific research\ndescription",
      "event",
      "media monitoring\ndescription",
      "patient condition",
      "person",
      "diabetes",
      "healthcare records management & pharma research",
      "year",
      "suspicious link",
      "metformin\n\ndosage",
      "extracted information",
      "time",
      "brand",
      "company",
      "typical",
      "example",
      "abc corp",
      "improvement percentage",
      "media monitoring",
      "financial investigation",
      "offshore",
      "input text",
      "smith",
      "healthcare records management",
      "automatically",
      "lec2",
      "typical information extraction applications\n\nbusiness intelligence\ndescription",
      "authors",
      "next",
      "nlp",
      "contribution",
      "john",
      "financial investigation\ndescription",
      "positive",
      "brand x\n\nsentiment",
      "xyz ltd",
      "medication",
      "pharma research\ndescription",
      "abc corp\n\nrevenue growth",
      "patient",
      "metformin",
      "the",
      "extracting",
      "tracking",
      "applications",
      "extraction",
      "using nlp",
      "abc",
      "john\n\namount",
      "improved"
    ],
    "keyword_string": "information scientific research product xyz brand x scientific research\ndescription event media monitoring\ndescription patient condition person diabetes healthcare records management & pharma research year suspicious link metformin\n\ndosage extracted information time brand company typical example abc corp improvement percentage media monitoring financial investigation offshore input text smith healthcare records management automatically lec2 typical information extraction applications\n\nbusiness intelligence\ndescription authors next nlp contribution john financial investigation\ndescription positive brand x\n\nsentiment xyz ltd medication pharma research\ndescription abc corp\n\nrevenue growth patient metformin the extracting tracking applications extraction using nlp abc john\n\namount improved",
    "token_count": 420,
    "word_count": 306,
    "sentence_count": 14,
    "paragraph_count": 29,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7285714285714285,
    "avg_sentence_length": 21.857142857142858,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": false,
    "content_type": "Technical, Structured"
  },
  {
    "document_id": 97,
    "document_hash": "ceebf5bd04d1",
    "content": "What is Information Retrieval (IR)?\n\nInformation Retrieval means finding useful information from a large collection of resources like websites, documents, images, or sounds.\n\nImagine you want to find answers to a question or get documents related to a topic. IR systems help you do that by searching through all available data and showing you the most relevant results.\n\nTwo ways to think about IR:\n\nDefinition 1:\nIt’s the process of getting information (like documents or websites) that match what you’re looking for from a big collection.\n\nDefinition 2:\nIt’s the science of searching through different types of data — like documents, images, or even sounds — to find exactly what you need.\n\nTypical IR Task:\n\nYou have a collection of documents (called a corpus).\n\nYou enter a search query (a few words or a question).\n\nThe IR system looks through the documents and gives you a list of documents ranked by how relevant they are to your query.\n\nHow Does an IR System Work? (Simple Overview)\n\nYou type your query into the system.\n\nThe system looks through its collection of documents.\n\nIt ranks these documents by how well they match your query.\n\nIt shows you the ranked list of documents.\n\nDetailed IR Process in Simple Terms:\n\nDocuments collection:\nThe system starts with many documents — articles, webpages, reports, etc.\n\nIndexing:\nThe system organizes and summarizes these documents in a way that makes searching faster. This is like making a \"map\" or \"catalog\" of what’s inside each document.\n\nQuery processing:\nWhen you enter your search words, the system processes your query to understand what you want.\n\nSearching:\nThe system compares your processed query to the index, trying to find documents that match best. It uses special models (like comparing words as vectors or calculating probabilities) to decide which documents are most relevant.\n\nRanking:\nFinally, it orders the documents from most to least relevant and shows you the list.",
    "enhanced_text": "[NLP] What is Information Retrieval (IR)?\n\nInformation Retrieval means finding useful information from a large collection of resources like websites, documents, images, or sounds.\n\nImagine you want to find answers to a question or get documents related to a topic. IR systems help you do that by searching through all available data and showing you the most relevant results.\n\nTwo ways to think about IR:\n\nDefinition 1:\nIt’s the process of getting information (like documents or websites) that match what you’re looking for from a big collection.\n\nDefinition 2:\nIt’s the science of searching through different types of data — like documents, images, or even sounds — to find exactly what you need.\n\nTypical IR Task:\n\nYou have a collection of documents (called a corpus).\n\nYou enter a search query (a few words or a question).\n\nThe IR system looks through the documents and gives you a list of documents ranked by how relevant they are to your query.\n\nHow Does an IR System Work? (Simple Overview)\n\nYou type your query into the system.\n\nThe system looks through its collection of documents.\n\nIt ranks these documents by how well they match your query.\n\nIt shows you the ranked list of documents.\n\nDetailed IR Process in Simple Terms:\n\nDocuments collection:\nThe system starts with many documents — articles, webpages, reports, etc.\n\nIndexing:\nThe system organizes and summarizes these documents in a way that makes searching faster. This is like making a \"map\" or \"catalog\" of what’s inside each document.\n\nQuery processing:\nWhen you enter your search words, the system processes your query to understand what you want.\n\nSearching:\nThe system compares your processed query to the index, trying to find documents that match best. It uses special models (like comparing words as vectors or calculating probabilities) to decide which documents are most relevant.\n\nRanking:\nFinally, it orders the documents from most to least relevant and shows you the list.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(c) Information Retrieval.txt",
    "file_name": "lec2-(c) Information Retrieval.txt",
    "filename_keywords": [
      "information",
      "lec2",
      "retrieval"
    ],
    "content_keywords": [
      "information retrieval",
      "query",
      "how does",
      "searching",
      "ir system work",
      "finally",
      "you",
      "ranking",
      "typical ir task",
      "detailed ir process",
      "when",
      "imagine",
      "this",
      "the ir",
      "simple overview",
      "map",
      "indexing",
      "catalog",
      "definition",
      "the",
      "simple terms",
      "what",
      "two",
      "documents"
    ],
    "technical_terms": [
      "information retrieval",
      "query",
      "how does",
      "searching",
      "ir system work",
      "finally",
      "you",
      "ranking",
      "typical ir task",
      "detailed ir process",
      "when",
      "imagine",
      "this",
      "the ir",
      "simple overview",
      "indexing",
      "definition",
      "the",
      "simple terms",
      "what",
      "two",
      "documents"
    ],
    "all_keywords": [
      "information",
      "information retrieval",
      "query",
      "how does",
      "searching",
      "ir system work",
      "finally",
      "you",
      "ranking",
      "typical ir task",
      "detailed ir process",
      "retrieval",
      "when",
      "lec2",
      "imagine",
      "this",
      "the ir",
      "simple overview",
      "map",
      "indexing",
      "catalog",
      "definition",
      "the",
      "simple terms",
      "what",
      "two",
      "documents"
    ],
    "keyword_string": "information information retrieval query how does searching ir system work finally you ranking typical ir task detailed ir process retrieval when lec2 imagine this the ir simple overview map indexing catalog definition the simple terms what two documents",
    "token_count": 390,
    "word_count": 317,
    "sentence_count": 21,
    "paragraph_count": 21,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.8128205128205128,
    "avg_sentence_length": 15.095238095238095,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 98,
    "document_hash": "e3d280d7eb61",
    "content": "What is Question Answering (QA)?\n\nQA systems are computer programs that answer questions posed in natural language. These systems often work by extracting facts from large sources like the web or databases and matching them to the question you ask.\n\nCommon Types of Questions in QA Systems\n\nFactoid Questions\n\nThese are simple fact-based questions.\n\nUsually start with question words like Who, What, When, Where.\n\nExample: “Who is the president of the USA?”\n\nAnswer: A specific fact like “Joe Biden.”\n\nList Questions\n\nQuestions that require multiple answers in a list form.\n\nExample: “What are the countries in the European Union?”\n\nAnswer: A list of countries (France, Germany, Italy, etc.).\n\nCasual Questions\n\nThese need explanations or reasons about something.\n\nOften start with How or Why.\n\nExample: “Why do leaves change color in autumn?”\n\nAnswer: An explanation involving science or facts.\n\nConfirmation Questions\n\nRequire a simple Yes or No answer.\n\nThese need the system to understand and reason based on facts or common sense.\n\nExample: “Is Paris the capital of Germany?”\n\nAnswer: “No.”\n\nHypothetical Questions\n\nAsk about imagined or “what if” situations.\n\nUsually start with “What would happen if…”\n\nThere are no fixed answers, often speculative.\n\nExample: “What would happen if humans could fly?”\n\nAnswer: Open-ended or speculative explanations.",
    "enhanced_text": "[NLP] What is Question Answering (QA)?\n\nQA systems are computer programs that answer questions posed in natural language. These systems often work by extracting facts from large sources like the web or databases and matching them to the question you ask.\n\nCommon Types of Questions in QA Systems\n\nFactoid Questions\n\nThese are simple fact-based questions.\n\nUsually start with question words like Who, What, When, Where.\n\nExample: “Who is the president of the USA?”\n\nAnswer: A specific fact like “Joe Biden.”\n\nList Questions\n\nQuestions that require multiple answers in a list form.\n\nExample: “What are the countries in the European Union?”\n\nAnswer: A list of countries (France, Germany, Italy, etc.).\n\nCasual Questions\n\nThese need explanations or reasons about something.\n\nOften start with How or Why.\n\nExample: “Why do leaves change color in autumn?”\n\nAnswer: An explanation involving science or facts.\n\nConfirmation Questions\n\nRequire a simple Yes or No answer.\n\nThese need the system to understand and reason based on facts or common sense.\n\nExample: “Is Paris the capital of Germany?”\n\nAnswer: “No.”\n\nHypothetical Questions\n\nAsk about imagined or “what if” situations.\n\nUsually start with “What would happen if…”\n\nThere are no fixed answers, often speculative.\n\nExample: “What would happen if humans could fly?”\n\nAnswer: Open-ended or speculative explanations.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(d) Question Answering.txt",
    "file_name": "lec2-(d) Question Answering.txt",
    "filename_keywords": [
      "question",
      "answering",
      "lec2"
    ],
    "content_keywords": [
      "joe biden",
      "yes",
      "how",
      "question answering",
      "who",
      "hypothetical questions\n\nask",
      "italy",
      "germany",
      "these",
      "france",
      "european union",
      "list questions\n\nquestions",
      "there",
      "example",
      "questions",
      "common types",
      "casual questions\n\nthese",
      "when",
      "confirmation questions\n\nrequire",
      "usa",
      "often",
      "is paris",
      "answer",
      "where",
      "open",
      "usually",
      "qa systems\n\nfactoid questions\n\nthese",
      "why",
      "what"
    ],
    "technical_terms": [
      "joe biden",
      "yes",
      "how",
      "question answering",
      "who",
      "hypothetical questions\n\nask",
      "italy",
      "germany",
      "these",
      "france",
      "european union",
      "list questions\n\nquestions",
      "there",
      "example",
      "questions",
      "common types",
      "casual questions\n\nthese",
      "when",
      "confirmation questions\n\nrequire",
      "usa",
      "often",
      "is paris",
      "answer",
      "where",
      "open",
      "usually",
      "qa systems\n\nfactoid questions\n\nthese",
      "why",
      "what"
    ],
    "all_keywords": [
      "joe biden",
      "yes",
      "how",
      "question answering",
      "who",
      "hypothetical questions\n\nask",
      "italy",
      "germany",
      "these",
      "france",
      "european union",
      "list questions\n\nquestions",
      "there",
      "example",
      "questions",
      "common types",
      "casual questions\n\nthese",
      "when",
      "lec2",
      "confirmation questions\n\nrequire",
      "usa",
      "often",
      "answering",
      "is paris",
      "answer",
      "question",
      "where",
      "open",
      "usually",
      "qa systems\n\nfactoid questions\n\nthese",
      "why",
      "what"
    ],
    "keyword_string": "joe biden yes how question answering who hypothetical questions\n\nask italy germany these france european union list questions\n\nquestions there example questions common types casual questions\n\nthese when lec2 confirmation questions\n\nrequire usa often answering is paris answer question where open usually qa systems\n\nfactoid questions\n\nthese why what",
    "token_count": 279,
    "word_count": 206,
    "sentence_count": 15,
    "paragraph_count": 28,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7383512544802867,
    "avg_sentence_length": 13.733333333333333,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 99,
    "document_hash": "fdecc4f42e78",
    "content": "What are Dialogue Systems?\n\nDialogue systems are computer programs designed to talk or interact with humans in ways both can understand. These interactions can happen through:\n\nSpeech (voice assistants like Siri or Alexa)\n\nText (chatbots on websites, messaging apps)\n\nGestures or graphics (like virtual avatars)\n\nOther sensory methods (haptics, animations, etc.)\n\nExamples of Dialogue Systems:\n\nChatGPT — A popular AI chatbot that answers questions and holds conversations in natural language.\n\nGemini — An example of a modern conversational AI system.\n\nRole of Generative AI in Dialogue Systems\n\nGenerative AI is crucial because it helps create new, meaningful content based on the input it receives. This means:\n\nIf you type or say something, the system can generate relevant responses or content.\n\nIt can create text, images, sounds, animations, 3D models, and more.\n\nFor example, you can ask ChatGPT to write a poem, generate a story, or explain a concept — all using generative AI.\n\nSimple Example Interaction:\n\nUser: \"What’s the weather like today?\"\nDialogue System: \"It’s sunny and 25°C in your area.\"",
    "enhanced_text": "[NLP] What are Dialogue Systems?\n\nDialogue systems are computer programs designed to talk or interact with humans in ways both can understand. These interactions can happen through:\n\nSpeech (voice assistants like Siri or Alexa)\n\nText (chatbots on websites, messaging apps)\n\nGestures or graphics (like virtual avatars)\n\nOther sensory methods (haptics, animations, etc.)\n\nExamples of Dialogue Systems:\n\nChatGPT — A popular AI chatbot that answers questions and holds conversations in natural language.\n\nGemini — An example of a modern conversational AI system.\n\nRole of Generative AI in Dialogue Systems\n\nGenerative AI is crucial because it helps create new, meaningful content based on the input it receives. This means:\n\nIf you type or say something, the system can generate relevant responses or content.\n\nIt can create text, images, sounds, animations, 3D models, and more.\n\nFor example, you can ask ChatGPT to write a poem, generate a story, or explain a concept — all using generative AI.\n\nSimple Example Interaction:\n\nUser: \"What’s the weather like today?\"\nDialogue System: \"It’s sunny and 25°C in your area.\"",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(e) Dialogue Systems.txt",
    "file_name": "lec2-(e) Dialogue Systems.txt",
    "filename_keywords": [
      "systems",
      "lec2",
      "dialogue"
    ],
    "content_keywords": [
      "examples",
      "text",
      "what’s the weather like today?",
      "user",
      "other",
      "simple example interaction",
      "these",
      "for",
      "dialogue systems\n\ngenerative ai",
      "dialogue systems",
      "generative ai",
      "role",
      "dialogue",
      "siri",
      "this",
      "gemini",
      "dialogue system",
      "chatgpt",
      "gestures",
      "alexa",
      "speech",
      "what"
    ],
    "technical_terms": [
      "examples",
      "text",
      "user",
      "other",
      "simple example interaction",
      "these",
      "for",
      "dialogue systems\n\ngenerative ai",
      "dialogue systems",
      "generative ai",
      "role",
      "dialogue",
      "siri",
      "this",
      "gemini",
      "dialogue system",
      "chatgpt",
      "gestures",
      "alexa",
      "speech",
      "what"
    ],
    "all_keywords": [
      "examples",
      "text",
      "what’s the weather like today?",
      "user",
      "other",
      "for",
      "these",
      "simple example interaction",
      "dialogue systems\n\ngenerative ai",
      "dialogue systems",
      "generative ai",
      "role",
      "dialogue",
      "lec2",
      "siri",
      "this",
      "gemini",
      "dialogue system",
      "chatgpt",
      "gestures",
      "systems",
      "alexa",
      "speech",
      "what"
    ],
    "keyword_string": "examples text what’s the weather like today? user other for these simple example interaction dialogue systems\n\ngenerative ai dialogue systems generative ai role dialogue lec2 siri this gemini dialogue system chatgpt gestures systems alexa speech what",
    "token_count": 235,
    "word_count": 171,
    "sentence_count": 11,
    "paragraph_count": 16,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7276595744680852,
    "avg_sentence_length": 15.545454545454545,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 100,
    "document_hash": "30001bc95ade",
    "content": "What is Text Summarization?\n\nText summarization is the process of creating a short, clear, and accurate summary from a longer text. It helps you quickly understand the main points without reading the entire document.\n\nFor example, search engines show short summaries (called snippets) under each search result to give you a quick idea of what the page is about.\n\nReal-World Example: Web Search Summarization\n\nWhen you search for “generative AI” on Google, you see a list of links. Under each link, there’s a short summary — a snippet taken from the web page. These snippets help you decide which link to click by showing key information in a few sentences.\n\nTypes of Summarization\n\nExtractive Summarization\n\nPicks important sentences or phrases directly from the original text.\n\nExample: Taking every 3rd or 4th sentence from a paragraph to form the summary.\n\nIt’s like highlighting key sentences.\n\nAbstractive Summarization\n\nUnderstands the main ideas and rewrites them in simpler, shorter language.\n\nThe summary may use new sentences or words not found exactly in the original text.\n\nSimilar to how humans write summaries by paraphrasing.\n\nSimple Example\n\nOriginal Text:\n“Generative AI refers to artificial intelligence systems that can create new content like text, images, or music. These systems learn patterns from data and generate creative outputs.”\n\nExtractive Summary:\n“Generative AI refers to artificial intelligence systems that can create new content like text, images, or music.”\n\nAbstractive Summary:\n“Generative AI creates new content by learning from existing data.”",
    "enhanced_text": "[NLP] What is Text Summarization?\n\nText summarization is the process of creating a short, clear, and accurate summary from a longer text. It helps you quickly understand the main points without reading the entire document.\n\nFor example, search engines show short summaries (called snippets) under each search result to give you a quick idea of what the page is about.\n\nReal-World Example: Web Search Summarization\n\nWhen you search for “generative AI” on Google, you see a list of links. Under each link, there’s a short summary — a snippet taken from the web page. These snippets help you decide which link to click by showing key information in a few sentences.\n\nTypes of Summarization\n\nExtractive Summarization\n\nPicks important sentences or phrases directly from the original text.\n\nExample: Taking every 3rd or 4th sentence from a paragraph to form the summary.\n\nIt’s like highlighting key sentences.\n\nAbstractive Summarization\n\nUnderstands the main ideas and rewrites them in simpler, shorter language.\n\nThe summary may use new sentences or words not found exactly in the original text.\n\nSimilar to how humans write summaries by paraphrasing.\n\nSimple Example\n\nOriginal Text:\n“Generative AI refers to artificial intelligence systems that can create new content like text, images, or music. These systems learn patterns from data and generate creative outputs.”\n\nExtractive Summary:\n“Generative AI refers to artificial intelligence systems that can create new content like text, images, or music.”\n\nAbstractive Summary:\n“Generative AI creates new content by learning from existing data.”",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(f) Summarization.txt",
    "file_name": "lec2-(f) Summarization.txt",
    "filename_keywords": [
      "summarization",
      "lec2"
    ],
    "content_keywords": [
      "text",
      "for",
      "real",
      "these",
      "under",
      "similar",
      "web search summarization\n\nwhen",
      "generative ai",
      "example",
      "extractive summary",
      "text summarization",
      "abstractive summary",
      "google",
      "summarization\n\nextractive summarization\n\npicks",
      "taking",
      "simple example\n\noriginal text",
      "types",
      "abstractive summarization\n\nunderstands",
      "the",
      "what",
      "world example"
    ],
    "technical_terms": [
      "text",
      "for",
      "real",
      "these",
      "under",
      "similar",
      "web search summarization\n\nwhen",
      "generative ai",
      "example",
      "extractive summary",
      "text summarization",
      "abstractive summary",
      "google",
      "summarization\n\nextractive summarization\n\npicks",
      "taking",
      "simple example\n\noriginal text",
      "types",
      "abstractive summarization\n\nunderstands",
      "the",
      "what",
      "world example"
    ],
    "all_keywords": [
      "text",
      "for",
      "real",
      "these",
      "under",
      "similar",
      "web search summarization\n\nwhen",
      "generative ai",
      "example",
      "extractive summary",
      "text summarization",
      "abstractive summary",
      "google",
      "lec2",
      "summarization\n\nextractive summarization\n\npicks",
      "taking",
      "simple example\n\noriginal text",
      "types",
      "abstractive summarization\n\nunderstands",
      "the",
      "summarization",
      "what",
      "world example"
    ],
    "keyword_string": "text for real these under similar web search summarization\n\nwhen generative ai example extractive summary text summarization abstractive summary google lec2 summarization\n\nextractive summarization\n\npicks taking simple example\n\noriginal text types abstractive summarization\n\nunderstands the summarization what world example",
    "token_count": 327,
    "word_count": 242,
    "sentence_count": 15,
    "paragraph_count": 18,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7400611620795107,
    "avg_sentence_length": 16.133333333333333,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 101,
    "document_hash": "d0c891da461e",
    "content": "What is Machine Translation (MT)?\n\nMachine Translation is the automatic process of converting text from one natural language (like English) into another language (like Portuguese or French).\n\nKey Challenges in Machine Translation\n\nLexical Ambiguity:\nA word can have multiple meanings. For example, the English word “bank” could mean a financial institution or the side of a river. The correct translation depends on the context.\n\nSyntactic Ambiguity:\nSentence structure can be unclear or complex, causing different possible interpretations and translations.\n\nBecause of these challenges, the system needs to understand the correct meaning and structure to provide an accurate translation.\n\nReal-World Example: Google Translate\n\nYou enter “Hello, how are you?” in English.\n\nGoogle Translate converts it into “Olá, como vai?” in Portuguese.\n\nThis shows a simple, direct translation from one language to another.\n\nAdvanced Example: DeepL Translator\n\nDeepL also translates “Hello, how are you?” to “Olá, como estás?” in Portuguese.\n\nIt offers extra features like:\n\nChoosing between Formal or Informal tone.\n\nProviding alternative translations.\n\nThis makes DeepL more nuanced and flexible than basic translation tools, adapting translations to the situation or style.\n\nSimple Example of Ambiguity\n\nEnglish:\n“He saw the bat.”\n\nPossible Translations in another language:\n\nBat (animal)\n\nBat (baseball equipment)\n\nThe system must decide which meaning fits the sentence to translate correctly.",
    "enhanced_text": "[NLP] What is Machine Translation (MT)?\n\nMachine Translation is the automatic process of converting text from one natural language (like English) into another language (like Portuguese or French).\n\nKey Challenges in Machine Translation\n\nLexical Ambiguity:\nA word can have multiple meanings. For example, the English word “bank” could mean a financial institution or the side of a river. The correct translation depends on the context.\n\nSyntactic Ambiguity:\nSentence structure can be unclear or complex, causing different possible interpretations and translations.\n\nBecause of these challenges, the system needs to understand the correct meaning and structure to provide an accurate translation.\n\nReal-World Example: Google Translate\n\nYou enter “Hello, how are you?” in English.\n\nGoogle Translate converts it into “Olá, como vai?” in Portuguese.\n\nThis shows a simple, direct translation from one language to another.\n\nAdvanced Example: DeepL Translator\n\nDeepL also translates “Hello, how are you?” to “Olá, como estás?” in Portuguese.\n\nIt offers extra features like:\n\nChoosing between Formal or Informal tone.\n\nProviding alternative translations.\n\nThis makes DeepL more nuanced and flexible than basic translation tools, adapting translations to the situation or style.\n\nSimple Example of Ambiguity\n\nEnglish:\n“He saw the bat.”\n\nPossible Translations in another language:\n\nBat (animal)\n\nBat (baseball equipment)\n\nThe system must decide which meaning fits the sentence to translate correctly.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(g) Machine Translation & Multilinguality.txt",
    "file_name": "lec2-(g) Machine Translation & Multilinguality.txt",
    "filename_keywords": [
      "translation",
      "machine",
      "multilinguality",
      "lec2"
    ],
    "content_keywords": [
      "because",
      "hello",
      "key challenges",
      "syntactic ambiguity",
      "google translate",
      "ambiguity\n\nenglish",
      "simple example",
      "french",
      "for",
      "real",
      "informal",
      "possible translations",
      "machine translation\n\nlexical ambiguity",
      "advanced example",
      "providing",
      "portuguese",
      "this",
      "deepl",
      "bat",
      "english",
      "choosing",
      "machine translation",
      "google translate\n\nyou",
      "the",
      "deepl translator\n\ndeepl",
      "sentence",
      "what",
      "world example",
      "formal"
    ],
    "technical_terms": [
      "because",
      "hello",
      "key challenges",
      "syntactic ambiguity",
      "google translate",
      "ambiguity\n\nenglish",
      "simple example",
      "french",
      "for",
      "real",
      "informal",
      "possible translations",
      "machine translation\n\nlexical ambiguity",
      "advanced example",
      "providing",
      "portuguese",
      "this",
      "deepl",
      "bat",
      "english",
      "choosing",
      "machine translation",
      "google translate\n\nyou",
      "the",
      "deepl translator\n\ndeepl",
      "sentence",
      "what",
      "world example",
      "formal"
    ],
    "all_keywords": [
      "because",
      "hello",
      "key challenges",
      "syntactic ambiguity",
      "google translate",
      "ambiguity\n\nenglish",
      "simple example",
      "french",
      "for",
      "real",
      "informal",
      "possible translations",
      "machine translation\n\nlexical ambiguity",
      "advanced example",
      "providing",
      "translation",
      "lec2",
      "portuguese",
      "this",
      "deepl",
      "multilinguality",
      "bat",
      "machine",
      "english",
      "choosing",
      "machine translation",
      "google translate\n\nyou",
      "the",
      "deepl translator\n\ndeepl",
      "sentence",
      "what",
      "world example",
      "formal"
    ],
    "keyword_string": "because hello key challenges syntactic ambiguity google translate ambiguity\n\nenglish simple example french for real informal possible translations machine translation\n\nlexical ambiguity advanced example providing translation lec2 portuguese this deepl multilinguality bat machine english choosing machine translation google translate\n\nyou the deepl translator\n\ndeepl sentence what world example formal",
    "token_count": 283,
    "word_count": 211,
    "sentence_count": 15,
    "paragraph_count": 22,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7455830388692579,
    "avg_sentence_length": 14.066666666666666,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 102,
    "document_hash": "cdf020f7ebc5",
    "content": "What is Speech Recognition?\n\nSpeech Recognition (also called Automatic Speech Recognition or ASR) is a technology that converts spoken language (human speech) into written text.\n\nIt allows computers and applications to understand and process what people say.\n\nOpposite Technology: Speech Synthesis (Text-to-Speech)\n\nSpeech Synthesis or Text-to-Speech (TTS) does the opposite — it converts written text into spoken words (an audio waveform).\n\nFor example, when your phone reads out a message aloud, it uses speech synthesis.\n\nHow Speech Recognition Works (Simple Explanation)\n\nWhen you speak, your voice produces sound waves (audio waveform).\n\nThe speech recognition system processes these sound waves to identify words and convert them into text.\n\nThis text can then be used by computers to perform tasks like sending messages, searching the web, or controlling devices.\n\nExample Applications of Speech Recognition\n\nVirtual Assistants: Siri, Alexa, Google Assistant\n\nTranscription Services: Converting meeting recordings to text\n\nVoice Commands: Controlling phones, cars, or smart home devices\n\nCustomer Support: Automated phone systems understanding spoken requests\n\nConceptual Visual\n\nImagine a graph showing a wavy line — this represents the sound waves of your speech. Speech recognition systems analyze these waves to understand and write down your spoken words.",
    "enhanced_text": "[NLP] What is Speech Recognition?\n\nSpeech Recognition (also called Automatic Speech Recognition or ASR) is a technology that converts spoken language (human speech) into written text.\n\nIt allows computers and applications to understand and process what people say.\n\nOpposite Technology: Speech Synthesis (Text-to-Speech)\n\nSpeech Synthesis or Text-to-Speech (TTS) does the opposite — it converts written text into spoken words (an audio waveform).\n\nFor example, when your phone reads out a message aloud, it uses speech synthesis.\n\nHow Speech Recognition Works (Simple Explanation)\n\nWhen you speak, your voice produces sound waves (audio waveform).\n\nThe speech recognition system processes these sound waves to identify words and convert them into text.\n\nThis text can then be used by computers to perform tasks like sending messages, searching the web, or controlling devices.\n\nExample Applications of Speech Recognition\n\nVirtual Assistants: Siri, Alexa, Google Assistant\n\nTranscription Services: Converting meeting recordings to text\n\nVoice Commands: Controlling phones, cars, or smart home devices\n\nCustomer Support: Automated phone systems understanding spoken requests\n\nConceptual Visual\n\nImagine a graph showing a wavy line — this represents the sound waves of your speech. Speech recognition systems analyze these waves to understand and write down your spoken words.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(h) speech recognition.txt",
    "file_name": "lec2-(h) speech recognition.txt",
    "filename_keywords": [
      "speech",
      "recognition",
      "lec2"
    ],
    "content_keywords": [
      "text",
      "opposite technology",
      "controlling",
      "for",
      "example applications",
      "asr",
      "how speech recognition works",
      "automatic speech recognition",
      "converting",
      "tts",
      "when",
      "siri",
      "voice commands",
      "speech recognition",
      "this",
      "google assistant\n\ntranscription services",
      "speech recognition\n\nvirtual assistants",
      "automated",
      "conceptual visual\n\nimagine",
      "simple explanation",
      "customer support",
      "the",
      "alexa",
      "speech",
      "speech synthesis",
      "what"
    ],
    "technical_terms": [
      "text",
      "opposite technology",
      "controlling",
      "for",
      "example applications",
      "asr",
      "how speech recognition works",
      "automatic speech recognition",
      "converting",
      "tts",
      "when",
      "siri",
      "voice commands",
      "speech recognition",
      "this",
      "google assistant\n\ntranscription services",
      "speech recognition\n\nvirtual assistants",
      "automated",
      "conceptual visual\n\nimagine",
      "simple explanation",
      "customer support",
      "the",
      "alexa",
      "speech",
      "speech synthesis",
      "what"
    ],
    "all_keywords": [
      "recognition",
      "text",
      "opposite technology",
      "controlling",
      "for",
      "example applications",
      "asr",
      "how speech recognition works",
      "automatic speech recognition",
      "converting",
      "tts",
      "when",
      "lec2",
      "siri",
      "voice commands",
      "speech recognition",
      "this",
      "google assistant\n\ntranscription services",
      "speech recognition\n\nvirtual assistants",
      "automated",
      "conceptual visual\n\nimagine",
      "simple explanation",
      "customer support",
      "the",
      "alexa",
      "speech",
      "speech synthesis",
      "what"
    ],
    "keyword_string": "recognition text opposite technology controlling for example applications asr how speech recognition works automatic speech recognition converting tts when lec2 siri voice commands speech recognition this google assistant\n\ntranscription services speech recognition\n\nvirtual assistants automated conceptual visual\n\nimagine simple explanation customer support the alexa speech speech synthesis what",
    "token_count": 245,
    "word_count": 194,
    "sentence_count": 10,
    "paragraph_count": 17,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7918367346938775,
    "avg_sentence_length": 19.4,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 103,
    "document_hash": "d806276837c9",
    "content": "What is Sentiment?\n\nSentiment means the positive, negative, or neutral feeling or attitude a writer shows toward something.\n\nFor example:\n\nA movie review might say, “The film was amazing!” (positive sentiment) or “The movie was boring.” (negative sentiment).\n\nA political editorial might express support or criticism for a candidate or policy.\n\nWhat is Sentiment Analysis?\n\nSentiment Analysis is the process of using computers to read and understand digital text (like reviews, tweets, or articles) and decide whether the writer’s attitude is:\n\nPositive (happy, supportive)\n\nNegative (angry, critical)\n\nNeutral (neutral or no strong feeling)\n\nSimple Example\n\nText: “I love this phone; the battery lasts all day!”\nSentiment: Positive\n\nText: “The service was terrible and the food was cold.”\nSentiment: Negative",
    "enhanced_text": "[NLP] What is Sentiment?\n\nSentiment means the positive, negative, or neutral feeling or attitude a writer shows toward something.\n\nFor example:\n\nA movie review might say, “The film was amazing!” (positive sentiment) or “The movie was boring.” (negative sentiment).\n\nA political editorial might express support or criticism for a candidate or policy.\n\nWhat is Sentiment Analysis?\n\nSentiment Analysis is the process of using computers to read and understand digital text (like reviews, tweets, or articles) and decide whether the writer’s attitude is:\n\nPositive (happy, supportive)\n\nNegative (angry, critical)\n\nNeutral (neutral or no strong feeling)\n\nSimple Example\n\nText: “I love this phone; the battery lasts all day!”\nSentiment: Positive\n\nText: “The service was terrible and the food was cold.”\nSentiment: Negative",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(i) Sentiment Analysis.txt",
    "file_name": "lec2-(i) Sentiment Analysis.txt",
    "filename_keywords": [
      "sentiment",
      "lec2",
      "analysis"
    ],
    "content_keywords": [
      "negative",
      "positive\n\ntext",
      "the",
      "sentiment analysis",
      "neutral",
      "sentiment",
      "what",
      "simple example\n\ntext",
      "positive",
      "for"
    ],
    "technical_terms": [
      "negative",
      "positive\n\ntext",
      "the",
      "sentiment analysis",
      "neutral",
      "sentiment",
      "what",
      "simple example\n\ntext",
      "positive",
      "for"
    ],
    "all_keywords": [
      "negative",
      "positive\n\ntext",
      "lec2",
      "the",
      "analysis",
      "sentiment analysis",
      "neutral",
      "sentiment",
      "what",
      "simple example\n\ntext",
      "positive",
      "for"
    ],
    "keyword_string": "negative positive\n\ntext lec2 the analysis sentiment analysis neutral sentiment what simple example\n\ntext positive for",
    "token_count": 166,
    "word_count": 119,
    "sentence_count": 6,
    "paragraph_count": 13,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7168674698795181,
    "avg_sentence_length": 19.833333333333332,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 104,
    "document_hash": "01a2827b31e0",
    "content": "What is Discourse Analysis?\n\nDiscourse Analysis studies how sentences connect and influence each other in a larger context like a paragraph or full document.\n\nIt looks beyond a single sentence to understand meaning based on what was said before.\n\nExample:\nSentence 1: “John didn’t come to work.”\nSentence 2: “He was sick.”\nSentence 2 makes more sense when we know Sentence 1.",
    "enhanced_text": "[NLP] What is Discourse Analysis?\n\nDiscourse Analysis studies how sentences connect and influence each other in a larger context like a paragraph or full document.\n\nIt looks beyond a single sentence to understand meaning based on what was said before.\n\nExample:\nSentence 1: “John didn’t come to work.”\nSentence 2: “He was sick.”\nSentence 2 makes more sense when we know Sentence 1.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(j) Discourse Analysis.txt",
    "file_name": "lec2-(j) Discourse Analysis.txt",
    "filename_keywords": [
      "discourse",
      "lec2",
      "analysis"
    ],
    "content_keywords": [
      "discourse analysis",
      "example",
      "sentence",
      "what",
      "john"
    ],
    "technical_terms": [
      "discourse analysis",
      "example",
      "sentence",
      "what",
      "john"
    ],
    "all_keywords": [
      "lec2",
      "analysis",
      "discourse analysis",
      "example",
      "sentence",
      "what",
      "john",
      "discourse"
    ],
    "keyword_string": "lec2 analysis discourse analysis example sentence what john discourse",
    "token_count": 77,
    "word_count": 62,
    "sentence_count": 4,
    "paragraph_count": 4,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.8051948051948052,
    "avg_sentence_length": 15.5,
    "readability_score": 100,
    "has_technical_terms": false,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "General"
  },
  {
    "document_id": 105,
    "document_hash": "44f5f85605cb",
    "content": "What is Pragmatic Analysis?\n\nPragmatic Analysis uses real-world knowledge and context to understand the actual meaning or intent behind a sentence.\n\nIt helps decide things like whether a sentence is a request, a command, or a question.\n\nExample:\n“Give me the pencil.”\n\nCould be a polite request or a strict order, depending on context.",
    "enhanced_text": "[NLP] What is Pragmatic Analysis?\n\nPragmatic Analysis uses real-world knowledge and context to understand the actual meaning or intent behind a sentence.\n\nIt helps decide things like whether a sentence is a request, a command, or a question.\n\nExample:\n“Give me the pencil.”\n\nCould be a polite request or a strict order, depending on context.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(k) Pragmatic Analysis.txt",
    "file_name": "lec2-(k) Pragmatic Analysis.txt",
    "filename_keywords": [
      "lec2",
      "analysis",
      "pragmatic"
    ],
    "content_keywords": [
      "pragmatic analysis",
      "could",
      "give",
      "example",
      "what"
    ],
    "technical_terms": [
      "pragmatic analysis",
      "could",
      "give",
      "example",
      "what"
    ],
    "all_keywords": [
      "pragmatic analysis",
      "lec2",
      "could",
      "analysis",
      "give",
      "example",
      "what",
      "pragmatic"
    ],
    "keyword_string": "pragmatic analysis lec2 could analysis give example what pragmatic",
    "token_count": 71,
    "word_count": 54,
    "sentence_count": 4,
    "paragraph_count": 5,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7605633802816901,
    "avg_sentence_length": 13.5,
    "readability_score": 100,
    "has_technical_terms": false,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "General"
  },
  {
    "document_id": 106,
    "document_hash": "240e53a3bf22",
    "content": "Popular NLP Libraries and Tools\n\nStanfordCoreNLP: A powerful toolkit for various NLP tasks, written in Java.\n\nNLTK (Natural Language Toolkit): A Python library for basic and educational NLP tasks.\n\nPyTorch: A Python deep learning framework used for building and training models.\n\nTextBlob: A Python library that simplifies text processing and analysis.\n\nspaCy: An industrial-strength Python library for advanced NLP tasks.\n\nPolyglot: A Python library that supports multilingual NLP tasks.\n\nscikit-learn: A Python machine learning library often used with NLP for classification and clustering.\n\nOpenAI API: Provides access to advanced AI models for text generation, understanding, and more.\n\nSpeechRecognition: A Python library for converting speech to text.\n\nGoogle Cloud Speech: A cloud-based API for speech recognition services.",
    "enhanced_text": "[NLP] Popular NLP Libraries and Tools\n\nStanfordCoreNLP: A powerful toolkit for various NLP tasks, written in Java.\n\nNLTK (Natural Language Toolkit): A Python library for basic and educational NLP tasks.\n\nPyTorch: A Python deep learning framework used for building and training models.\n\nTextBlob: A Python library that simplifies text processing and analysis.\n\nspaCy: An industrial-strength Python library for advanced NLP tasks.\n\nPolyglot: A Python library that supports multilingual NLP tasks.\n\nscikit-learn: A Python machine learning library often used with NLP for classification and clustering.\n\nOpenAI API: Provides access to advanced AI models for text generation, understanding, and more.\n\nSpeechRecognition: A Python library for converting speech to text.\n\nGoogle Cloud Speech: A cloud-based API for speech recognition services.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec2-(l) Popular NLP Libraries and Tools.txt",
    "file_name": "lec2-(l) Popular NLP Libraries and Tools.txt",
    "filename_keywords": [
      "tools",
      "lec2",
      "libraries",
      "nlp",
      "popular"
    ],
    "content_keywords": [
      "pytorch",
      "openai api",
      "a python",
      "google cloud speech",
      "speechrecognition",
      "popular nlp libraries",
      "nltk",
      "provides",
      "textblob",
      "tools\n\nstanfordcorenlp",
      "nlp",
      "polyglot",
      "natural language toolkit",
      "api",
      "python",
      "java"
    ],
    "technical_terms": [
      "pytorch",
      "openai api",
      "a python",
      "google cloud speech",
      "speechrecognition",
      "popular nlp libraries",
      "nltk",
      "provides",
      "textblob",
      "tools\n\nstanfordcorenlp",
      "nlp",
      "polyglot",
      "natural language toolkit",
      "api",
      "python",
      "java"
    ],
    "all_keywords": [
      "openai api",
      "python",
      "tools",
      "speechrecognition",
      "popular nlp libraries",
      "textblob",
      "google cloud speech",
      "pytorch",
      "lec2",
      "nltk",
      "provides",
      "libraries",
      "tools\n\nstanfordcorenlp",
      "nlp",
      "polyglot",
      "a python",
      "api",
      "java",
      "popular",
      "natural language toolkit"
    ],
    "keyword_string": "openai api python tools speechrecognition popular nlp libraries textblob google cloud speech pytorch lec2 nltk provides libraries tools\n\nstanfordcorenlp nlp polyglot a python api java popular natural language toolkit",
    "token_count": 179,
    "word_count": 116,
    "sentence_count": 10,
    "paragraph_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6480446927374302,
    "avg_sentence_length": 11.6,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 107,
    "document_hash": "27078891de6a",
    "content": "What is Search?\n\nSearch is a fundamental task where you look for information or documents that match your needs.\n\nIn NLP, search helps find relevant documents or even parts of documents — like a paragraph, section, or sentence — from a large collection.\n\nHow NLP Helps in Search\n\nNLP techniques process the text to understand the meaning of your query and the documents, making the search more accurate.\n\nRetrieval Models\n\nThese are methods or algorithms used to find and rank documents based on how well they match the user’s query. They help identify the most relevant documents quickly.",
    "enhanced_text": "[NLP] What is Search?\n\nSearch is a fundamental task where you look for information or documents that match your needs.\n\nIn NLP, search helps find relevant documents or even parts of documents — like a paragraph, section, or sentence — from a large collection.\n\nHow NLP Helps in Search\n\nNLP techniques process the text to understand the meaning of your query and the documents, making the search more accurate.\n\nRetrieval Models\n\nThese are methods or algorithms used to find and rank documents based on how well they match the user’s query. They help identify the most relevant documents quickly.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(a) Search.txt",
    "file_name": "lec3-(a) Search.txt",
    "filename_keywords": [
      "search",
      "lec3"
    ],
    "content_keywords": [
      "in nlp",
      "they",
      "retrieval models\n\nthese",
      "search",
      "how nlp helps",
      "nlp",
      "what",
      "search\n\nnlp"
    ],
    "technical_terms": [
      "in nlp",
      "they",
      "retrieval models\n\nthese",
      "search",
      "how nlp helps",
      "nlp",
      "what",
      "search\n\nnlp"
    ],
    "all_keywords": [
      "in nlp",
      "they",
      "retrieval models\n\nthese",
      "search",
      "how nlp helps",
      "lec3",
      "nlp",
      "what",
      "search\n\nnlp"
    ],
    "keyword_string": "in nlp they retrieval models\n\nthese search how nlp helps lec3 nlp what search\n\nnlp",
    "token_count": 113,
    "word_count": 98,
    "sentence_count": 6,
    "paragraph_count": 7,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.8672566371681416,
    "avg_sentence_length": 16.333333333333332,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 108,
    "document_hash": "e21d1bf66627",
    "content": "What is the Boolean Model?\n\nThe Boolean Model is a way to search documents based on whether certain keywords are present or absent in the document.\n\nEach document is treated like a set of keywords (words that appear in it).\n\nA user’s query is made up of keywords combined with Boolean operators like AND, OR, and NOT.\n\nYou can also use brackets to group parts of the query for clarity.\n\nHow does it work?\n\nThe system checks if the document matches the query exactly.\n\nThe output is simple: either the document is relevant (matches) or not relevant.\n\nThere is no ranking or partial matching — documents either satisfy the query or they don’t.\n\nWhy is the Boolean Model popular?\n\nIt’s easy to understand and use for simple searches.\n\nIt has a clear and formal structure.\n\nIt can be extended to add ranking if needed.\n\nIt’s efficient to implement for typical queries.",
    "enhanced_text": "[NLP] What is the Boolean Model?\n\nThe Boolean Model is a way to search documents based on whether certain keywords are present or absent in the document.\n\nEach document is treated like a set of keywords (words that appear in it).\n\nA user’s query is made up of keywords combined with Boolean operators like AND, OR, and NOT.\n\nYou can also use brackets to group parts of the query for clarity.\n\nHow does it work?\n\nThe system checks if the document matches the query exactly.\n\nThe output is simple: either the document is relevant (matches) or not relevant.\n\nThere is no ranking or partial matching — documents either satisfy the query or they don’t.\n\nWhy is the Boolean Model popular?\n\nIt’s easy to understand and use for simple searches.\n\nIt has a clear and formal structure.\n\nIt can be extended to add ranking if needed.\n\nIt’s efficient to implement for typical queries.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(b) Boolean Model.txt",
    "file_name": "lec3-(b) Boolean Model.txt",
    "filename_keywords": [
      "boolean",
      "model",
      "lec3"
    ],
    "content_keywords": [
      "the boolean model",
      "boolean",
      "boolean model",
      "not",
      "the",
      "how",
      "there",
      "why",
      "you",
      "what",
      "and",
      "each"
    ],
    "technical_terms": [
      "the boolean model",
      "boolean",
      "boolean model",
      "not",
      "the",
      "how",
      "there",
      "why",
      "you",
      "what",
      "and",
      "each"
    ],
    "all_keywords": [
      "boolean",
      "the boolean model",
      "boolean model",
      "not",
      "the",
      "how",
      "there",
      "why",
      "you",
      "model",
      "lec3",
      "what",
      "and",
      "each"
    ],
    "keyword_string": "boolean the boolean model boolean model not the how there why you model lec3 what and each",
    "token_count": 188,
    "word_count": 151,
    "sentence_count": 14,
    "paragraph_count": 14,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.8031914893617021,
    "avg_sentence_length": 10.785714285714286,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 109,
    "document_hash": "b9e5ed0d1fc5",
    "content": "Problems with the Boolean Model\n\nVery rigid:\n\nAND means all keywords must be present in a document.\n\nOR means any keyword can be present.\nThis makes it hard to express complex or nuanced user requests.\n\nDifficult to control the number of documents retrieved:\n\nAll documents that match the query are returned, often leading to too many or too few results.\n\nNo easy way to adjust how many documents you get back.\n\nNo ranking of results:\n\nAll matched documents satisfy the query equally, so there's no way to order them by relevance.\n\nDifficult to perform relevance feedback:\n\nWhen a user marks a document as relevant or irrelevant, it’s unclear how to modify the query to improve results.\n\nExample (Document-Term Matrix)\n\nImagine a table where:\n\nRows = documents (D1 to D5)\n\nColumns = terms (T1 to T6)\n\nEach cell shows if a term appears in a document (present or absent).\n\nExample queries and results:\n\nQuery: Islamabad OR University\n\nFinds documents containing either \"Islamabad\" or \"University\".\n\nReturns documents: D1, D2, D3, and D4.\n\nQuery: Islamabad OR Capital\n\nFinds documents with \"Islamabad\" or \"Capital\".\n\nReturns: D1, D2, D3, D4, and D5.\n\nQuery: Islamabad NOT University\n\nFinds documents with \"Islamabad\" but not \"University\".\n\nReturns: D2, D3, and D4.\n\nFeast or Famine Problem\n\nBoolean queries often return too few or too many results.\n\nUsing AND narrows results too much, often too few documents.\n\nUsing OR broadens results too much, often too many documents.\n\nIt takes skill to write queries that return a useful number of documents.\n\nEvaluating Results\n\nWhen using Boolean search, you need to check:\n\nAre the correct documents included?\n\nHow many incorrect or irrelevant documents are included?\n\nHow do you know if the model is trustworthy and effective?",
    "enhanced_text": "[NLP] Problems with the Boolean Model\n\nVery rigid:\n\nAND means all keywords must be present in a document.\n\nOR means any keyword can be present.\nThis makes it hard to express complex or nuanced user requests.\n\nDifficult to control the number of documents retrieved:\n\nAll documents that match the query are returned, often leading to too many or too few results.\n\nNo easy way to adjust how many documents you get back.\n\nNo ranking of results:\n\nAll matched documents satisfy the query equally, so there's no way to order them by relevance.\n\nDifficult to perform relevance feedback:\n\nWhen a user marks a document as relevant or irrelevant, it’s unclear how to modify the query to improve results.\n\nExample (Document-Term Matrix)\n\nImagine a table where:\n\nRows = documents (D1 to D5)\n\nColumns = terms (T1 to T6)\n\nEach cell shows if a term appears in a document (present or absent).\n\nExample queries and results:\n\nQuery: Islamabad OR University\n\nFinds documents containing either \"Islamabad\" or \"University\".\n\nReturns documents: D1, D2, D3, and D4.\n\nQuery: Islamabad OR Capital\n\nFinds documents with \"Islamabad\" or \"Capital\".\n\nReturns: D1, D2, D3, D4, and D5.\n\nQuery: Islamabad NOT University\n\nFinds documents with \"Islamabad\" but not \"University\".\n\nReturns: D2, D3, and D4.\n\nFeast or Famine Problem\n\nBoolean queries often return too few or too many results.\n\nUsing AND narrows results too much, often too few documents.\n\nUsing OR broadens results too much, often too many documents.\n\nIt takes skill to write queries that return a useful number of documents.\n\nEvaluating Results\n\nWhen using Boolean search, you need to check:\n\nAre the correct documents included?\n\nHow many incorrect or irrelevant documents are included?\n\nHow do you know if the model is trustworthy and effective?",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(c)Boolean Models Problems.txt",
    "file_name": "lec3-(c)Boolean Models Problems.txt",
    "filename_keywords": [
      "models",
      "boolean",
      "lec3",
      "problems"
    ],
    "content_keywords": [
      "islamabad",
      "query",
      "problems",
      "term matrix",
      "how",
      "using or",
      "capital",
      "islamabad or university\n\nfinds",
      "university",
      "all",
      "example",
      "islamabad or capital\n\nfinds",
      "evaluating results\n\nwhen",
      "boolean model\n\nvery",
      "and",
      "rows",
      "islamabad not university\n\nfinds",
      "when",
      "feast",
      "imagine",
      "using and",
      "this",
      "columns",
      "are",
      "boolean",
      "famine problem\n\nboolean",
      "not",
      "returns",
      "document",
      "each",
      "difficult"
    ],
    "technical_terms": [
      "islamabad",
      "query",
      "problems",
      "term matrix",
      "how",
      "using or",
      "capital",
      "islamabad or university\n\nfinds",
      "university",
      "all",
      "example",
      "islamabad or capital\n\nfinds",
      "evaluating results\n\nwhen",
      "boolean model\n\nvery",
      "and",
      "rows",
      "islamabad not university\n\nfinds",
      "when",
      "feast",
      "imagine",
      "using and",
      "this",
      "columns",
      "are",
      "boolean",
      "famine problem\n\nboolean",
      "not",
      "returns",
      "document",
      "each",
      "difficult"
    ],
    "all_keywords": [
      "islamabad",
      "query",
      "problems",
      "term matrix",
      "how",
      "using or",
      "capital",
      "islamabad or university\n\nfinds",
      "university",
      "all",
      "example",
      "islamabad or capital\n\nfinds",
      "evaluating results\n\nwhen",
      "boolean model\n\nvery",
      "and",
      "rows",
      "islamabad not university\n\nfinds",
      "models",
      "when",
      "feast",
      "imagine",
      "using and",
      "this",
      "columns",
      "are",
      "boolean",
      "famine problem\n\nboolean",
      "not",
      "returns",
      "document",
      "lec3",
      "each",
      "difficult"
    ],
    "keyword_string": "islamabad query problems term matrix how using or capital islamabad or university\n\nfinds university all example islamabad or capital\n\nfinds evaluating results\n\nwhen boolean model\n\nvery and rows islamabad not university\n\nfinds models when feast imagine using and this columns are boolean famine problem\n\nboolean not returns document lec3 each difficult",
    "token_count": 387,
    "word_count": 284,
    "sentence_count": 21,
    "paragraph_count": 36,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7338501291989664,
    "avg_sentence_length": 13.523809523809524,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 110,
    "document_hash": "d71dbfaad95e",
    "content": "What is Text Classification?\n\nText Classification is the process of assigning a label or category to a piece of text based on its content.\n\nCommon Applications of Text Classification\n\nAssigning subject categories or topics:\nAutomatically sorting news articles into categories like sports, politics, or technology.\n\nSpam detection:\nIdentifying unwanted emails or messages and filtering them out.\n\nAuthorship identification:\nFiguring out who wrote a particular text based on writing style.\n\nAge/gender identification:\nPredicting the age group or gender of the writer from their text.\n\nLanguage identification:\nDetermining the language of a given text, like English, Spanish, or French.\n\nSentiment analysis:\nDetecting if the text expresses positive, negative, or neutral feelings.",
    "enhanced_text": "[NLP] What is Text Classification?\n\nText Classification is the process of assigning a label or category to a piece of text based on its content.\n\nCommon Applications of Text Classification\n\nAssigning subject categories or topics:\nAutomatically sorting news articles into categories like sports, politics, or technology.\n\nSpam detection:\nIdentifying unwanted emails or messages and filtering them out.\n\nAuthorship identification:\nFiguring out who wrote a particular text based on writing style.\n\nAge/gender identification:\nPredicting the age group or gender of the writer from their text.\n\nLanguage identification:\nDetermining the language of a given text, like English, Spanish, or French.\n\nSentiment analysis:\nDetecting if the text expresses positive, negative, or neutral feelings.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(d) Text Classification.txt",
    "file_name": "lec3-(d) Text Classification.txt",
    "filename_keywords": [
      "lec3",
      "text",
      "classification"
    ],
    "content_keywords": [
      "text classification\n\nassigning",
      "common applications",
      "automatically",
      "age",
      "predicting",
      "language",
      "text classification",
      "figuring",
      "english",
      "identifying",
      "spanish",
      "spam",
      "detecting",
      "determining",
      "what",
      "sentiment",
      "authorship",
      "french"
    ],
    "technical_terms": [
      "text classification\n\nassigning",
      "common applications",
      "automatically",
      "age",
      "predicting",
      "language",
      "text classification",
      "figuring",
      "english",
      "identifying",
      "spanish",
      "spam",
      "detecting",
      "determining",
      "what",
      "sentiment",
      "authorship",
      "french"
    ],
    "all_keywords": [
      "language",
      "text",
      "figuring",
      "french",
      "authorship",
      "text classification\n\nassigning",
      "predicting",
      "text classification",
      "identifying",
      "spanish",
      "detecting",
      "classification",
      "common applications",
      "automatically",
      "age",
      "spam",
      "determining",
      "english",
      "lec3",
      "sentiment",
      "what"
    ],
    "keyword_string": "language text figuring french authorship text classification\n\nassigning predicting text classification identifying spanish detecting classification common applications automatically age spam determining english lec3 sentiment what",
    "token_count": 135,
    "word_count": 109,
    "sentence_count": 8,
    "paragraph_count": 9,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.8074074074074075,
    "avg_sentence_length": 13.625,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 111,
    "document_hash": "8f80b3e8506e",
    "content": "Naive Bayes Classifier\n\nNaive Bayes is a popular classification algorithm used for both binary and multi-class problems. It’s based on Bayes’ theorem, which calculates the probability of a class given some observed data.\n\nConceptual Example: Fruit Classification\n\nImagine you have a mixed basket of fruits—strawberries, watermelons, and bananas. A classifier looks at each fruit and sorts them into groups based on their characteristics. Similarly, Naive Bayes looks at data points and assigns them to categories based on the probability that they belong there.\n\nBag of Words Representation (for Text)\n\nText data needs to be converted into numbers for the classifier to work. One common way is the Bag of Words model:\n\nThe text is broken into unique words.\n\nFor each word, count how many times it appears in the text.\n\nThe order and grammar of words are ignored.\n\nExample:\n\"I love this movie! It’s sweet, but with satirical humor.\"\n\nWords like \"It\" might appear 6 times, \"the\" 4 times, \"and\" 3 times, etc.\n\nThis numeric representation helps Naive Bayes understand the content of the text.\n\nHow Naive Bayes Works\n\nAssumption: All features (words) are independent from each other (this is the \"naive\" assumption).\n\nBayes’ theorem: It calculates the probability that a text belongs to a class based on the presence of words.\n\nExample: Movie Review Sentiment Classification\n\nReview: \"This movie was great!\"\nLabel: Positive\n\nReview: \"The acting was terrible\"\nLabel: Negative\n\nReview: \"I love this film\"\nLabel: Positive\n\nReview: \"It was boring and dull\"\nLabel: Negative\n\nGoal: Given a new review, classify it as Positive or Negative based on the words it contains and what the classifier has learned.\n\nSteps to Build Naive Bayes Classifier\n\nData Preparation:\nSplit the reviews into individual words (tokenization) and create a vocabulary of all unique words.\n\nFeature Extraction:\nRepresent each review as a vector indicating which words are present or absent.\nFor example, “This movie was great!” might become:\n[1, 1, 1, 0, 0, 0, ...] where 1 means the word exists in the review.\n\nTraining:\nCalculate the probability of each word appearing in positive and negative reviews.\nCalculate the prior probabilities of positive and negative classes (how common each class is).\n\nClassification:\nFor a new review, compute the probability it is positive or negative using Bayes’ theorem.\nAssign the label with the higher probability.\n\nEvaluation:\nTest the classifier on new data using metrics such as accuracy, precision, recall, and F1-score.",
    "enhanced_text": "[NLP] Naive Bayes Classifier\n\nNaive Bayes is a popular classification algorithm used for both binary and multi-class problems. It’s based on Bayes’ theorem, which calculates the probability of a class given some observed data.\n\nConceptual Example: Fruit Classification\n\nImagine you have a mixed basket of fruits—strawberries, watermelons, and bananas. A classifier looks at each fruit and sorts them into groups based on their characteristics. Similarly, Naive Bayes looks at data points and assigns them to categories based on the probability that they belong there.\n\nBag of Words Representation (for Text)\n\nText data needs to be converted into numbers for the classifier to work. One common way is the Bag of Words model:\n\nThe text is broken into unique words.\n\nFor each word, count how many times it appears in the text.\n\nThe order and grammar of words are ignored.\n\nExample:\n\"I love this movie! It’s sweet, but with satirical humor.\"\n\nWords like \"It\" might appear 6 times, \"the\" 4 times, \"and\" 3 times, etc.\n\nThis numeric representation helps Naive Bayes understand the content of the text.\n\nHow Naive Bayes Works\n\nAssumption: All features (words) are independent from each other (this is the \"naive\" assumption).\n\nBayes’ theorem: It calculates the probability that a text belongs to a class based on the presence of words.\n\nExample: Movie Review Sentiment Classification\n\nReview: \"This movie was great!\"\nLabel: Positive\n\nReview: \"The acting was terrible\"\nLabel: Negative\n\nReview: \"I love this film\"\nLabel: Positive\n\nReview: \"It was boring and dull\"\nLabel: Negative\n\nGoal: Given a new review, classify it as Positive or Negative based on the words it contains and what the classifier has learned.\n\nSteps to Build Naive Bayes Classifier\n\nData Preparation:\nSplit the reviews into individual words (tokenization) and create a vocabulary of all unique words.\n\nFeature Extraction:\nRepresent each review as a vector indicating which words are present or absent.\nFor example, “This movie was great!” might become:\n[1, 1, 1, 0, 0, 0, ...] where 1 means the word exists in the review.\n\nTraining:\nCalculate the probability of each word appearing in positive and negative reviews.\nCalculate the prior probabilities of positive and negative classes (how common each class is).\n\nClassification:\nFor a new review, compute the probability it is positive or negative using Bayes’ theorem.\nAssign the label with the higher probability.\n\nEvaluation:\nTest the classifier on new data using metrics such as accuracy, precision, recall, and F1-score.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(e) Naive Bayes Classifier.txt",
    "file_name": "lec3-(e) Naive Bayes Classifier.txt",
    "filename_keywords": [
      "bayes",
      "classifier",
      "lec3",
      "naive"
    ],
    "content_keywords": [
      "evaluation",
      "text",
      "label",
      "test",
      "bayes",
      "build naive bayes classifier\n\ndata preparation",
      "split",
      "words like",
      "4 times,",
      "training",
      "calculate",
      "for",
      "positive\n\nreview",
      "fruit classification\n\nimagine",
      "negative",
      "might appear 6 times,",
      "words representation",
      "how naive bayes works\n\nassumption",
      "all",
      "example",
      "the acting was terrible",
      "bag",
      "given",
      "assign",
      "words",
      "one",
      "represent",
      "naive bayes",
      "classification",
      "movie review sentiment classification\n\nreview",
      "steps",
      "this",
      "naive",
      "negative\n\nreview",
      "it was boring and dull",
      "positive",
      "feature extraction",
      "i love this film",
      "the",
      "this movie was great!",
      "conceptual example",
      "negative\n\ngoal",
      "similarly",
      "naive bayes classifier\n\nnaive bayes"
    ],
    "technical_terms": [
      "evaluation",
      "text",
      "label",
      "test",
      "bayes",
      "build naive bayes classifier\n\ndata preparation",
      "split",
      "training",
      "calculate",
      "for",
      "positive\n\nreview",
      "fruit classification\n\nimagine",
      "negative",
      "words representation",
      "how naive bayes works\n\nassumption",
      "all",
      "example",
      "bag",
      "given",
      "assign",
      "words",
      "one",
      "represent",
      "naive bayes",
      "classification",
      "movie review sentiment classification\n\nreview",
      "steps",
      "this",
      "negative\n\nreview",
      "positive",
      "feature extraction",
      "the",
      "conceptual example",
      "negative\n\ngoal",
      "similarly",
      "naive bayes classifier\n\nnaive bayes"
    ],
    "all_keywords": [
      "classifier",
      "evaluation",
      "text",
      "label",
      "test",
      "bayes",
      "build naive bayes classifier\n\ndata preparation",
      "split",
      "words like",
      "4 times,",
      "training",
      "calculate",
      "for",
      "positive\n\nreview",
      "fruit classification\n\nimagine",
      "negative",
      "might appear 6 times,",
      "words representation",
      "how naive bayes works\n\nassumption",
      "all",
      "example",
      "the acting was terrible",
      "bag",
      "given",
      "assign",
      "words",
      "one",
      "represent",
      "naive bayes",
      "classification",
      "movie review sentiment classification\n\nreview",
      "steps",
      "this",
      "naive",
      "negative\n\nreview",
      "it was boring and dull",
      "positive",
      "feature extraction",
      "i love this film",
      "the",
      "this movie was great!",
      "conceptual example",
      "lec3",
      "negative\n\ngoal",
      "similarly",
      "naive bayes classifier\n\nnaive bayes"
    ],
    "keyword_string": "classifier evaluation text label test bayes build naive bayes classifier\n\ndata preparation split words like 4 times, training calculate for positive\n\nreview fruit classification\n\nimagine negative might appear 6 times, words representation how naive bayes works\n\nassumption all example the acting was terrible bag given assign words one represent naive bayes classification movie review sentiment classification\n\nreview steps this naive negative\n\nreview it was boring and dull positive feature extraction i love this film the this movie was great! conceptual example lec3 negative\n\ngoal similarly naive bayes classifier\n\nnaive bayes",
    "token_count": 537,
    "word_count": 397,
    "sentence_count": 25,
    "paragraph_count": 27,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7392923649906891,
    "avg_sentence_length": 15.88,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 112,
    "document_hash": "35a9b2bc4401",
    "content": "Evaluation Measures\n\nAfter an NLP task (like classification or extraction) is performed, we need to check how well the system did. This is called evaluation.\n\nThere are two main ways to evaluate:\n\nManual evaluation: Experts look at the results and judge how good they are.\n\nAutomated evaluation: The system calculates scores using formulas to measure performance.\n\nMost automated evaluation methods use something called a Confusion Matrix, which compares the system’s predictions with the true answers.\n\nCommon metrics derived from the Confusion Matrix include:\n\nAccuracy: How many predictions were correct out of all predictions.\n\nPrecision: Out of all the results the system said were positive, how many were actually positive.\n\nRecall: Out of all the actual positives, how many did the system correctly find.\n\nF-measure (F1-score): The harmonic mean of precision and recall, balancing both.",
    "enhanced_text": "[NLP] Evaluation Measures\n\nAfter an NLP task (like classification or extraction) is performed, we need to check how well the system did. This is called evaluation.\n\nThere are two main ways to evaluate:\n\nManual evaluation: Experts look at the results and judge how good they are.\n\nAutomated evaluation: The system calculates scores using formulas to measure performance.\n\nMost automated evaluation methods use something called a Confusion Matrix, which compares the system’s predictions with the true answers.\n\nCommon metrics derived from the Confusion Matrix include:\n\nAccuracy: How many predictions were correct out of all predictions.\n\nPrecision: Out of all the results the system said were positive, how many were actually positive.\n\nRecall: Out of all the actual positives, how many did the system correctly find.\n\nF-measure (F1-score): The harmonic mean of precision and recall, balancing both.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(f) Evaluation measure.txt",
    "file_name": "lec3-(f) Evaluation measure.txt",
    "filename_keywords": [
      "lec3",
      "evaluation",
      "measure"
    ],
    "content_keywords": [
      "precision",
      "accuracy",
      "experts",
      "recall",
      "the",
      "there",
      "confusion matrix",
      "how",
      "manual",
      "evaluation measures\n\nafter",
      "this",
      "nlp",
      "common",
      "most",
      "out",
      "automated"
    ],
    "technical_terms": [
      "precision",
      "accuracy",
      "experts",
      "recall",
      "the",
      "there",
      "confusion matrix",
      "how",
      "manual",
      "evaluation measures\n\nafter",
      "this",
      "nlp",
      "common",
      "most",
      "out",
      "automated"
    ],
    "all_keywords": [
      "evaluation",
      "recall",
      "confusion matrix",
      "how",
      "manual",
      "most",
      "precision",
      "there",
      "common",
      "out",
      "accuracy",
      "experts",
      "measure",
      "this",
      "nlp",
      "automated",
      "the",
      "evaluation measures\n\nafter",
      "lec3"
    ],
    "keyword_string": "evaluation recall confusion matrix how manual most precision there common out accuracy experts measure this nlp automated the evaluation measures\n\nafter lec3",
    "token_count": 170,
    "word_count": 134,
    "sentence_count": 9,
    "paragraph_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.788235294117647,
    "avg_sentence_length": 14.88888888888889,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 113,
    "document_hash": "0f9df3ff6d2c",
    "content": "Confusion Matrix\n\nA confusion matrix helps us see where a classification model is making mistakes—where it gets \"confused\" between different classes.\n\nThe rows show the actual classes (the true labels).\n\nThe columns show the predicted classes (what the model guessed).\n\nFor a binary classification (two classes, like Positive and Negative), the confusion matrix has four parts:\n\nTrue Positive (TP): The model correctly predicted Positive when the actual class is Positive.\n\nFalse Positive (FP): The model predicted Positive but the actual class is Negative (a wrong positive).\n\nFalse Negative (FN): The model predicted Negative but the actual class is Positive (a missed positive).\n\nTrue Negative (TN): The model correctly predicted Negative when the actual class is Negative.\n\nThis matrix is essential because it helps calculate evaluation metrics like accuracy, precision, and recall.\n\nFor more than two classes (multi-class), the confusion matrix grows into an N x N matrix, where N is the number of classes.\n\nFor example, with three classes like urgent, normal, and spam:\n\nThe matrix shows how many items of each actual class were predicted as each possible class.\n\nThe diagonal cells represent correct predictions for each class (true positives).\n\nThe off-diagonal cells show errors, like an \"urgent\" message wrongly classified as \"normal.\"\n\nThis helps calculate class-specific precision and recall, so you know how well the model performs on each category.",
    "enhanced_text": "[NLP] Confusion Matrix\n\nA confusion matrix helps us see where a classification model is making mistakes—where it gets \"confused\" between different classes.\n\nThe rows show the actual classes (the true labels).\n\nThe columns show the predicted classes (what the model guessed).\n\nFor a binary classification (two classes, like Positive and Negative), the confusion matrix has four parts:\n\nTrue Positive (TP): The model correctly predicted Positive when the actual class is Positive.\n\nFalse Positive (FP): The model predicted Positive but the actual class is Negative (a wrong positive).\n\nFalse Negative (FN): The model predicted Negative but the actual class is Positive (a missed positive).\n\nTrue Negative (TN): The model correctly predicted Negative when the actual class is Negative.\n\nThis matrix is essential because it helps calculate evaluation metrics like accuracy, precision, and recall.\n\nFor more than two classes (multi-class), the confusion matrix grows into an N x N matrix, where N is the number of classes.\n\nFor example, with three classes like urgent, normal, and spam:\n\nThe matrix shows how many items of each actual class were predicted as each possible class.\n\nThe diagonal cells represent correct predictions for each class (true positives).\n\nThe off-diagonal cells show errors, like an \"urgent\" message wrongly classified as \"normal.\"\n\nThis helps calculate class-specific precision and recall, so you know how well the model performs on each category.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(g) Confusion Matrix.txt",
    "file_name": "lec3-(g) Confusion Matrix.txt",
    "filename_keywords": [
      "matrix",
      "lec3",
      "confusion"
    ],
    "content_keywords": [
      "negative",
      "false negative",
      "false positive",
      "the",
      "confusion matrix\n\na",
      "confused",
      "this",
      "true negative",
      "urgent",
      "normal.",
      "positive",
      "true positive",
      "for"
    ],
    "technical_terms": [
      "negative",
      "false negative",
      "false positive",
      "the",
      "confusion matrix\n\na",
      "this",
      "true negative",
      "positive",
      "true positive",
      "for"
    ],
    "all_keywords": [
      "negative",
      "false negative",
      "false positive",
      "matrix",
      "the",
      "confusion matrix\n\na",
      "confusion",
      "confused",
      "this",
      "lec3",
      "true negative",
      "urgent",
      "normal.",
      "positive",
      "true positive",
      "for"
    ],
    "keyword_string": "negative false negative false positive matrix the confusion matrix\n\na confusion confused this lec3 true negative urgent normal. positive true positive for",
    "token_count": 294,
    "word_count": 222,
    "sentence_count": 13,
    "paragraph_count": 16,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7551020408163265,
    "avg_sentence_length": 17.076923076923077,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 114,
    "document_hash": "75315d2983c6",
    "content": "1. Accuracy\n\nDefinition:\n\nAccuracy is a performance metric that measures the overall correctness of a classification model. It represents the proportion of total instances (or predictions) that the model classified correctly, encompassing both true positives and true negatives.\n\nFormula:\n\nAccuracy = (TP + TN) / (TP + FP + FN + TN)\n\nExplanation:\n\n* TP (True Positives): The number of positive instances that were correctly classified as positive.\n\n* TN (True Negatives): The number of negative instances that were correctly classified as negative.\n\n* FP (False Positives): The number of negative instances that were incorrectly classified as positive (also known as a Type I error).\n\n* FN (False Negatives): The number of positive instances that were incorrectly classified as negative (also known as a Type II error).\n\n* The numerator (TP + TN) represents the total number of correct predictions made by the model.\n\n* The denominator (TP + FP + FN + TN) represents the total number of instances or predictions made.\n\n* Accuracy provides a general sense of how often the classifier is right overall. However, it can be misleading for imbalanced datasets (where one class significantly outnumbers others).\n\n________________________________________\n\n2. Precision\n\nDefinition:\n\nPrecision is a performance metric that measures the model’s ability to correctly identify only the relevant (positive) data points from all the instances it predicted as positive. It answers the question: \"Of all the instances that the model labeled as positive, how many were actually positive?\"\n\nFormula:\n\nPrecision = TP / (TP + FP)\n\nExplanation:\n\n* TP (True Positives): The number of positive instances correctly identified as positive.\n\n* FP (False Positives): The number of negative instances incorrectly identified as positive.\n\n* The denominator (TP + FP) represents the total number of instances the model predicted as positive.\n\n* Precision focuses on the reliability of the positive predictions.\n\n* High precision indicates that the model makes few false positive errors. For example, if a spam filter has high precision, it means that when it flags an email as spam, it is very likely to actually be spam.\n\n________________________________________\n\n3. Recall (also known as Sensitivity or True Positive Rate)\n\nDefinition:\n\nRecall measures the model’s ability to find and identify all the actual relevant (positive) cases within the dataset. It answers the question: \"Of all the actual positive instances in the data, how many did the model successfully detect?\"\n\nFormula:\n\nRecall = TP / (TP + FN)\n\nExplanation:\n\n* TP (True Positives): The number of positive instances correctly identified as positive.\n\n* FN (False Negatives): The number of positive instances that the model incorrectly identified as negative (i.e., missed positive cases).\n\n* The denominator (TP + FN) represents the total number of actual positive instances in the dataset.\n\n* Recall focuses on the completeness of the positive predictions – how well the model avoids missing positives.\n\n* High recall indicates that the model makes few false negative errors. For example, in a medical diagnosis task for a serious disease, high recall is crucial to ensure that most patients who actually have the disease are correctly identified.",
    "enhanced_text": "[NLP] 1. Accuracy\n\nDefinition:\n\nAccuracy is a performance metric that measures the overall correctness of a classification model. It represents the proportion of total instances (or predictions) that the model classified correctly, encompassing both true positives and true negatives.\n\nFormula:\n\nAccuracy = (TP + TN) / (TP + FP + FN + TN)\n\nExplanation:\n\n* TP (True Positives): The number of positive instances that were correctly classified as positive.\n\n* TN (True Negatives): The number of negative instances that were correctly classified as negative.\n\n* FP (False Positives): The number of negative instances that were incorrectly classified as positive (also known as a Type I error).\n\n* FN (False Negatives): The number of positive instances that were incorrectly classified as negative (also known as a Type II error).\n\n* The numerator (TP + TN) represents the total number of correct predictions made by the model.\n\n* The denominator (TP + FP + FN + TN) represents the total number of instances or predictions made.\n\n* Accuracy provides a general sense of how often the classifier is right overall. However, it can be misleading for imbalanced datasets (where one class significantly outnumbers others).\n\n________________________________________\n\n2. Precision\n\nDefinition:\n\nPrecision is a performance metric that measures the model’s ability to correctly identify only the relevant (positive) data points from all the instances it predicted as positive. It answers the question: \"Of all the instances that the model labeled as positive, how many were actually positive?\"\n\nFormula:\n\nPrecision = TP / (TP + FP)\n\nExplanation:\n\n* TP (True Positives): The number of positive instances correctly identified as positive.\n\n* FP (False Positives): The number of negative instances incorrectly identified as positive.\n\n* The denominator (TP + FP) represents the total number of instances the model predicted as positive.\n\n* Precision focuses on the reliability of the positive predictions.\n\n* High precision indicates that the model makes few false positive errors. For example, if a spam filter has high precision, it means that when it flags an email as spam, it is very likely to actually be spam.\n\n________________________________________\n\n3. Recall (also known as Sensitivity or True Positive Rate)\n\nDefinition:\n\nRecall measures the model’s ability to find and identify all the actual relevant (positive) cases within the dataset. It answers the question: \"Of all the actual positive instances in the data, how many did the model successfully detect?\"\n\nFormula:\n\nRecall = TP / (TP + FN)\n\nExplanation:\n\n* TP (True Positives): The number of positive instances correctly identified as positive.\n\n* FN (False Negatives): The number of positive instances that the model incorrectly identified as negative (i.e., missed positive cases).\n\n* The denominator (TP + FN) represents the total number of actual positive instances in the dataset.\n\n* Recall focuses on the completeness of the positive predictions – how well the model avoids missing positives.\n\n* High recall indicates that the model makes few false negative errors. For example, in a medical diagnosis task for a serious disease, high recall is crucial to ensure that most patients who actually have the disease are correctly identified.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(h) accuaracy , precision , recall.txt",
    "file_name": "lec3-(h) accuaracy , precision , recall.txt",
    "filename_keywords": [
      "accuaracy",
      "lec3",
      "precision",
      "recall"
    ],
    "content_keywords": [
      "accuracy\n\ndefinition",
      "recall",
      "sensitivity",
      "for",
      "true negatives",
      "precision",
      "true positive rate",
      "false positives",
      "true positives",
      "precision\n\ndefinition",
      "type ii",
      "accuracy",
      "explanation",
      "high",
      "type i",
      "recall (also known as sensitivity or true positive",
      "the",
      "however",
      "definition",
      "false negatives",
      "formula"
    ],
    "technical_terms": [
      "accuracy\n\ndefinition",
      "recall",
      "sensitivity",
      "for",
      "true negatives",
      "precision",
      "true positive rate",
      "false positives",
      "true positives",
      "precision\n\ndefinition",
      "type ii",
      "accuracy",
      "explanation",
      "high",
      "type i",
      "the",
      "however",
      "definition",
      "false negatives",
      "formula"
    ],
    "all_keywords": [
      "accuracy\n\ndefinition",
      "recall",
      "sensitivity",
      "for",
      "precision",
      "true negatives",
      "true positive rate",
      "accuaracy",
      "false positives",
      "true positives",
      "precision\n\ndefinition",
      "type ii",
      "accuracy",
      "explanation",
      "high",
      "type i",
      "recall (also known as sensitivity or true positive",
      "the",
      "however",
      "definition",
      "lec3",
      "false negatives",
      "formula"
    ],
    "keyword_string": "accuracy\n\ndefinition recall sensitivity for precision true negatives true positive rate accuaracy false positives true positives precision\n\ndefinition type ii accuracy explanation high type i recall (also known as sensitivity or true positive the however definition lec3 false negatives formula",
    "token_count": 762,
    "word_count": 509,
    "sentence_count": 29,
    "paragraph_count": 37,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6679790026246719,
    "avg_sentence_length": 17.551724137931036,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": false,
    "content_type": "Technical, Structured"
  },
  {
    "document_id": 115,
    "document_hash": "89896ef3448d",
    "content": "Precision vs. Recall\n\nPrecision is the percentage of selected items that are actually correct.\nExample: Out of all the emails your spam filter marked as spam, what fraction truly were spam?\n\nRecall is the percentage of correct items that are successfully selected.\nExample: Out of all the actual spam emails in your inbox, what fraction did the filter catch?\n\nThese two metrics have an inverse relationship:\n\nWhen you try to increase recall (catch more relevant items), precision often drops because you include more irrelevant items too.\n\nWhen you try to increase precision (be more selective), recall may drop because you miss some relevant items.\n\nTrade-off Between Precision and Recall (Precision-Recall Curve)\n\nImagine a graph where the x-axis is recall (from 0 to 1) and the y-axis is precision (also 0 to 1):\n\nIn the top-left corner (high precision, low recall), the system returns only highly relevant results but misses many useful ones. It’s very careful but may overlook many positives.\n\nIn the bottom-right corner (low precision, high recall), the system finds most relevant items but includes a lot of irrelevant ones too — so it returns “junk” along with the good stuff.\n\nThe ideal point is the top-right corner (precision = 1, recall = 1), meaning the system finds all relevant items and nothing irrelevant, but this is very hard to achieve.\n\nThis curve shows the typical trade-off between these two metrics.",
    "enhanced_text": "[NLP] Precision vs. Recall\n\nPrecision is the percentage of selected items that are actually correct.\nExample: Out of all the emails your spam filter marked as spam, what fraction truly were spam?\n\nRecall is the percentage of correct items that are successfully selected.\nExample: Out of all the actual spam emails in your inbox, what fraction did the filter catch?\n\nThese two metrics have an inverse relationship:\n\nWhen you try to increase recall (catch more relevant items), precision often drops because you include more irrelevant items too.\n\nWhen you try to increase precision (be more selective), recall may drop because you miss some relevant items.\n\nTrade-off Between Precision and Recall (Precision-Recall Curve)\n\nImagine a graph where the x-axis is recall (from 0 to 1) and the y-axis is precision (also 0 to 1):\n\nIn the top-left corner (high precision, low recall), the system returns only highly relevant results but misses many useful ones. It’s very careful but may overlook many positives.\n\nIn the bottom-right corner (low precision, high recall), the system finds most relevant items but includes a lot of irrelevant ones too — so it returns “junk” along with the good stuff.\n\nThe ideal point is the top-right corner (precision = 1, recall = 1), meaning the system finds all relevant items and nothing irrelevant, but this is very hard to achieve.\n\nThis curve shows the typical trade-off between these two metrics.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(i) Precision vs recall.txt",
    "file_name": "lec3-(i) Precision vs recall.txt",
    "filename_keywords": [
      "precision",
      "lec3",
      "recall"
    ],
    "content_keywords": [
      "these",
      "when",
      "precision",
      "trade",
      "recall curve",
      "recall",
      "imagine",
      "the",
      "recall\n\nprecision",
      "example",
      "this",
      "between precision",
      "out"
    ],
    "technical_terms": [
      "these",
      "when",
      "precision",
      "trade",
      "recall curve",
      "recall",
      "imagine",
      "the",
      "recall\n\nprecision",
      "example",
      "this",
      "between precision",
      "out"
    ],
    "all_keywords": [
      "these",
      "when",
      "precision",
      "trade",
      "recall curve",
      "recall",
      "imagine",
      "the",
      "recall\n\nprecision",
      "example",
      "this",
      "lec3",
      "between precision",
      "out"
    ],
    "keyword_string": "these when precision trade recall curve recall imagine the recall\n\nprecision example this lec3 between precision out",
    "token_count": 303,
    "word_count": 232,
    "sentence_count": 11,
    "paragraph_count": 12,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7656765676567657,
    "avg_sentence_length": 21.09090909090909,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 116,
    "document_hash": "8cda11f81d18",
    "content": "Why is Recall Hard to Determine?\n\nRecall calculation requires knowing the total number of relevant items in the entire dataset, which may not be available in real life. Two common ways to estimate this are:\n\nTaking a random sample from the dataset and having experts label what is relevant.\n\nUsing multiple retrieval systems to find relevant items and combining their results as an estimate of all relevant items.",
    "enhanced_text": "[NLP] Why is Recall Hard to Determine?\n\nRecall calculation requires knowing the total number of relevant items in the entire dataset, which may not be available in real life. Two common ways to estimate this are:\n\nTaking a random sample from the dataset and having experts label what is relevant.\n\nUsing multiple retrieval systems to find relevant items and combining their results as an estimate of all relevant items.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(j) reacll hard to determined.txt",
    "file_name": "lec3-(j) reacll hard to determined.txt",
    "filename_keywords": [
      "determined",
      "hard",
      "lec3",
      "reacll"
    ],
    "content_keywords": [
      "using",
      "recall",
      "taking",
      "why",
      "determine",
      "recall hard",
      "two"
    ],
    "technical_terms": [
      "using",
      "recall",
      "taking",
      "why",
      "determine",
      "recall hard",
      "two"
    ],
    "all_keywords": [
      "using",
      "reacll",
      "recall",
      "determined",
      "taking",
      "why",
      "determine",
      "lec3",
      "recall hard",
      "hard",
      "two"
    ],
    "keyword_string": "using reacll recall determined taking why determine lec3 recall hard hard two",
    "token_count": 76,
    "word_count": 68,
    "sentence_count": 4,
    "paragraph_count": 4,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.8947368421052632,
    "avg_sentence_length": 17.0,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 117,
    "document_hash": "e0d9342d5594",
    "content": "F-Measure (F1 Score)\n\nDefinition:\n\nThe F-Measure, commonly known as the F1 Score, is a metric that combines precision and recall into a single value. It is the harmonic mean of precision and recall. The F1 score is particularly useful when you need a balance between precision and recall, especially if there's an uneven class distribution.\n\nFormula:\n\nF1 Score = (2 * Precision * Recall) / (Precision + Recall)\n\nAlternatively, it can be expressed as:\n\nF1 Score = 2 / ( (1 / Precision) + (1 / Recall) )\n\nExplanation:\n\n* Precision: Measures the accuracy of positive predictions (TP / (TP + FP)).\n\n* Recall: Measures the ability to find all actual positives (TP / (TP + FN)).\n\n* The F1 score seeks to find an optimal blend of precision and recall.\n\n* It ranges from 0 to 1, where 1 indicates perfect precision and recall, and 0 indicates that either precision or recall (or both) is zero.\n\n* The use of the harmonic mean is significant:\n\n * It penalizes extreme values more than an arithmetic mean would. This means if either precision or recall is very low, the F1 score will also be low. To achieve a high F1 score, both precision and recall must be reasonably high.\n\n * Unlike the arithmetic mean, which could be high even if one metric is very low and the other very high, the harmonic mean gives more weight to lower values. This reflects the common desire in many classification tasks to optimize both the ability to avoid false positives (precision) and the ability to avoid false negatives (recall) simultaneously.\n\n * For instance, if Precision = 1.0 (perfect) but Recall = 0.1 (very poor), the F1 score would be 2 * (1.0 * 0.1) / (1.0 + 0.1) = 0.2 / 1.1 ≈ 0.18, which is low, reflecting the poor recall. An arithmetic mean would be (1.0 + 0.1) / 2 = 0.55, which might give a misleadingly optimistic view of performance.\n\nWhen to use F1 Score:\n\n* When you care equally about precision and recall.\n\n* When dealing with imbalanced datasets, where accuracy can be misleading. For example, if 99% of instances are negative, a model predicting everything as negative would have 99% accuracy but would be useless for identifying positive cases. The F1 score would better reflect its poor performance on the positive class.",
    "enhanced_text": "[NLP] F-Measure (F1 Score)\n\nDefinition:\n\nThe F-Measure, commonly known as the F1 Score, is a metric that combines precision and recall into a single value. It is the harmonic mean of precision and recall. The F1 score is particularly useful when you need a balance between precision and recall, especially if there's an uneven class distribution.\n\nFormula:\n\nF1 Score = (2 * Precision * Recall) / (Precision + Recall)\n\nAlternatively, it can be expressed as:\n\nF1 Score = 2 / ( (1 / Precision) + (1 / Recall) )\n\nExplanation:\n\n* Precision: Measures the accuracy of positive predictions (TP / (TP + FP)).\n\n* Recall: Measures the ability to find all actual positives (TP / (TP + FN)).\n\n* The F1 score seeks to find an optimal blend of precision and recall.\n\n* It ranges from 0 to 1, where 1 indicates perfect precision and recall, and 0 indicates that either precision or recall (or both) is zero.\n\n* The use of the harmonic mean is significant:\n\n * It penalizes extreme values more than an arithmetic mean would. This means if either precision or recall is very low, the F1 score will also be low. To achieve a high F1 score, both precision and recall must be reasonably high.\n\n * Unlike the arithmetic mean, which could be high even if one metric is very low and the other very high, the harmonic mean gives more weight to lower values. This reflects the common desire in many classification tasks to optimize both the ability to avoid false positives (precision) and the ability to avoid false negatives (recall) simultaneously.\n\n * For instance, if Precision = 1.0 (perfect) but Recall = 0.1 (very poor), the F1 score would be 2 * (1.0 * 0.1) / (1.0 + 0.1) = 0.2 / 1.1 ≈ 0.18, which is low, reflecting the poor recall. An arithmetic mean would be (1.0 + 0.1) / 2 = 0.55, which might give a misleadingly optimistic view of performance.\n\nWhen to use F1 Score:\n\n* When you care equally about precision and recall.\n\n* When dealing with imbalanced datasets, where accuracy can be misleading. For example, if 99% of instances are negative, a model predicting everything as negative would have 99% accuracy but would be useless for identifying positive cases. The F1 score would better reflect its poor performance on the positive class.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(k) F-measure.txt",
    "file_name": "lec3-(k) F-measure.txt",
    "filename_keywords": [
      "lec3",
      "measure"
    ],
    "content_keywords": [
      "when",
      "precision",
      "the f",
      "recall",
      "measure",
      "score",
      "definition",
      "the",
      "explanation",
      "unlike",
      "measures",
      "this",
      "alternatively",
      "formula",
      "for"
    ],
    "technical_terms": [
      "when",
      "precision",
      "the f",
      "recall",
      "measure",
      "score",
      "definition",
      "the",
      "explanation",
      "unlike",
      "measures",
      "this",
      "alternatively",
      "formula",
      "for"
    ],
    "all_keywords": [
      "when",
      "precision",
      "the f",
      "recall",
      "measure",
      "score",
      "definition",
      "the",
      "explanation",
      "unlike",
      "measures",
      "this",
      "lec3",
      "alternatively",
      "formula",
      "for"
    ],
    "keyword_string": "when precision the f recall measure score definition the explanation unlike measures this lec3 alternatively formula for",
    "token_count": 519,
    "word_count": 392,
    "sentence_count": 18,
    "paragraph_count": 19,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7552986512524085,
    "avg_sentence_length": 21.77777777777778,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 118,
    "document_hash": "908b654b4845",
    "content": "Precision-Critical vs. Recall-Critical Tasks\n\nPrecision-critical tasks:\n\nTime is limited; you want only a few, very accurate results.\n\nIt's okay to miss some relevant items as long as the returned ones are very reliable.\n\nExample: Web search for factual info where users want quick, correct answers.\n\nRecall-critical tasks:\n\nTime is less important; missing relevant items is costly.\n\nIt’s better to find all relevant items, even if you get some irrelevant ones.\n\nExample: Patent search, where missing a relevant patent could have serious consequences.",
    "enhanced_text": "[NLP] Precision-Critical vs. Recall-Critical Tasks\n\nPrecision-critical tasks:\n\nTime is limited; you want only a few, very accurate results.\n\nIt's okay to miss some relevant items as long as the returned ones are very reliable.\n\nExample: Web search for factual info where users want quick, correct answers.\n\nRecall-critical tasks:\n\nTime is less important; missing relevant items is costly.\n\nIt’s better to find all relevant items, even if you get some irrelevant ones.\n\nExample: Patent search, where missing a relevant patent could have serious consequences.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(l) Precision-Critical vs. Recall-Critical Tasks.txt",
    "file_name": "lec3-(l) Precision-Critical vs. Recall-Critical Tasks.txt",
    "filename_keywords": [
      "precision",
      "recall",
      "lec3",
      "critical",
      "tasks"
    ],
    "content_keywords": [
      "precision",
      "recall",
      "web",
      "patent",
      "example",
      "critical",
      "critical tasks\n\nprecision",
      "time"
    ],
    "technical_terms": [
      "precision",
      "recall",
      "web",
      "patent",
      "example",
      "critical",
      "critical tasks\n\nprecision",
      "time"
    ],
    "all_keywords": [
      "precision",
      "recall",
      "web",
      "patent",
      "example",
      "lec3",
      "critical",
      "tasks",
      "critical tasks\n\nprecision",
      "time"
    ],
    "keyword_string": "precision recall web patent example lec3 critical tasks critical tasks\n\nprecision time",
    "token_count": 111,
    "word_count": 82,
    "sentence_count": 6,
    "paragraph_count": 9,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7387387387387387,
    "avg_sentence_length": 13.666666666666666,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 119,
    "document_hash": "a8fd8151e58c",
    "content": "Multi-class Confusion Matrix and Evaluation\n\nFor multiple classes (like \"urgent,\" \"normal,\" and \"spam\"), the confusion matrix expands beyond 2x2, and you compute precision and recall for each class separately:\n\nFor a given class (e.g., \"urgent\"):\n\nTrue Positives (TP) are items actually urgent and predicted urgent.\n\nFalse Positives (FP) are items predicted urgent but actually belong to other classes (\"normal\" or \"spam\").\n\nFalse Negatives (FN) are actual urgent items predicted as other classes.\n\nYou calculate:\n\nPrecision for \"urgent\" = TP_urgent / (TP_urgent + FP_urgent)\n\nRecall for \"urgent\" = TP_urgent / (TP_urgent + FN_urgent)\n\nSimilarly, you compute precision and recall for \"normal\" and \"spam\" classes.\n\nOverall accuracy is the sum of all correct predictions (all true positives across classes) divided by total items.\n\nMulti-class Exercise Example (no table format)\n\nImagine a dataset with these counts:\n\nActual urgent items predicted as urgent: 96\n\nActual urgent items predicted as normal: 3\n\nActual urgent items predicted as spam: 1\n\nActual normal items predicted as urgent: 4\n\nActual normal items predicted as normal: 89\n\nActual normal items predicted as spam: 7\n\nActual spam items predicted as urgent: 2\n\nActual spam items predicted as normal: 3\n\nActual spam items predicted as spam: 95\n\nYou can calculate precision and recall for each class using the formulas above:\n\nFor urgent precision: true positives are 96; false positives are predictions of urgent that were actually normal or spam (4 + 2).\n\nFor urgent recall: false negatives are actual urgent items predicted as normal or spam (3 + 1).\n\nAnd similarly for the other classes.",
    "enhanced_text": "[NLP] Multi-class Confusion Matrix and Evaluation\n\nFor multiple classes (like \"urgent,\" \"normal,\" and \"spam\"), the confusion matrix expands beyond 2x2, and you compute precision and recall for each class separately:\n\nFor a given class (e.g., \"urgent\"):\n\nTrue Positives (TP) are items actually urgent and predicted urgent.\n\nFalse Positives (FP) are items predicted urgent but actually belong to other classes (\"normal\" or \"spam\").\n\nFalse Negatives (FN) are actual urgent items predicted as other classes.\n\nYou calculate:\n\nPrecision for \"urgent\" = TP_urgent / (TP_urgent + FP_urgent)\n\nRecall for \"urgent\" = TP_urgent / (TP_urgent + FN_urgent)\n\nSimilarly, you compute precision and recall for \"normal\" and \"spam\" classes.\n\nOverall accuracy is the sum of all correct predictions (all true positives across classes) divided by total items.\n\nMulti-class Exercise Example (no table format)\n\nImagine a dataset with these counts:\n\nActual urgent items predicted as urgent: 96\n\nActual urgent items predicted as normal: 3\n\nActual urgent items predicted as spam: 1\n\nActual normal items predicted as urgent: 4\n\nActual normal items predicted as normal: 89\n\nActual normal items predicted as spam: 7\n\nActual spam items predicted as urgent: 2\n\nActual spam items predicted as normal: 3\n\nActual spam items predicted as spam: 95\n\nYou can calculate precision and recall for each class using the formulas above:\n\nFor urgent precision: true positives are 96; false positives are predictions of urgent that were actually normal or spam (4 + 2).\n\nFor urgent recall: false negatives are actual urgent items predicted as normal or spam (3 + 1).\n\nAnd similarly for the other classes.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3-(m) Multi-class Confusion Matrix.txt",
    "file_name": "lec3-(m) Multi-class Confusion Matrix.txt",
    "filename_keywords": [
      "matrix",
      "multi",
      "class",
      "confusion",
      "lec3"
    ],
    "content_keywords": [
      "evaluation\n\nfor",
      "recall",
      "confusion matrix",
      "you",
      "for",
      "precision",
      "urgent,",
      "normal,",
      "false positives",
      "true positives",
      "and",
      "actual",
      "imagine",
      "spam",
      "exercise example",
      "normal",
      "multi",
      "false negatives",
      "urgent",
      "overall",
      "similarly"
    ],
    "technical_terms": [
      "precision",
      "evaluation\n\nfor",
      "recall",
      "imagine",
      "confusion matrix",
      "actual",
      "multi",
      "false positives",
      "and",
      "true positives",
      "false negatives",
      "overall",
      "you",
      "exercise example",
      "similarly",
      "for"
    ],
    "all_keywords": [
      "evaluation\n\nfor",
      "recall",
      "matrix",
      "confusion matrix",
      "confusion",
      "you",
      "for",
      "precision",
      "urgent,",
      "normal,",
      "class",
      "false positives",
      "true positives",
      "and",
      "actual",
      "imagine",
      "spam",
      "exercise example",
      "normal",
      "multi",
      "lec3",
      "false negatives",
      "urgent",
      "overall",
      "similarly"
    ],
    "keyword_string": "evaluation\n\nfor recall matrix confusion matrix confusion you for precision urgent, normal, class false positives true positives and actual imagine spam exercise example normal multi lec3 false negatives urgent overall similarly",
    "token_count": 378,
    "word_count": 254,
    "sentence_count": 8,
    "paragraph_count": 26,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.671957671957672,
    "avg_sentence_length": 31.75,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 120,
    "document_hash": "3c8199e211e1",
    "content": "Confusion Matrix Example: Email Spam Detection\n\nConfusion Matrix Example: Email Spam Detection\n\nScenario:\n\nA spam detection model has been developed and was tested on a dataset of 1000 emails. The goal is to classify emails as either \"Spam\" (positive class) or \"Not Spam\" (negative class, i.e., legitimate email).\n\nTest Results Overview:\n\n* Total emails tested: 1000\n\n* Actual spam emails in the dataset: 100\n\n* Actual legitimate (not spam) emails in the dataset: 900\n\nConfusion Matrix:\n\nThe results of the model's predictions are summarized in the following confusion matrix:\n\n Predicted: Spam Predicted: Not Spam | Actual Totals\n\n +-----------------+---------------------+\n\nActual: Spam | 85 (TP) | 15 (FN) | 100\n\n +-----------------+---------------------+\n\nActual: Not | 25 (FP) | 875 (TN) | 900\n\nSpam +-----------------+---------------------+\n\n | Predicted Totals| |\n\n | 110 | 890 | 1000 (Grand Total)\n\nBreaking down the numbers:\n\nTrue Positives (TP): 85 spam emails correctly identified as spam\n\nTrue Negatives (TN): 875 legitimate emails correctly identified as legitimate\n\nFalse Positives (FP): 25 legitimate emails incorrectly flagged as spam\n\nFalse Negatives (FN): 15 spam emails that slipped through as legitimate\n\nCalculated Performance Metrics:\n\n1. Accuracy:\n\n * Formula: (TP + TN) / (TP + FP + FN + TN)\n\n * Calculation: (85 + 875) / (85 + 25 + 15 + 875) = 960 / 1000 = 0.96\n\n * Interpretation: 96% or 0.96. The model correctly classified 960 out of the 1000 emails overall.\n\n2. Precision (for the \"Spam\" class):\n\n * Formula: TP / (TP + FP)\n\n * Calculation: 85 / (85 + 25) = 85 / 110 ≈ 0.773\n\n * Interpretation: Approximately 77.3%. Of all the emails that the model predicted as spam, 77.3% were actually spam. This means that about 22.7% (100% - 77.3%) of emails flagged as spam were actually legitimate (false alarms).\n\n3. Recall (Sensitivity, True Positive Rate for the \"Spam\" class):\n\n * Formula: TP / (TP + FN)\n\n * Calculation: 85 / (85 + 15) = 85 / 100 = 0.85\n\n * Interpretation: 85%. The model successfully caught 85% of all the actual spam emails present in the dataset. However, it missed 15% of the spam emails.\n\n4. F1-Score (for the \"Spam\" class):\n\n * Formula: 2 * (Precision * Recall) / (Precision + Recall)\n\n * Calculation (using rounded precision 0.77 for simplicity as in original): 2 * (0.77 * 0.85) / (0.77 + 0.85) = 2 * 0.6545 / 1.62 = 1.309 / 1.62 ≈ 0.808\n\n * (Using more precise precision 0.773: 2 * (0.773 * 0.85) / (0.773 + 0.85) = 2 * 0.65705 / 1.623 = 1.3141 / 1.623 ≈ 0.809)\n\n * Interpretation: Approximately 81%. This balanced metric, which considers both precision and recall, indicates that the model has a reasonably good overall performance in identifying spam, though there's room for improvement, especially in precision.\n\nCalculated Metrics:\n\nAccuracy: (85 + 875) / 1000 = 0.96 or 96% The model correctly classified 960 out of 1000 emails.\n\nPrecision: 85 / (85 + 25) = 0.77 or 77% Of all emails flagged as spam, 77% were actually spam. This means 23% of flagged emails were false alarms.\n\nRecall: 85 / (85 + 15) = 0.85 or 85% The model caught 85% of all actual spam emails, but missed 15% of spam.\n\nF1-Score: 2 × (0.77 × 0.85) / (0.77 + 0.85) = 0.81 or 81% This balanced metric shows the model performs reasonably well overall.\n\nThe model shows good overall performance with 96% accuracy, but the 77% precision means roughly 1 in 4 spam-flagged emails are actually legitimate.",
    "enhanced_text": "[NLP] Confusion Matrix Example: Email Spam Detection\n\nConfusion Matrix Example: Email Spam Detection\n\nScenario:\n\nA spam detection model has been developed and was tested on a dataset of 1000 emails. The goal is to classify emails as either \"Spam\" (positive class) or \"Not Spam\" (negative class, i.e., legitimate email).\n\nTest Results Overview:\n\n* Total emails tested: 1000\n\n* Actual spam emails in the dataset: 100\n\n* Actual legitimate (not spam) emails in the dataset: 900\n\nConfusion Matrix:\n\nThe results of the model's predictions are summarized in the following confusion matrix:\n\n Predicted: Spam Predicted: Not Spam | Actual Totals\n\n +-----------------+---------------------+\n\nActual: Spam | 85 (TP) | 15 (FN) | 100\n\n +-----------------+---------------------+\n\nActual: Not | 25 (FP) | 875 (TN) | 900\n\nSpam +-----------------+---------------------+\n\n | Predicted Totals| |\n\n | 110 | 890 | 1000 (Grand Total)\n\nBreaking down the numbers:\n\nTrue Positives (TP): 85 spam emails correctly identified as spam\n\nTrue Negatives (TN): 875 legitimate emails correctly identified as legitimate\n\nFalse Positives (FP): 25 legitimate emails incorrectly flagged as spam\n\nFalse Negatives (FN): 15 spam emails that slipped through as legitimate\n\nCalculated Performance Metrics:\n\n1. Accuracy:\n\n * Formula: (TP + TN) / (TP + FP + FN + TN)\n\n * Calculation: (85 + 875) / (85 + 25 + 15 + 875) = 960 / 1000 = 0.96\n\n * Interpretation: 96% or 0.96. The model correctly classified 960 out of the 1000 emails overall.\n\n2. Precision (for the \"Spam\" class):\n\n * Formula: TP / (TP + FP)\n\n * Calculation: 85 / (85 + 25) = 85 / 110 ≈ 0.773\n\n * Interpretation: Approximately 77.3%. Of all the emails that the model predicted as spam, 77.3% were actually spam. This means that about 22.7% (100% - 77.3%) of emails flagged as spam were actually legitimate (false alarms).\n\n3. Recall (Sensitivity, True Positive Rate for the \"Spam\" class):\n\n * Formula: TP / (TP + FN)\n\n * Calculation: 85 / (85 + 15) = 85 / 100 = 0.85\n\n * Interpretation: 85%. The model successfully caught 85% of all the actual spam emails present in the dataset. However, it missed 15% of the spam emails.\n\n4. F1-Score (for the \"Spam\" class):\n\n * Formula: 2 * (Precision * Recall) / (Precision + Recall)\n\n * Calculation (using rounded precision 0.77 for simplicity as in original): 2 * (0.77 * 0.85) / (0.77 + 0.85) = 2 * 0.6545 / 1.62 = 1.309 / 1.62 ≈ 0.808\n\n * (Using more precise precision 0.773: 2 * (0.773 * 0.85) / (0.773 + 0.85) = 2 * 0.65705 / 1.623 = 1.3141 / 1.623 ≈ 0.809)\n\n * Interpretation: Approximately 81%. This balanced metric, which considers both precision and recall, indicates that the model has a reasonably good overall performance in identifying spam, though there's room for improvement, especially in precision.\n\nCalculated Metrics:\n\nAccuracy: (85 + 875) / 1000 = 0.96 or 96% The model correctly classified 960 out of 1000 emails.\n\nPrecision: 85 / (85 + 25) = 0.77 or 77% Of all emails flagged as spam, 77% were actually spam. This means 23% of flagged emails were false alarms.\n\nRecall: 85 / (85 + 15) = 0.85 or 85% The model caught 85% of all actual spam emails, but missed 15% of spam.\n\nF1-Score: 2 × (0.77 × 0.85) / (0.77 + 0.85) = 0.81 or 81% This balanced metric shows the model performs reasonably well overall.\n\nThe model shows good overall performance with 96% accuracy, but the 77% precision means roughly 1 in 4 spam-flagged emails are actually legitimate.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec3_(n)_Example_of_Confusion_matrix_percision_recall_accuracy_F1_Score.txt",
    "file_name": "lec3_(n)_Example_of_Confusion_matrix_percision_recall_accuracy_F1_Score.txt",
    "filename_keywords": [
      "accuracy",
      "recall",
      "score",
      "matrix",
      "confusion",
      "example",
      "lec3",
      "percision"
    ],
    "content_keywords": [
      "recall",
      "score",
      "email spam detection\n\nscenario",
      "confusion matrix",
      "sensitivity",
      "recall (sensitivity, true positive rate for the \"sp",
      "predicted totals",
      "email spam detection\n\nconfusion matrix example",
      "precision (for the \"spam\" class):",
      "calculated performance metrics",
      "true negatives",
      "actual totals",
      "precision",
      "true positive rate",
      "grand total",
      "false positives",
      "spam predicted",
      "true positives",
      "approximately",
      "using",
      "interpretation",
      "accuracy",
      "actual",
      "f1-score (for the \"spam\" class):",
      "spam",
      "predicted",
      "not spam",
      "this",
      "breaking",
      "the",
      "not",
      "calculation",
      "however",
      "calculated metrics",
      "test results overview",
      "total",
      "confusion matrix example",
      "false negatives",
      "formula"
    ],
    "technical_terms": [
      "recall",
      "score",
      "email spam detection\n\nscenario",
      "confusion matrix",
      "sensitivity",
      "predicted totals",
      "email spam detection\n\nconfusion matrix example",
      "calculated performance metrics",
      "true negatives",
      "actual totals",
      "precision",
      "true positive rate",
      "grand total",
      "false positives",
      "spam predicted",
      "true positives",
      "approximately",
      "using",
      "interpretation",
      "accuracy",
      "actual",
      "spam",
      "predicted",
      "not spam",
      "this",
      "breaking",
      "the",
      "not",
      "calculation",
      "however",
      "calculated metrics",
      "test results overview",
      "total",
      "confusion matrix example",
      "false negatives",
      "formula"
    ],
    "all_keywords": [
      "recall",
      "score",
      "matrix",
      "email spam detection\n\nscenario",
      "confusion matrix",
      "sensitivity",
      "confusion",
      "recall (sensitivity, true positive rate for the \"sp",
      "predicted totals",
      "email spam detection\n\nconfusion matrix example",
      "precision (for the \"spam\" class):",
      "calculated performance metrics",
      "true negatives",
      "actual totals",
      "precision",
      "true positive rate",
      "grand total",
      "false positives",
      "example",
      "spam predicted",
      "true positives",
      "approximately",
      "using",
      "interpretation",
      "accuracy",
      "actual",
      "f1-score (for the \"spam\" class):",
      "spam",
      "predicted",
      "not spam",
      "this",
      "breaking",
      "the",
      "not",
      "calculation",
      "however",
      "calculated metrics",
      "lec3",
      "test results overview",
      "total",
      "confusion matrix example",
      "false negatives",
      "formula",
      "percision"
    ],
    "keyword_string": "recall score matrix email spam detection\n\nscenario confusion matrix sensitivity confusion recall (sensitivity, true positive rate for the \"sp predicted totals email spam detection\n\nconfusion matrix example precision (for the \"spam\" class): calculated performance metrics true negatives actual totals precision true positive rate grand total false positives example spam predicted true positives approximately using interpretation accuracy actual f1-score (for the \"spam\" class): spam predicted not spam this breaking the not calculation however calculated metrics lec3 test results overview total confusion matrix example false negatives formula percision",
    "token_count": 1040,
    "word_count": 580,
    "sentence_count": 22,
    "paragraph_count": 47,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5576923076923077,
    "avg_sentence_length": 26.363636363636363,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": true,
    "content_type": "Technical, Structured, Definitions"
  },
  {
    "document_id": 121,
    "document_hash": "2ef8cc4e0e4b",
    "content": "Probability in Natural Language Processing (NLP)\n\nCore Idea:\n\nNatural language is inherently filled with uncertainty and ambiguity.\n\n* Words can have multiple meanings (polysemy) depending on the surrounding context (e.g., \"bank\" can be a financial institution or the side of a river).\n\n* Sentences can be structured in many grammatically correct ways while conveying similar or different meanings.\n\n* Textual data can be noisy, containing misspellings, ungrammatical constructions, or incomplete information.\n\nProbability theory provides a mathematical framework to model and manage this uncertainty in language.\n\nProbabilistic Models in NLP:\n\nMany advanced NLP techniques and algorithms are built upon probabilistic foundations. These models use probability to:\n\n* Predict or generate language (e.g., predicting the next word in a sentence).\n\n* Estimate the likelihood of certain words, sequences of words (phrases, sentences), or linguistic structures occurring.\n\n* Make decisions or classifications based on patterns and likelihoods observed in textual data.\n\n________________________________________\n\nKey Examples in NLP Using Probability\n\n1. Language Modeling:\n\n * Concept: A language model assigns a probability to a sequence of words. It aims to capture how likely a given phrase or sentence is in a particular language.\n\n * Example: Estimate P(\"I love NLP\") – this represents the probability that the specific phrase \"I love NLP\" would appear naturally in English text. A well-trained language model would assign a higher probability to \"I love NLP\" than to a nonsensical or ungrammatical sequence like \"NLP love I\".\n\n * Applications:\n\n * Autocomplete / Predictive Text: Suggests the most probable next words as a user types.\n\n * Speech Recognition: Helps distinguish between acoustically similar phrases by choosing the one that is more probable in the language (e.g., \"recognize speech\" vs. \"wreck a nice beach\").\n\n * Machine Translation: Helps ensure the translated output is fluent and natural in the target language.\n\n * Spelling/Grammar Correction: Identifies and suggests corrections for improbable word sequences.\n\n2. Naive Bayes Classifier:\n\n * Concept: A classification algorithm based on Bayes' Theorem, with a \"naive\" assumption of conditional independence between features (words).\n\n * Application: Used to classify text into predefined categories.\n\n * Spam Detection: Calculates P(Spam | Document_Words) – the probability that an email is spam given the words it contains.\n\n * Sentiment Analysis: Calculates P(Positive_Sentiment | Review_Text) – the probability that a product review expresses positive sentiment given its text.\n\n * Mechanism: It learns the probability of words appearing in documents of different classes from training data (e.g., P(\"free\" | Spam) vs. P(\"free\" | Not_Spam)).\n\n3. Word Sense Disambiguation (WSD):\n\n * Concept: The task of identifying which specific meaning (sense) of a word is used in a particular context, especially when the word has multiple meanings.\n\n * Probabilistic Approach: Models estimate P(Sense_k | Context_Words) – the probability of a particular sense_k of a target word being the correct one, given the surrounding words (the context).\n\n4. Part-of-Speech (POS) Tagging:\n\n * Concept: Assigning grammatical categories (like noun, verb, adjective, adverb, etc.) to each word in a sentence.\n\n * Probabilistic Approach: Models like Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs) assign the most probable sequence of tags to a sentence. This involves:\n\n * P(Tag | Word): Probability of a tag given a word (e.g., \"book\" is more likely P(Noun | \"book\") than P(Verb | \"book\") in many contexts, but the reverse can also be true).\n\n * P(Current_Tag | Previous_Tag): Probability of a tag given the preceding tag (e.g., a determiner is often followed by a noun).\n\n5. Statistical Machine Translation (SMT) (though increasingly replaced by Neural Machine Translation, SMT laid important groundwork):\n\n * Concept: Translates text from one language (source) to another (target) by modeling probabilities.\n\n * Probabilistic Approach: Aims to find the target sentence (T) that maximizes P(T | S), where S is the source sentence. Using Bayes' theorem, this is often decomposed into:\n\n * P(S | T) (Translation Model): Probability that S is the translation of T (learned from parallel corpora).\n\n * P(T) (Language Model): Probability of T being a fluent sentence in the target language.\n\n________________________________________\n\nWhy Use Probability in NLP?\n\n* Handles Ambiguity and Variability: Probability provides a principled way to deal with the inherent fuzziness and multiple interpretations present in natural language.\n\n* Enables Learning from Data: Probabilistic models can learn patterns, distributions, and dependencies from large amounts of text data, even if the data is imperfect, incomplete, or noisy.\n\n* Ranks Possible Outputs: Allows NLP systems to evaluate multiple possible interpretations, predictions, or outputs and rank them by their likelihood, leading to more accurate and robust performance. For instance, a speech recognizer can consider several acoustically plausible word sequences and choose the one that the language model deems most probable.",
    "enhanced_text": "[NLP] Probability in Natural Language Processing (NLP)\n\nCore Idea:\n\nNatural language is inherently filled with uncertainty and ambiguity.\n\n* Words can have multiple meanings (polysemy) depending on the surrounding context (e.g., \"bank\" can be a financial institution or the side of a river).\n\n* Sentences can be structured in many grammatically correct ways while conveying similar or different meanings.\n\n* Textual data can be noisy, containing misspellings, ungrammatical constructions, or incomplete information.\n\nProbability theory provides a mathematical framework to model and manage this uncertainty in language.\n\nProbabilistic Models in NLP:\n\nMany advanced NLP techniques and algorithms are built upon probabilistic foundations. These models use probability to:\n\n* Predict or generate language (e.g., predicting the next word in a sentence).\n\n* Estimate the likelihood of certain words, sequences of words (phrases, sentences), or linguistic structures occurring.\n\n* Make decisions or classifications based on patterns and likelihoods observed in textual data.\n\n________________________________________\n\nKey Examples in NLP Using Probability\n\n1. Language Modeling:\n\n * Concept: A language model assigns a probability to a sequence of words. It aims to capture how likely a given phrase or sentence is in a particular language.\n\n * Example: Estimate P(\"I love NLP\") – this represents the probability that the specific phrase \"I love NLP\" would appear naturally in English text. A well-trained language model would assign a higher probability to \"I love NLP\" than to a nonsensical or ungrammatical sequence like \"NLP love I\".\n\n * Applications:\n\n * Autocomplete / Predictive Text: Suggests the most probable next words as a user types.\n\n * Speech Recognition: Helps distinguish between acoustically similar phrases by choosing the one that is more probable in the language (e.g., \"recognize speech\" vs. \"wreck a nice beach\").\n\n * Machine Translation: Helps ensure the translated output is fluent and natural in the target language.\n\n * Spelling/Grammar Correction: Identifies and suggests corrections for improbable word sequences.\n\n2. Naive Bayes Classifier:\n\n * Concept: A classification algorithm based on Bayes' Theorem, with a \"naive\" assumption of conditional independence between features (words).\n\n * Application: Used to classify text into predefined categories.\n\n * Spam Detection: Calculates P(Spam | Document_Words) – the probability that an email is spam given the words it contains.\n\n * Sentiment Analysis: Calculates P(Positive_Sentiment | Review_Text) – the probability that a product review expresses positive sentiment given its text.\n\n * Mechanism: It learns the probability of words appearing in documents of different classes from training data (e.g., P(\"free\" | Spam) vs. P(\"free\" | Not_Spam)).\n\n3. Word Sense Disambiguation (WSD):\n\n * Concept: The task of identifying which specific meaning (sense) of a word is used in a particular context, especially when the word has multiple meanings.\n\n * Probabilistic Approach: Models estimate P(Sense_k | Context_Words) – the probability of a particular sense_k of a target word being the correct one, given the surrounding words (the context).\n\n4. Part-of-Speech (POS) Tagging:\n\n * Concept: Assigning grammatical categories (like noun, verb, adjective, adverb, etc.) to each word in a sentence.\n\n * Probabilistic Approach: Models like Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs) assign the most probable sequence of tags to a sentence. This involves:\n\n * P(Tag | Word): Probability of a tag given a word (e.g., \"book\" is more likely P(Noun | \"book\") than P(Verb | \"book\") in many contexts, but the reverse can also be true).\n\n * P(Current_Tag | Previous_Tag): Probability of a tag given the preceding tag (e.g., a determiner is often followed by a noun).\n\n5. Statistical Machine Translation (SMT) (though increasingly replaced by Neural Machine Translation, SMT laid important groundwork):\n\n * Concept: Translates text from one language (source) to another (target) by modeling probabilities.\n\n * Probabilistic Approach: Aims to find the target sentence (T) that maximizes P(T | S), where S is the source sentence. Using Bayes' theorem, this is often decomposed into:\n\n * P(S | T) (Translation Model): Probability that S is the translation of T (learned from parallel corpora).\n\n * P(T) (Language Model): Probability of T being a fluent sentence in the target language.\n\n________________________________________\n\nWhy Use Probability in NLP?\n\n* Handles Ambiguity and Variability: Probability provides a principled way to deal with the inherent fuzziness and multiple interpretations present in natural language.\n\n* Enables Learning from Data: Probabilistic models can learn patterns, distributions, and dependencies from large amounts of text data, even if the data is imperfect, incomplete, or noisy.\n\n* Ranks Possible Outputs: Allows NLP systems to evaluate multiple possible interpretations, predictions, or outputs and rank them by their likelihood, leading to more accurate and robust performance. For instance, a speech recognizer can consider several acoustically plausible word sequences and choose the one that the language model deems most probable.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(a) Probability.txt",
    "file_name": "lec4-(a) Probability.txt",
    "filename_keywords": [
      "lec4",
      "probability"
    ],
    "content_keywords": [
      "wsd",
      "pos",
      "identifies",
      "using bayes",
      "for",
      "these",
      "estimate p",
      "smt",
      "hidden markov models",
      "spelling",
      "statistical machine translation (smt) (though incre",
      "translation model",
      "bank",
      "predictive text",
      "variability",
      "nlp using probability",
      "applications",
      "naive bayes classifier",
      "speech",
      "tag",
      "probability",
      "helps",
      "spam detection",
      "natural",
      "allows nlp",
      "textual",
      "i love nlp",
      "book",
      "theorem, with a",
      "example",
      "neural machine translation",
      "estimate",
      "wreck a nice beach",
      "suggests",
      "words",
      "nlp love i",
      "part-of-speech (pos) tagging:",
      "word sense disambiguation (wsd):",
      "data",
      "application",
      "machine translation",
      "probabilistic",
      "the",
      "language modeling",
      "grammar correction",
      "sentiment analysis",
      "language model",
      "language modeling:",
      "hmms",
      "enables learning",
      "many",
      "recognize speech",
      "why use probability",
      "handles ambiguity",
      "natural language processing",
      "concept",
      "assigning",
      "spam",
      "tagging",
      "this",
      "english",
      "predict",
      "sentences",
      "free",
      "conditional random fields",
      "part",
      "core idea",
      "bayes",
      "naive bayes classifier:",
      "mechanism",
      "verb",
      "key examples",
      "theorem",
      "statistical machine translation",
      "models",
      "calculates p",
      "probabilistic models",
      "used",
      "make",
      "word",
      "speech recognition",
      "nlp",
      "noun",
      "ranks possible outputs",
      "autocomplete",
      "translates",
      "aims",
      "crfs",
      "probabilistic approach",
      "word sense disambiguation"
    ],
    "technical_terms": [
      "wsd",
      "pos",
      "identifies",
      "using bayes",
      "for",
      "these",
      "estimate p",
      "smt",
      "hidden markov models",
      "spelling",
      "translation model",
      "predictive text",
      "variability",
      "nlp using probability",
      "applications",
      "naive bayes classifier",
      "speech",
      "tag",
      "probability",
      "helps",
      "spam detection",
      "natural",
      "allows nlp",
      "textual",
      "example",
      "neural machine translation",
      "estimate",
      "suggests",
      "words",
      "data",
      "application",
      "machine translation",
      "probabilistic",
      "the",
      "language modeling",
      "grammar correction",
      "sentiment analysis",
      "language model",
      "hmms",
      "enables learning",
      "many",
      "why use probability",
      "handles ambiguity",
      "natural language processing",
      "concept",
      "assigning",
      "spam",
      "tagging",
      "this",
      "english",
      "predict",
      "sentences",
      "conditional random fields",
      "part",
      "core idea",
      "bayes",
      "mechanism",
      "verb",
      "key examples",
      "theorem",
      "statistical machine translation",
      "models",
      "calculates p",
      "probabilistic models",
      "used",
      "make",
      "word",
      "speech recognition",
      "nlp",
      "noun",
      "ranks possible outputs",
      "autocomplete",
      "translates",
      "aims",
      "crfs",
      "probabilistic approach",
      "word sense disambiguation"
    ],
    "all_keywords": [
      "wsd",
      "pos",
      "identifies",
      "using bayes",
      "for",
      "these",
      "estimate p",
      "smt",
      "hidden markov models",
      "spelling",
      "statistical machine translation (smt) (though incre",
      "translation model",
      "bank",
      "predictive text",
      "variability",
      "nlp using probability",
      "applications",
      "naive bayes classifier",
      "speech",
      "tag",
      "probability",
      "helps",
      "spam detection",
      "natural",
      "allows nlp",
      "textual",
      "i love nlp",
      "book",
      "theorem, with a",
      "example",
      "neural machine translation",
      "estimate",
      "wreck a nice beach",
      "suggests",
      "words",
      "nlp love i",
      "part-of-speech (pos) tagging:",
      "word sense disambiguation (wsd):",
      "data",
      "application",
      "machine translation",
      "probabilistic",
      "the",
      "language modeling",
      "grammar correction",
      "sentiment analysis",
      "language model",
      "language modeling:",
      "hmms",
      "enables learning",
      "many",
      "recognize speech",
      "why use probability",
      "handles ambiguity",
      "natural language processing",
      "concept",
      "assigning",
      "spam",
      "tagging",
      "this",
      "english",
      "predict",
      "sentences",
      "free",
      "conditional random fields",
      "lec4",
      "part",
      "core idea",
      "bayes",
      "naive bayes classifier:",
      "mechanism",
      "verb",
      "key examples",
      "theorem",
      "statistical machine translation",
      "models",
      "calculates p",
      "probabilistic models",
      "used",
      "make",
      "word",
      "speech recognition",
      "nlp",
      "noun",
      "ranks possible outputs",
      "autocomplete",
      "translates",
      "aims",
      "crfs",
      "probabilistic approach",
      "word sense disambiguation"
    ],
    "keyword_string": "wsd pos identifies using bayes for these estimate p smt hidden markov models spelling statistical machine translation (smt) (though incre translation model bank predictive text variability nlp using probability applications naive bayes classifier speech tag probability helps spam detection natural allows nlp textual i love nlp book theorem, with a example neural machine translation estimate wreck a nice beach suggests words nlp love i part-of-speech (pos) tagging: word sense disambiguation (wsd): data application machine translation probabilistic the language modeling grammar correction sentiment analysis language model language modeling: hmms enables learning many recognize speech why use probability handles ambiguity natural language processing concept assigning spam tagging this english predict sentences free conditional random fields lec4 part core idea bayes naive bayes classifier: mechanism verb key examples theorem statistical machine translation models calculates p probabilistic models used make word speech recognition nlp noun ranks possible outputs autocomplete translates aims crfs probabilistic approach word sense disambiguation",
    "token_count": 1211,
    "word_count": 765,
    "sentence_count": 43,
    "paragraph_count": 46,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6317093311312965,
    "avg_sentence_length": 17.790697674418606,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": true,
    "content_type": "Technical, Structured, Definitions"
  },
  {
    "document_id": 122,
    "document_hash": "98f8f7c31e17",
    "content": "Conditional Probability in Natural Language Processing (NLP)\n\nConditional probability is a fundamental concept in probability theory that measures the likelihood of an event occurring, given that another event has already occurred. In NLP, this \"event\" can be the occurrence of a word, a phrase, a part-of-speech tag, a document class, or any other linguistic feature. Modeling the likelihood of one linguistic event given the presence or context of another is central to how many NLP tasks are approached.\n\n________________________________________\n\nNLP Examples Using Conditional Probability\n\n1. Language Modeling:\n\n * Goal: To predict the probability of the next word (w_n) in a sequence, given the sequence of words that have come before it (w_1, w_2, ..., w_{n-1}).\n\n * Notation: This is written as P(w_n | w_1, w_2, ..., w_{n-1}).\n\n This reads as \"the probability of word w_n given the preceding words w_1 through w_{n-1}.\"\n\n * Example:\n\n * P(\"morning\" | \"Good\") would likely be high, as \"Good morning\" is a common phrase.\n\n * P(\"banana\" | \"Good\") would likely be very low, as \"Good banana\" is not a typical sequence.\n\n * How it's used: Language models, such as n-gram models or more advanced neural network models (like RNNs or Transformers), estimate these conditional probabilities. N-gram models, for instance, simplify this by assuming the next word only depends on a fixed number of preceding words (e.g., P(w_n | w_{n-1}) for a bigram model).\n\n2. Naive Bayes Text Classification:\n\n * Goal: To assign a category or class label to a document (e.g., spam/not-spam, positive/negative sentiment). This involves calculating P(Class | Document) — the probability that a document belongs to a particular class, given the words (or other features) in that document.\n\n * Using Bayes’ Theorem:\n\n P(Class | Document) = [P(Document | Class) * P(Class)] / P(Document)\n\n The term P(Document | Class) is often broken down (assuming conditional independence of words, hence \"Naive\") into a product of conditional probabilities of individual words given the class.\n\n * Example conditional probabilities involved:\n\n * P(\"free\" | spam): The probability of the word \"free\" appearing in a document, given that the document is spam. This might be high.\n\n * P(\"meeting\" | not-spam): The probability of the word \"meeting\" appearing, given the document is not spam.\n\n * How it's used: The classifier assigns the class that has the highest posterior probability P(Class | Document).\n\n3. Part-of-Speech (POS) Tagging:\n\n * Goal: To assign the correct grammatical tag (e.g., noun, verb, adjective) to each word in a sentence.\n\n * Conditional probabilities used:\n\n * Transition Probabilities: P(Tag_i | Tag_{i-1}), the probability of the current tag (Tag_i) given the previous tag (Tag_{i-1}). This captures grammatical structure (e.g., a determiner is often followed by a noun).\n\n * Emission/Observation Probabilities: P(Word_i | Tag_i), the probability of a specific word (Word_i) being observed, given a particular tag (Tag_i). For example, P(\"run\" | Verb) would be higher than P(\"run\" | Noun) if \"run\" is more commonly a verb. (Though sometimes P(Tag_i | Word_i) is also considered).\n\n * How it's used: Models like Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs) use these probabilities to find the most likely sequence of POS tags for a given sentence.\n\n4. Word Sense Disambiguation (WSD):\n\n * Goal: To identify the correct meaning (sense) of a word that has multiple meanings (polysemy), based on its surrounding context.\n\n * Notation: P(Sense_k | Context) – the probability of a word having a particular sense (Sense_k) given the words or features in its surrounding context.\n\n * Example: For the word \"bank,\"\n\n * P(financial_institution_sense | \"money\", \"loan\", \"account\") would be high.\n\n * P(river_edge_sense | \"river\", \"water\", \"boat\") would be high.\n\n * How it's used: WSD systems estimate these probabilities based on features extracted from the context, often using supervised machine learning or knowledge-based approaches.\n\n________________________________________\n\nSummary\n\nConditional probability is a cornerstone of statistical NLP. It allows systems to make more informed and accurate predictions about language by explicitly modeling how the likelihood of linguistic phenomena changes based on relevant context or prior information. This ability to quantify and leverage dependencies is crucial for tackling the inherent ambiguity, variability, and often noisy nature of natural language data. It helps NLP models \"make sense\" of text and speech by understanding relationships between linguistic units.",
    "enhanced_text": "[NLP] Conditional Probability in Natural Language Processing (NLP)\n\nConditional probability is a fundamental concept in probability theory that measures the likelihood of an event occurring, given that another event has already occurred. In NLP, this \"event\" can be the occurrence of a word, a phrase, a part-of-speech tag, a document class, or any other linguistic feature. Modeling the likelihood of one linguistic event given the presence or context of another is central to how many NLP tasks are approached.\n\n________________________________________\n\nNLP Examples Using Conditional Probability\n\n1. Language Modeling:\n\n * Goal: To predict the probability of the next word (w_n) in a sequence, given the sequence of words that have come before it (w_1, w_2, ..., w_{n-1}).\n\n * Notation: This is written as P(w_n | w_1, w_2, ..., w_{n-1}).\n\n This reads as \"the probability of word w_n given the preceding words w_1 through w_{n-1}.\"\n\n * Example:\n\n * P(\"morning\" | \"Good\") would likely be high, as \"Good morning\" is a common phrase.\n\n * P(\"banana\" | \"Good\") would likely be very low, as \"Good banana\" is not a typical sequence.\n\n * How it's used: Language models, such as n-gram models or more advanced neural network models (like RNNs or Transformers), estimate these conditional probabilities. N-gram models, for instance, simplify this by assuming the next word only depends on a fixed number of preceding words (e.g., P(w_n | w_{n-1}) for a bigram model).\n\n2. Naive Bayes Text Classification:\n\n * Goal: To assign a category or class label to a document (e.g., spam/not-spam, positive/negative sentiment). This involves calculating P(Class | Document) — the probability that a document belongs to a particular class, given the words (or other features) in that document.\n\n * Using Bayes’ Theorem:\n\n P(Class | Document) = [P(Document | Class) * P(Class)] / P(Document)\n\n The term P(Document | Class) is often broken down (assuming conditional independence of words, hence \"Naive\") into a product of conditional probabilities of individual words given the class.\n\n * Example conditional probabilities involved:\n\n * P(\"free\" | spam): The probability of the word \"free\" appearing in a document, given that the document is spam. This might be high.\n\n * P(\"meeting\" | not-spam): The probability of the word \"meeting\" appearing, given the document is not spam.\n\n * How it's used: The classifier assigns the class that has the highest posterior probability P(Class | Document).\n\n3. Part-of-Speech (POS) Tagging:\n\n * Goal: To assign the correct grammatical tag (e.g., noun, verb, adjective) to each word in a sentence.\n\n * Conditional probabilities used:\n\n * Transition Probabilities: P(Tag_i | Tag_{i-1}), the probability of the current tag (Tag_i) given the previous tag (Tag_{i-1}). This captures grammatical structure (e.g., a determiner is often followed by a noun).\n\n * Emission/Observation Probabilities: P(Word_i | Tag_i), the probability of a specific word (Word_i) being observed, given a particular tag (Tag_i). For example, P(\"run\" | Verb) would be higher than P(\"run\" | Noun) if \"run\" is more commonly a verb. (Though sometimes P(Tag_i | Word_i) is also considered).\n\n * How it's used: Models like Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs) use these probabilities to find the most likely sequence of POS tags for a given sentence.\n\n4. Word Sense Disambiguation (WSD):\n\n * Goal: To identify the correct meaning (sense) of a word that has multiple meanings (polysemy), based on its surrounding context.\n\n * Notation: P(Sense_k | Context) – the probability of a word having a particular sense (Sense_k) given the words or features in its surrounding context.\n\n * Example: For the word \"bank,\"\n\n * P(financial_institution_sense | \"money\", \"loan\", \"account\") would be high.\n\n * P(river_edge_sense | \"river\", \"water\", \"boat\") would be high.\n\n * How it's used: WSD systems estimate these probabilities based on features extracted from the context, often using supervised machine learning or knowledge-based approaches.\n\n________________________________________\n\nSummary\n\nConditional probability is a cornerstone of statistical NLP. It allows systems to make more informed and accurate predictions about language by explicitly modeling how the likelihood of linguistic phenomena changes based on relevant context or prior information. This ability to quantify and leverage dependencies is crucial for tackling the inherent ambiguity, variability, and often noisy nature of natural language data. It helps NLP models \"make sense\" of text and speech by understanding relationships between linguistic units.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(b) Conditional Probability.txt",
    "file_name": "lec4-(b) Conditional Probability.txt",
    "filename_keywords": [
      "lec4",
      "conditional",
      "probability"
    ],
    "content_keywords": [
      "language",
      "in nlp",
      "wsd",
      "pos",
      "notation",
      "how",
      "part",
      "account",
      "event",
      "conditional",
      "language modeling:",
      "hmms",
      "verb",
      "using bayes",
      "for",
      "observation probabilities",
      "rnns",
      "theorem",
      "naive bayes text classification:",
      "make sense",
      ") would likely be high, as",
      "class",
      "example",
      "transformers",
      "naive bayes text classification",
      "summary\n\nconditional",
      "meeting",
      "emission",
      "models",
      "transition probabilities",
      "river",
      "water",
      "part-of-speech (pos) tagging:",
      "natural language processing",
      "hidden markov models",
      "word sense disambiguation (wsd):",
      "nlp examples using conditional probability",
      "tagging",
      "modeling",
      "this",
      "nlp",
      "though",
      "naive",
      "good",
      "noun",
      "* example:\n\n * p(",
      "is a common phrase.\n\n * p(",
      "money",
      "goal",
      "the",
      "context",
      "crfs",
      "document",
      "speech",
      "word sense disambiguation",
      "good banana",
      "bank,",
      "boat",
      "conditional probability",
      "loan",
      "free",
      "language modeling",
      "run",
      "conditional random fields"
    ],
    "technical_terms": [
      "language",
      "in nlp",
      "wsd",
      "pos",
      "notation",
      "how",
      "part",
      "conditional",
      "hmms",
      "verb",
      "using bayes",
      "for",
      "observation probabilities",
      "rnns",
      "theorem",
      "class",
      "example",
      "transformers",
      "naive bayes text classification",
      "summary\n\nconditional",
      "emission",
      "models",
      "transition probabilities",
      "natural language processing",
      "hidden markov models",
      "nlp examples using conditional probability",
      "tagging",
      "modeling",
      "this",
      "nlp",
      "though",
      "naive",
      "good",
      "noun",
      "goal",
      "the",
      "context",
      "crfs",
      "document",
      "speech",
      "word sense disambiguation",
      "conditional probability",
      "language modeling",
      "conditional random fields"
    ],
    "all_keywords": [
      "language",
      "lec4",
      "in nlp",
      "wsd",
      "pos",
      "notation",
      "how",
      "part",
      "account",
      "event",
      "conditional",
      "language modeling:",
      "hmms",
      "verb",
      "using bayes",
      "for",
      "observation probabilities",
      "rnns",
      "theorem",
      "naive bayes text classification:",
      "make sense",
      ") would likely be high, as",
      "class",
      "example",
      "transformers",
      "naive bayes text classification",
      "summary\n\nconditional",
      "meeting",
      "emission",
      "models",
      "transition probabilities",
      "river",
      "water",
      "part-of-speech (pos) tagging:",
      "natural language processing",
      "hidden markov models",
      "word sense disambiguation (wsd):",
      "nlp examples using conditional probability",
      "tagging",
      "modeling",
      "this",
      "nlp",
      "though",
      "naive",
      "good",
      "noun",
      "* example:\n\n * p(",
      "is a common phrase.\n\n * p(",
      "money",
      "goal",
      "the",
      "context",
      "crfs",
      "document",
      "speech",
      "word sense disambiguation",
      "good banana",
      "bank,",
      "boat",
      "conditional probability",
      "loan",
      "free",
      "language modeling",
      "probability",
      "run",
      "conditional random fields"
    ],
    "keyword_string": "language lec4 in nlp wsd pos notation how part account event conditional language modeling: hmms verb using bayes for observation probabilities rnns theorem naive bayes text classification: make sense ) would likely be high, as class example transformers naive bayes text classification summary\n\nconditional meeting emission models transition probabilities river water part-of-speech (pos) tagging: natural language processing hidden markov models word sense disambiguation (wsd): nlp examples using conditional probability tagging modeling this nlp though naive good noun * example:\n\n * p( is a common phrase.\n\n * p( money goal the context crfs document speech word sense disambiguation good banana bank, boat conditional probability loan free language modeling probability run conditional random fields",
    "token_count": 1215,
    "word_count": 694,
    "sentence_count": 37,
    "paragraph_count": 37,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5711934156378601,
    "avg_sentence_length": 18.756756756756758,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": true,
    "content_type": "Technical, Structured, Definitions"
  },
  {
    "document_id": 123,
    "document_hash": "b44628b806fe",
    "content": "Language Models: Deeper Dive\n\nA language model is essentially a probability distribution over sequences of words. It tells us how likely a sequence is to appear in a language.\n\nMathematical Formulation\n\nGiven a sequence of words W=w1,w2,...,wnW = w_1, w_2, ..., w_nW=w1​,w2​,...,wn​, the language model computes:\n\nP(W)=P(w1,w2,...,wn)P(W) = P(w_1, w_2, ..., w_n)P(W)=P(w1​,w2​,...,wn​) \n\nThis joint probability can be decomposed using the chain rule of probability:\n\nP(W)=P(w1)×P(w2∣w1)×P(w3∣w1,w2)×⋯×P(wn∣w1,...,wn−1)P(W) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_1, w_2) \\times \\cdots \\times P(w_n | w_1, ..., w_{n-1})P(W)=P(w1​)×P(w2​∣w1​)×P(w3​∣w1​,w2​)×⋯×P(wn​∣w1​,...,wn−1​) \n\nWhy Decompose?\n\nBecause directly computing P(w1,w2,...,wn)P(w_1, w_2, ..., w_n)P(w1​,w2​,...,wn​) is almost impossible due to the enormous number of possible sequences.\n\nSo instead, we calculate the probability of each word given the words that came before it.\n\nExample\n\nConsider the sentence: \"I love natural language processing\"\n\nA language model estimates:\n\nP(\"I love natural language processing\")=P(\"I\")×P(\"love\"∣\"I\")×P(\"natural\"∣\"I love\")×P(\"language\"∣\"I love natural\")×P(\"processing\"∣\"I love natural language\")P(\\text{\"I love natural language processing\"}) = P(\\text{\"I\"}) \\times P(\\text{\"love\"}|\\text{\"I\"}) \\times P(\\text{\"natural\"}|\\text{\"I love\"}) \\times P(\\text{\"language\"}|\\text{\"I love natural\"}) \\times P(\\text{\"processing\"}|\\text{\"I love natural language\"})P(\"I love natural language processing\")=P(\"I\")×P(\"love\"∣\"I\")×P(\"natural\"∣\"I love\")×P(\"language\"∣\"I love natural\")×P(\"processing\"∣\"I love natural language\") \n\nTypes of Language Models\n\nUnigram Model:\nAssumes each word is independent of others:\n\nP(W)=∏i=1nP(wi)P(W) = \\prod_{i=1}^{n} P(w_i)P(W)=i=1∏n​P(wi​) \n\n(ignores context, so not very accurate)\n\nBigram Model:\nLooks at only the previous word (Markov assumption):\n\nP(wi∣w1,...,wi−1)≈P(wi∣wi−1)P(w_i | w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-1})P(wi​∣w1​,...,wi−1​)≈P(wi​∣wi−1​) \n\nTrigram Model:\nConsiders two previous words:\n\nP(wi∣w1,...,wi−1)≈P(wi∣wi−2,wi−1)P(w_i | w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-2}, w_{i-1})P(wi​∣w1​,...,wi−1​)≈P(wi​∣wi−2​,wi−1​) \n\nNeural Language Models / Transformer-based Models:\nUse deep learning to consider long-range dependencies and complex contexts.\n\nSummary\n\nLanguage models predict the next word given the previous context.\n\nThey enable applications like autocomplete, speech recognition, machine translation, and text generation.\n\nThe core of language models is conditional probability — predicting P(wn∣w1,...,wn−1)P(w_n | w_1, ..., w_{n-1})P(wn​∣w1​,...,wn−1​).",
    "enhanced_text": "[NLP] Language Models: Deeper Dive\n\nA language model is essentially a probability distribution over sequences of words. It tells us how likely a sequence is to appear in a language.\n\nMathematical Formulation\n\nGiven a sequence of words W=w1,w2,...,wnW = w_1, w_2, ..., w_nW=w1​,w2​,...,wn​, the language model computes:\n\nP(W)=P(w1,w2,...,wn)P(W) = P(w_1, w_2, ..., w_n)P(W)=P(w1​,w2​,...,wn​) \n\nThis joint probability can be decomposed using the chain rule of probability:\n\nP(W)=P(w1)×P(w2∣w1)×P(w3∣w1,w2)×⋯×P(wn∣w1,...,wn−1)P(W) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_1, w_2) \\times \\cdots \\times P(w_n | w_1, ..., w_{n-1})P(W)=P(w1​)×P(w2​∣w1​)×P(w3​∣w1​,w2​)×⋯×P(wn​∣w1​,...,wn−1​) \n\nWhy Decompose?\n\nBecause directly computing P(w1,w2,...,wn)P(w_1, w_2, ..., w_n)P(w1​,w2​,...,wn​) is almost impossible due to the enormous number of possible sequences.\n\nSo instead, we calculate the probability of each word given the words that came before it.\n\nExample\n\nConsider the sentence: \"I love natural language processing\"\n\nA language model estimates:\n\nP(\"I love natural language processing\")=P(\"I\")×P(\"love\"∣\"I\")×P(\"natural\"∣\"I love\")×P(\"language\"∣\"I love natural\")×P(\"processing\"∣\"I love natural language\")P(\\text{\"I love natural language processing\"}) = P(\\text{\"I\"}) \\times P(\\text{\"love\"}|\\text{\"I\"}) \\times P(\\text{\"natural\"}|\\text{\"I love\"}) \\times P(\\text{\"language\"}|\\text{\"I love natural\"}) \\times P(\\text{\"processing\"}|\\text{\"I love natural language\"})P(\"I love natural language processing\")=P(\"I\")×P(\"love\"∣\"I\")×P(\"natural\"∣\"I love\")×P(\"language\"∣\"I love natural\")×P(\"processing\"∣\"I love natural language\") \n\nTypes of Language Models\n\nUnigram Model:\nAssumes each word is independent of others:\n\nP(W)=∏i=1nP(wi)P(W) = \\prod_{i=1}^{n} P(w_i)P(W)=i=1∏n​P(wi​) \n\n(ignores context, so not very accurate)\n\nBigram Model:\nLooks at only the previous word (Markov assumption):\n\nP(wi∣w1,...,wi−1)≈P(wi∣wi−1)P(w_i | w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-1})P(wi​∣w1​,...,wi−1​)≈P(wi​∣wi−1​) \n\nTrigram Model:\nConsiders two previous words:\n\nP(wi∣w1,...,wi−1)≈P(wi∣wi−2,wi−1)P(w_i | w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-2}, w_{i-1})P(wi​∣w1​,...,wi−1​)≈P(wi​∣wi−2​,wi−1​) \n\nNeural Language Models / Transformer-based Models:\nUse deep learning to consider long-range dependencies and complex contexts.\n\nSummary\n\nLanguage models predict the next word given the previous context.\n\nThey enable applications like autocomplete, speech recognition, machine translation, and text generation.\n\nThe core of language models is conditional probability — predicting P(wn∣w1,...,wn−1)P(w_n | w_1, ..., w_{n-1})P(wn​∣w1​,...,wn−1​).",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(c) Language Models.txt",
    "file_name": "lec4-(c) Language Models.txt",
    "filename_keywords": [
      "language",
      "lec4",
      "models"
    ],
    "content_keywords": [
      "assumes",
      "because",
      "language",
      "i love",
      ")=p(",
      "markov",
      "processing",
      "neural language models",
      "summary\n\nlanguage",
      "bigram model",
      "deeper dive\n\na",
      "})p(",
      "i love natural",
      "models",
      "they",
      "transformer",
      "i love natural language",
      ")×p(",
      "why decompose",
      "mathematical formulation\n\ngiven",
      "this",
      "types",
      "}) \\times p(\\text{",
      "}|\\text{",
      "trigram model",
      "language models",
      "looks",
      "example\n\nconsider",
      "language models\n\nunigram model",
      "the",
      "}) = p(\\text{",
      "considers",
      "use"
    ],
    "technical_terms": [
      "assumes",
      "because",
      "markov",
      "neural language models",
      "summary\n\nlanguage",
      "bigram model",
      "deeper dive\n\na",
      "models",
      "they",
      "transformer",
      "why decompose",
      "mathematical formulation\n\ngiven",
      "this",
      "types",
      "trigram model",
      "language models",
      "looks",
      "example\n\nconsider",
      "language models\n\nunigram model",
      "the",
      "considers",
      "use"
    ],
    "all_keywords": [
      "language",
      "lec4",
      "because",
      "assumes",
      "i love",
      ")=p(",
      "markov",
      "processing",
      "neural language models",
      "summary\n\nlanguage",
      "bigram model",
      "deeper dive\n\na",
      "})p(",
      "i love natural",
      "models",
      "they",
      "transformer",
      "i love natural language",
      ")×p(",
      "why decompose",
      "mathematical formulation\n\ngiven",
      "this",
      "types",
      "}) \\times p(\\text{",
      "}|\\text{",
      "trigram model",
      "language models",
      "looks",
      "example\n\nconsider",
      "language models\n\nunigram model",
      "the",
      "}) = p(\\text{",
      "considers",
      "use"
    ],
    "keyword_string": "language lec4 because assumes i love )=p( markov processing neural language models summary\n\nlanguage bigram model deeper dive\n\na })p( i love natural models they transformer i love natural language )×p( why decompose mathematical formulation\n\ngiven this types }) \\times p(\\text{ }|\\text{ trigram model language models looks example\n\nconsider language models\n\nunigram model the }) = p(\\text{ considers use",
    "token_count": 1018,
    "word_count": 286,
    "sentence_count": 9,
    "paragraph_count": 27,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.28094302554027506,
    "avg_sentence_length": 31.77777777777778,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 124,
    "document_hash": "1800f8289cb3",
    "content": "Quick Recap of the Chain Rule of Probability\n\nThe chain rule of probability allows us to calculate the joint probability of a sequence of events occurring together. For any sequence of events, say A, B, C, and D:\n\nP(A, B, C, D) = P(A) * P(B | A) * P(C | A, B) * P(D | A, B, C)\n\nIn words:\n\n* P(A, B, C, D) is the probability that all events A, B, C, and D occur.\n\n* P(A) is the probability of event A occurring.\n\n* P(B | A) is the conditional probability of event B occurring, given that event A has already occurred.\n\n* P(C | A, B) is the conditional probability of event C occurring, given that events A and B have already occurred.\n\n* P(D | A, B, C) is the conditional probability of event D occurring, given that events A, B, and C have already occurred.\n\nThis rule expresses the joint probability of a sequence of events as a product of conditional probabilities, where the probability of each subsequent event is conditioned on all the preceding events.\n\n________________________________________\n\nApplying the Chain Rule to Words in a Sentence\n\nWe can apply the chain rule to determine the probability of a sequence of words, i.e., a sentence. Consider the sentence:\n\n\"its water is so transparent\"\n\nThe joint probability of this entire sentence occurring is:\n\nP(\"its\", \"water\", \"is\", \"so\", \"transparent\") =\n\n P(\"its\") *\n\n P(\"water\" | \"its\") *\n\n P(\"is\" | \"its\", \"water\") *\n\n P(\"so\" | \"its\", \"water\", \"is\") *\n\n P(\"transparent\" | \"its\", \"water\", \"is\", \"so\")\n\n________________________________________\n\nIntuition Behind It (for the example sentence)\n\nLet's break down what each term means:\n\n* P(\"its\"): This is the probability that a sentence (or a text segment) starts with the word \"its\".\n\n* P(\"water\" | \"its\"): This is the probability that the word \"water\" appears immediately after the word \"its\".\n\n* P(\"is\" | \"its\", \"water\"): This is the probability that the word \"is\" appears immediately after the sequence \"its water\".\n\n* P(\"so\" | \"its\", \"water\", \"is\"): This is the probability that the word \"so\" appears immediately after the sequence \"its water is\".\n\n* P(\"transparent\" | \"its\", \"water\", \"is\", \"so\"): This is the probability that the word \"transparent\" appears immediately after the sequence \"its water is so\".\n\nEssentially, each conditional probability term tells us how likely the next word is, given the full context of all the words that have come before it in the sentence.\n\n________________________________________\n\nWhy is This Important for Language Modeling?\n\n* It captures context: The chain rule provides a formal way to express how the probability of a word depends on the preceding words. This dependency is crucial for understanding and modeling the structure and meaning of natural language.\n\n* It enables language models to assign probabilities to sentences: By calculating this joint probability, a language model can determine how likely a given sentence is according to the model's learned understanding of the language.\n\n* Applications: This ability to assign probabilities to sequences of words is fundamental for many NLP tasks, such as:\n\n * Predicting the next word in a sequence (e.g., in auto-complete features).\n\n * Spelling and grammar correction (suggesting more probable sequences).\n\n * Machine translation (choosing the most probable translation).\n\n * Speech recognition (disambiguating between acoustically similar phrases).\n\n * Text generation (creating coherent and natural-sounding text).\n\n________________________________________\n\nPractical Note: The Need for Approximation\n\nWhile the chain rule provides a theoretically complete way to calculate the probability of a sentence, directly estimating the conditional probabilities P(word_n | word_1, ..., word_{n-1}) for long contexts is very difficult in practice:\n\n* Data Sparsity: Many long sequences of words will occur very rarely, or not at all, in any finite training corpus, making it impossible to get reliable probability estimates.\n\n* Computational Complexity: The number of possible preceding contexts grows exponentially with the length of the context.\n\nTo address these issues, practical language models often use approximations like n-gram models (as discussed previously). N-gram models make a Markov assumption, stating that the probability of a word depends only on a fixed number (n-1) of preceding words, rather than the entire history. For example, a bigram model approximates P(word_k | word_1, ..., word_{k-1}) with P(word_k | word_{k-1}).",
    "enhanced_text": "[NLP] Quick Recap of the Chain Rule of Probability\n\nThe chain rule of probability allows us to calculate the joint probability of a sequence of events occurring together. For any sequence of events, say A, B, C, and D:\n\nP(A, B, C, D) = P(A) * P(B | A) * P(C | A, B) * P(D | A, B, C)\n\nIn words:\n\n* P(A, B, C, D) is the probability that all events A, B, C, and D occur.\n\n* P(A) is the probability of event A occurring.\n\n* P(B | A) is the conditional probability of event B occurring, given that event A has already occurred.\n\n* P(C | A, B) is the conditional probability of event C occurring, given that events A and B have already occurred.\n\n* P(D | A, B, C) is the conditional probability of event D occurring, given that events A, B, and C have already occurred.\n\nThis rule expresses the joint probability of a sequence of events as a product of conditional probabilities, where the probability of each subsequent event is conditioned on all the preceding events.\n\n________________________________________\n\nApplying the Chain Rule to Words in a Sentence\n\nWe can apply the chain rule to determine the probability of a sequence of words, i.e., a sentence. Consider the sentence:\n\n\"its water is so transparent\"\n\nThe joint probability of this entire sentence occurring is:\n\nP(\"its\", \"water\", \"is\", \"so\", \"transparent\") =\n\n P(\"its\") *\n\n P(\"water\" | \"its\") *\n\n P(\"is\" | \"its\", \"water\") *\n\n P(\"so\" | \"its\", \"water\", \"is\") *\n\n P(\"transparent\" | \"its\", \"water\", \"is\", \"so\")\n\n________________________________________\n\nIntuition Behind It (for the example sentence)\n\nLet's break down what each term means:\n\n* P(\"its\"): This is the probability that a sentence (or a text segment) starts with the word \"its\".\n\n* P(\"water\" | \"its\"): This is the probability that the word \"water\" appears immediately after the word \"its\".\n\n* P(\"is\" | \"its\", \"water\"): This is the probability that the word \"is\" appears immediately after the sequence \"its water\".\n\n* P(\"so\" | \"its\", \"water\", \"is\"): This is the probability that the word \"so\" appears immediately after the sequence \"its water is\".\n\n* P(\"transparent\" | \"its\", \"water\", \"is\", \"so\"): This is the probability that the word \"transparent\" appears immediately after the sequence \"its water is so\".\n\nEssentially, each conditional probability term tells us how likely the next word is, given the full context of all the words that have come before it in the sentence.\n\n________________________________________\n\nWhy is This Important for Language Modeling?\n\n* It captures context: The chain rule provides a formal way to express how the probability of a word depends on the preceding words. This dependency is crucial for understanding and modeling the structure and meaning of natural language.\n\n* It enables language models to assign probabilities to sentences: By calculating this joint probability, a language model can determine how likely a given sentence is according to the model's learned understanding of the language.\n\n* Applications: This ability to assign probabilities to sequences of words is fundamental for many NLP tasks, such as:\n\n * Predicting the next word in a sequence (e.g., in auto-complete features).\n\n * Spelling and grammar correction (suggesting more probable sequences).\n\n * Machine translation (choosing the most probable translation).\n\n * Speech recognition (disambiguating between acoustically similar phrases).\n\n * Text generation (creating coherent and natural-sounding text).\n\n________________________________________\n\nPractical Note: The Need for Approximation\n\nWhile the chain rule provides a theoretically complete way to calculate the probability of a sentence, directly estimating the conditional probabilities P(word_n | word_1, ..., word_{n-1}) for long contexts is very difficult in practice:\n\n* Data Sparsity: Many long sequences of words will occur very rarely, or not at all, in any finite training corpus, making it impossible to get reliable probability estimates.\n\n* Computational Complexity: The number of possible preceding contexts grows exponentially with the length of the context.\n\nTo address these issues, practical language models often use approximations like n-gram models (as discussed previously). N-gram models make a Markov assumption, stating that the probability of a word depends only on a fixed number (n-1) of preceding words, rather than the entire history. For example, a bigram model approximates P(word_k | word_1, ..., word_{k-1}) with P(word_k | word_{k-1}).",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(d) Chain Rule of Probability.txt",
    "file_name": "lec4-(d) Chain Rule of Probability.txt",
    "filename_keywords": [
      "lec4",
      "probability",
      "rule",
      "chain"
    ],
    "content_keywords": [
      "text",
      "the need",
      "probability\n\nthe",
      "chain rule",
      "its water is so transparent",
      "this important",
      "markov",
      "many",
      "its water is so",
      "for",
      "let",
      "predicting",
      "transparent",
      "essentially",
      "approximation\n\nwhile",
      "computational complexity",
      "its water is",
      "words",
      "practical note",
      "its",
      "water",
      "sentence\n\nwe",
      "quick recap",
      "spelling",
      "applying",
      "this",
      "nlp",
      "intuition behind it",
      ") *\n\n p(",
      "its water",
      "machine",
      "data sparsity",
      "consider",
      "the",
      "why",
      "applications",
      "speech",
      "language modeling"
    ],
    "technical_terms": [
      "text",
      "the need",
      "probability\n\nthe",
      "chain rule",
      "this important",
      "markov",
      "many",
      "for",
      "let",
      "predicting",
      "essentially",
      "approximation\n\nwhile",
      "computational complexity",
      "words",
      "practical note",
      "sentence\n\nwe",
      "quick recap",
      "spelling",
      "applying",
      "this",
      "nlp",
      "intuition behind it",
      "machine",
      "data sparsity",
      "consider",
      "the",
      "why",
      "applications",
      "speech",
      "language modeling"
    ],
    "all_keywords": [
      "lec4",
      "rule",
      "text",
      "the need",
      "probability\n\nthe",
      "chain rule",
      "its water is so transparent",
      "this important",
      "markov",
      "many",
      "its water is so",
      "for",
      "let",
      "predicting",
      "transparent",
      "essentially",
      "approximation\n\nwhile",
      "computational complexity",
      "its water is",
      "words",
      "practical note",
      "its",
      "chain",
      "water",
      "sentence\n\nwe",
      "quick recap",
      "spelling",
      "applying",
      "this",
      "nlp",
      "intuition behind it",
      ") *\n\n p(",
      "its water",
      "machine",
      "data sparsity",
      "consider",
      "the",
      "why",
      "applications",
      "speech",
      "language modeling",
      "probability"
    ],
    "keyword_string": "lec4 rule text the need probability\n\nthe chain rule its water is so transparent this important markov many its water is so for let predicting transparent essentially approximation\n\nwhile computational complexity its water is words practical note its chain water sentence\n\nwe quick recap spelling applying this nlp intuition behind it ) *\n\n p( its water machine data sparsity consider the why applications speech language modeling probability",
    "token_count": 1212,
    "word_count": 695,
    "sentence_count": 28,
    "paragraph_count": 46,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5734323432343235,
    "avg_sentence_length": 24.821428571428573,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 125,
    "document_hash": "9b24d47eae65",
    "content": "What is an N-gram Model?\n\n* An n-gram is a contiguous sequence of 'n' items (typically words, but can also be characters or other linguistic units) from a given sample of text or speech.\n\n * Unigram (1-gram): A single word.\n\n Examples: \"please\", \"turn\", \"your\", \"homework\", \"in\"\n\n * Bigram (2-gram): A sequence of two consecutive words.\n\n Examples: \"please turn\", \"turn your\", \"your homework\", \"homework in\"\n\n * Trigram (3-gram): A sequence of three consecutive words.\n\n Examples: \"please turn your\", \"turn your homework\", \"your homework in\"\n\n * And so on for 4-grams (four words), 5-grams (five words), etc.\n\n* An n-gram model is a type of probabilistic language model that predicts the probability of a given word (w_n) occurring after a sequence of preceding words. It makes a simplifying assumption (called the Markov assumption) that the probability of the next word depends only on the previous 'n-1' words.\n\n* The general probability of a word w_n given all preceding words w_1, w_2, ..., w_{n-1} is written as:\n\n P(w_n | w_1, w_2, ..., w_{n-1})\n\n* In an n-gram model, this probability is approximated by considering only the 'n-1' immediately preceding words:\n\n P(w_n | w_1, w_2, ..., w_{n-1}) is approximated by P(w_n | w_{n-(n-1)}, ..., w_{n-1})\n\n This means the model looks at a context window of size 'n-1' to predict the n-th word.\n\n For specific values of 'n':\n\n * Unigram model (n=1): Assumes the probability of a word is independent of any prior words.\n\n P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n)\n\n This is simply the frequency of the word in the corpus.\n\n * Bigram model (n=2): The probability of a word depends only on the single immediately preceding word.\n\n P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n | w_{n-1})\n\n Example: P(\"your\" | \"turn\")\n\n * Trigram model (n=3): The probability of a word depends only on the two immediately preceding words.\n\n P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n | w_{n-2}, w_{n-1})\n\n Example: P(\"homework\" | \"turn\", \"your\")\n\n________________________________________\n\nWhy do we use N-gram Models?\n\n* Simplification (Markov Assumption): Calculating the probability of a word given *all* previous words in a long sequence is computationally very complex and requires an enormous amount of data to estimate reliably (due to the curse of dimensionality). N-gram models simplify this by assuming that only a limited, fixed-size context (the last n-1 words) is relevant for predicting the next word.\n\n* Data efficiency and Estimation: The probabilities for n-grams can be estimated directly from their frequencies in a large training corpus.\n\n For example, P(w_n | w_{n-1}) in a bigram model can be estimated as:\n\n Count(w_{n-1}, w_n) / Count(w_{n-1})\n\n where Count(w_{n-1}, w_n) is how many times the sequence \"w_{n-1} w_n\" appears, and Count(w_{n-1}) is how many times \"w_{n-1}\" appears. This count-based estimation is relatively straightforward.\n\n* Good balance between context and feasibility:\n\n * Unigrams capture no context.\n\n * Very high 'n' values (e.g., 5-grams, 6-grams) capture more context but lead to sparsity issues (many n-grams will occur very few times or not at all in the training data, making probability estimates unreliable).\n\n * Bigrams and trigrams often provide a good balance. They capture some local word-to-word dependencies and context, which is useful for many NLP tasks (like speech recognition, machine translation, spelling correction, text generation), without requiring excessively large datasets or computational power.",
    "enhanced_text": "[NLP] What is an N-gram Model?\n\n* An n-gram is a contiguous sequence of 'n' items (typically words, but can also be characters or other linguistic units) from a given sample of text or speech.\n\n * Unigram (1-gram): A single word.\n\n Examples: \"please\", \"turn\", \"your\", \"homework\", \"in\"\n\n * Bigram (2-gram): A sequence of two consecutive words.\n\n Examples: \"please turn\", \"turn your\", \"your homework\", \"homework in\"\n\n * Trigram (3-gram): A sequence of three consecutive words.\n\n Examples: \"please turn your\", \"turn your homework\", \"your homework in\"\n\n * And so on for 4-grams (four words), 5-grams (five words), etc.\n\n* An n-gram model is a type of probabilistic language model that predicts the probability of a given word (w_n) occurring after a sequence of preceding words. It makes a simplifying assumption (called the Markov assumption) that the probability of the next word depends only on the previous 'n-1' words.\n\n* The general probability of a word w_n given all preceding words w_1, w_2, ..., w_{n-1} is written as:\n\n P(w_n | w_1, w_2, ..., w_{n-1})\n\n* In an n-gram model, this probability is approximated by considering only the 'n-1' immediately preceding words:\n\n P(w_n | w_1, w_2, ..., w_{n-1}) is approximated by P(w_n | w_{n-(n-1)}, ..., w_{n-1})\n\n This means the model looks at a context window of size 'n-1' to predict the n-th word.\n\n For specific values of 'n':\n\n * Unigram model (n=1): Assumes the probability of a word is independent of any prior words.\n\n P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n)\n\n This is simply the frequency of the word in the corpus.\n\n * Bigram model (n=2): The probability of a word depends only on the single immediately preceding word.\n\n P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n | w_{n-1})\n\n Example: P(\"your\" | \"turn\")\n\n * Trigram model (n=3): The probability of a word depends only on the two immediately preceding words.\n\n P(w_n | w_1, ..., w_{n-1}) is approximated by P(w_n | w_{n-2}, w_{n-1})\n\n Example: P(\"homework\" | \"turn\", \"your\")\n\n________________________________________\n\nWhy do we use N-gram Models?\n\n* Simplification (Markov Assumption): Calculating the probability of a word given *all* previous words in a long sequence is computationally very complex and requires an enormous amount of data to estimate reliably (due to the curse of dimensionality). N-gram models simplify this by assuming that only a limited, fixed-size context (the last n-1 words) is relevant for predicting the next word.\n\n* Data efficiency and Estimation: The probabilities for n-grams can be estimated directly from their frequencies in a large training corpus.\n\n For example, P(w_n | w_{n-1}) in a bigram model can be estimated as:\n\n Count(w_{n-1}, w_n) / Count(w_{n-1})\n\n where Count(w_{n-1}, w_n) is how many times the sequence \"w_{n-1} w_n\" appears, and Count(w_{n-1}) is how many times \"w_{n-1}\" appears. This count-based estimation is relatively straightforward.\n\n* Good balance between context and feasibility:\n\n * Unigrams capture no context.\n\n * Very high 'n' values (e.g., 5-grams, 6-grams) capture more context but lead to sparsity issues (many n-grams will occur very few times or not at all in the training data, making probability estimates unreliable).\n\n * Bigrams and trigrams often provide a good balance. They capture some local word-to-word dependencies and context, which is useful for many NLP tasks (like speech recognition, machine translation, spelling correction, text generation), without requiring excessively large datasets or computational power.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(e) N-gram Model.txt",
    "file_name": "lec4-(e) N-gram Model.txt",
    "filename_keywords": [
      "lec4",
      "gram",
      "model"
    ],
    "content_keywords": [
      "assumes",
      "examples",
      "count",
      "bigrams",
      "w_{n-1} w_n",
      "model",
      "n-1",
      "markov",
      "bigram",
      "for",
      "homework",
      "very",
      "example",
      "turn your homework",
      "and",
      "please turn",
      "w_{n-1}",
      "models",
      "they",
      "turn your",
      "this",
      "unigram",
      "nlp",
      "simplification",
      "data",
      "good",
      "estimation",
      "unigrams",
      "your",
      "homework in",
      "the",
      "trigram",
      "why",
      "your homework in",
      "your homework",
      "turn",
      "calculating",
      "what",
      "markov assumption",
      "please",
      "please turn your"
    ],
    "technical_terms": [
      "assumes",
      "examples",
      "count",
      "bigrams",
      "model",
      "markov",
      "bigram",
      "for",
      "very",
      "example",
      "and",
      "models",
      "they",
      "this",
      "unigram",
      "nlp",
      "simplification",
      "data",
      "good",
      "estimation",
      "unigrams",
      "the",
      "trigram",
      "why",
      "calculating",
      "what",
      "markov assumption"
    ],
    "all_keywords": [
      "assumes",
      "lec4",
      "examples",
      "count",
      "bigrams",
      "w_{n-1} w_n",
      "model",
      "n-1",
      "markov",
      "bigram",
      "gram",
      "for",
      "homework",
      "very",
      "example",
      "turn your homework",
      "and",
      "please turn",
      "w_{n-1}",
      "models",
      "they",
      "turn your",
      "this",
      "unigram",
      "nlp",
      "simplification",
      "data",
      "good",
      "estimation",
      "unigrams",
      "your",
      "homework in",
      "the",
      "trigram",
      "why",
      "your homework in",
      "your homework",
      "turn",
      "calculating",
      "what",
      "markov assumption",
      "please",
      "please turn your"
    ],
    "keyword_string": "assumes lec4 examples count bigrams w_{n-1} w_n model n-1 markov bigram gram for homework very example turn your homework and please turn w_{n-1} models they turn your this unigram nlp simplification data good estimation unigrams your homework in the trigram why your homework in your homework turn calculating what markov assumption please please turn your",
    "token_count": 1080,
    "word_count": 545,
    "sentence_count": 23,
    "paragraph_count": 36,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5046296296296297,
    "avg_sentence_length": 23.695652173913043,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 126,
    "document_hash": "79b1b20d10fe",
    "content": "Markov Assumption\n\nThe Markov assumption states:\nThe probability of the next word depends only on a limited number of previous words (usually one or two), not the entire history.\n\nIn language modeling terms:\nFor a first-order Markov model (bigram model),\n\nP(wn∣w1,w2,...,wn−1)≈P(wn∣wn−1)P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-1})P(wn​∣w1​,w2​,...,wn−1​)≈P(wn​∣wn−1​) \n\nFor a second-order Markov model (trigram model),\n\nP(wn∣w1,w2,...,wn−1)≈P(wn∣wn−2,wn−1)P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-2}, w_{n-1})P(wn​∣w1​,w2​,...,wn−1​)≈P(wn​∣wn−2​,wn−1​) \n\nExample of Markov Assumption in Practice\n\nLet's say you want to estimate:\n\nP(\"the\"∣\"its water so transparent that\")P(\\text{\"the\"} | \\text{\"its water so transparent that\"})P(\"the\"∣\"its water so transparent that\") \n\nWithout Markov assumption, you consider the entire history:\n\nP(\"the\"∣\"its water so transparent that\")P(\\text{\"the\"} | \\text{\"its water so transparent that\"})P(\"the\"∣\"its water so transparent that\") \n\nWith a first-order Markov assumption (bigram), you approximate:\n\nP(\"the\"∣\"that\")P(\\text{\"the\"} | \\text{\"that\"})P(\"the\"∣\"that\") \n\nOr with a second-order Markov assumption (trigram), you approximate:\n\nP(\"the\"∣\"transparent that\")P(\\text{\"the\"} | \\text{\"transparent that\"})P(\"the\"∣\"transparent that\") \n\nThis massively reduces complexity and data requirements.",
    "enhanced_text": "[NLP] Markov Assumption\n\nThe Markov assumption states:\nThe probability of the next word depends only on a limited number of previous words (usually one or two), not the entire history.\n\nIn language modeling terms:\nFor a first-order Markov model (bigram model),\n\nP(wn∣w1,w2,...,wn−1)≈P(wn∣wn−1)P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-1})P(wn​∣w1​,w2​,...,wn−1​)≈P(wn​∣wn−1​) \n\nFor a second-order Markov model (trigram model),\n\nP(wn∣w1,w2,...,wn−1)≈P(wn∣wn−2,wn−1)P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-2}, w_{n-1})P(wn​∣w1​,w2​,...,wn−1​)≈P(wn​∣wn−2​,wn−1​) \n\nExample of Markov Assumption in Practice\n\nLet's say you want to estimate:\n\nP(\"the\"∣\"its water so transparent that\")P(\\text{\"the\"} | \\text{\"its water so transparent that\"})P(\"the\"∣\"its water so transparent that\") \n\nWithout Markov assumption, you consider the entire history:\n\nP(\"the\"∣\"its water so transparent that\")P(\\text{\"the\"} | \\text{\"its water so transparent that\"})P(\"the\"∣\"its water so transparent that\") \n\nWith a first-order Markov assumption (bigram), you approximate:\n\nP(\"the\"∣\"that\")P(\\text{\"the\"} | \\text{\"that\"})P(\"the\"∣\"that\") \n\nOr with a second-order Markov assumption (trigram), you approximate:\n\nP(\"the\"∣\"transparent that\")P(\\text{\"the\"} | \\text{\"transparent that\"})P(\"the\"∣\"transparent that\") \n\nThis massively reduces complexity and data requirements.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(f) Markov Assumption.txt",
    "file_name": "lec4-(f) Markov Assumption.txt",
    "filename_keywords": [
      "lec4",
      "markov",
      "assumption"
    ],
    "content_keywords": [
      "markov assumption\n\nthe markov",
      "the",
      "transparent that",
      "with",
      "its water so transparent that",
      "example",
      "without markov",
      "its water so transparent that",
      "this",
      "markov assumption",
      "markov",
      "that",
      "practice\n\nlet",
      "transparent that",
      "for"
    ],
    "technical_terms": [
      "markov assumption\n\nthe markov",
      "the",
      "with",
      "example",
      "without markov",
      "this",
      "markov assumption",
      "markov",
      "practice\n\nlet",
      "for"
    ],
    "all_keywords": [
      "lec4",
      "transparent that",
      "markov assumption\n\nthe markov",
      "the",
      "transparent that",
      "with",
      "its water so transparent that",
      "example",
      "without markov",
      "its water so transparent that",
      "this",
      "markov assumption",
      "markov",
      "that",
      "practice\n\nlet",
      "for",
      "assumption"
    ],
    "keyword_string": "lec4 transparent that markov assumption\n\nthe markov the transparent that with its water so transparent that example without markov its water so transparent that this markov assumption markov that practice\n\nlet for assumption",
    "token_count": 497,
    "word_count": 151,
    "sentence_count": 2,
    "paragraph_count": 16,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.3038229376257545,
    "avg_sentence_length": 75.5,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 127,
    "document_hash": "f7d933d05654",
    "content": "Spelling Correction Using N-grams and Edit Distance\n\nN-grams for Spelling Correction\n\nAn n-gram is a sequence of nnn contiguous units (words or characters).\n\nFor spelling correction, character n-grams are often used:\n\nHelps detect misspellings by comparing likely sequences of letters.\n\nFor example, “spelling” might generate bigrams: \"sp\", \"pe\", \"el\", \"ll\", \"in\", \"ng\".\n\nComparing n-gram overlaps can suggest the closest correct spelling to a misspelled word.\n\nAllowing Errors in Spelling Queries\n\nReal-world text input often contains typos or misspellings. To handle this:\n\nEdit Distance (Levenshtein Distance): Measures how many operations (insertions, deletions, substitutions) it takes to transform one string into another.\n\nExample:\n\n\"misspell\" → \"mispell\" (distance = 1, delete 's')\n\n\"misspell\" → \"mistell\" (distance = 2, substitute 's'→'t', delete 'p')\n\n\"misspell\" → \"misspelling\" (distance = 3, add 'ing')\n\nComputed efficiently with dynamic programming in O(mn)O(mn)O(mn) time where m,nm,nm,n are string lengths.\n\nLongest Common Subsequence (LCS): Finds the longest sequence of characters common to both strings in order, useful to judge similarity.\n\nProximity Search: Retrieve documents/words within a certain edit distance threshold from the query to handle typos or variations.\n\nPattern Matching\n\nSearching for variations or partial matches in text requires pattern matching:\n\nSimple Patterns:\n\nPrefixes: match start of words\n\n\"anti\" → matches \"antibody\", \"antique\"\n\nSuffixes: match end of words\n\n\"ix\" → matches \"matrix\", \"fix\"\n\nSubstrings: match any part within a word\n\n\"rapt\" → matches \"enrapture\", \"velociraptor\"\n\nRanges: match words lexicographically between two strings\n\n\"tin\" to \"tix\" → matches \"tip\", \"tire\", \"title\"\n\nPattern matching is more complex than exact word lookup and requires specialized data structures beyond inverted indices, like tries or suffix trees.",
    "enhanced_text": "[NLP] Spelling Correction Using N-grams and Edit Distance\n\nN-grams for Spelling Correction\n\nAn n-gram is a sequence of nnn contiguous units (words or characters).\n\nFor spelling correction, character n-grams are often used:\n\nHelps detect misspellings by comparing likely sequences of letters.\n\nFor example, “spelling” might generate bigrams: \"sp\", \"pe\", \"el\", \"ll\", \"in\", \"ng\".\n\nComparing n-gram overlaps can suggest the closest correct spelling to a misspelled word.\n\nAllowing Errors in Spelling Queries\n\nReal-world text input often contains typos or misspellings. To handle this:\n\nEdit Distance (Levenshtein Distance): Measures how many operations (insertions, deletions, substitutions) it takes to transform one string into another.\n\nExample:\n\n\"misspell\" → \"mispell\" (distance = 1, delete 's')\n\n\"misspell\" → \"mistell\" (distance = 2, substitute 's'→'t', delete 'p')\n\n\"misspell\" → \"misspelling\" (distance = 3, add 'ing')\n\nComputed efficiently with dynamic programming in O(mn)O(mn)O(mn) time where m,nm,nm,n are string lengths.\n\nLongest Common Subsequence (LCS): Finds the longest sequence of characters common to both strings in order, useful to judge similarity.\n\nProximity Search: Retrieve documents/words within a certain edit distance threshold from the query to handle typos or variations.\n\nPattern Matching\n\nSearching for variations or partial matches in text requires pattern matching:\n\nSimple Patterns:\n\nPrefixes: match start of words\n\n\"anti\" → matches \"antibody\", \"antique\"\n\nSuffixes: match end of words\n\n\"ix\" → matches \"matrix\", \"fix\"\n\nSubstrings: match any part within a word\n\n\"rapt\" → matches \"enrapture\", \"velociraptor\"\n\nRanges: match words lexicographically between two strings\n\n\"tin\" to \"tix\" → matches \"tip\", \"tire\", \"title\"\n\nPattern matching is more complex than exact word lookup and requires specialized data structures beyond inverted indices, like tries or suffix trees.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(g) Spelling Correction Using N-grams and Edit Distance.txt",
    "file_name": "lec4-(g) Spelling Correction Using N-grams and Edit Distance.txt",
    "filename_keywords": [
      "using",
      "lec4",
      "distance",
      "spelling",
      "edit",
      "grams",
      "correction"
    ],
    "content_keywords": [
      "helps",
      "tire",
      "tip",
      "edit distance",
      "antique",
      "tix",
      "allowing errors",
      "tin",
      "spelling correction using n",
      "prefixes",
      "enrapture",
      "retrieve",
      "for",
      "pattern",
      "lcs",
      "edit distance\n\nn",
      "measures",
      "example",
      ", delete",
      "misspell",
      "finds",
      "anti",
      "comparing",
      "longest common subsequence",
      "(distance = 3, add",
      "rapt",
      "antibody",
      "fix",
      "substrings",
      "computed",
      "simple patterns",
      "levenshtein distance",
      "ranges",
      "suffixes",
      "spelling queries\n\nreal",
      "velociraptor",
      "title",
      "mispell",
      "spelling correction\n\nan",
      "proximity search",
      "(distance = 2, substitute",
      "pattern matching\n\nsearching",
      "→ matches"
    ],
    "technical_terms": [
      "helps",
      "edit distance",
      "allowing errors",
      "spelling correction using n",
      "prefixes",
      "retrieve",
      "for",
      "pattern",
      "lcs",
      "edit distance\n\nn",
      "measures",
      "example",
      "finds",
      "comparing",
      "longest common subsequence",
      "substrings",
      "computed",
      "simple patterns",
      "levenshtein distance",
      "ranges",
      "suffixes",
      "spelling queries\n\nreal",
      "spelling correction\n\nan",
      "proximity search",
      "pattern matching\n\nsearching"
    ],
    "all_keywords": [
      "helps",
      "lec4",
      "tire",
      "distance",
      "tip",
      "edit distance",
      "antique",
      "tix",
      "allowing errors",
      "correction",
      "tin",
      "spelling correction using n",
      "prefixes",
      "enrapture",
      "for",
      "retrieve",
      "pattern",
      "lcs",
      "edit distance\n\nn",
      "grams",
      "measures",
      "example",
      ", delete",
      "misspell",
      "finds",
      "anti",
      "comparing",
      "longest common subsequence",
      "using",
      "(distance = 3, add",
      "rapt",
      "antibody",
      "fix",
      "spelling",
      "edit",
      "substrings",
      "computed",
      "simple patterns",
      "levenshtein distance",
      "ranges",
      "suffixes",
      "spelling queries\n\nreal",
      "velociraptor",
      "title",
      "mispell",
      "spelling correction\n\nan",
      "proximity search",
      "(distance = 2, substitute",
      "pattern matching\n\nsearching",
      "→ matches"
    ],
    "keyword_string": "helps lec4 tire distance tip edit distance antique tix allowing errors correction tin spelling correction using n prefixes enrapture for retrieve pattern lcs edit distance\n\nn grams measures example , delete misspell finds anti comparing longest common subsequence using (distance = 3, add rapt antibody fix spelling edit substrings computed simple patterns levenshtein distance ranges suffixes spelling queries\n\nreal velociraptor title mispell spelling correction\n\nan proximity search (distance = 2, substitute pattern matching\n\nsearching → matches",
    "token_count": 474,
    "word_count": 262,
    "sentence_count": 10,
    "paragraph_count": 29,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5527426160337553,
    "avg_sentence_length": 26.2,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 128,
    "document_hash": "7339e35ff387",
    "content": "Google Search Operators (Conceptual Table Explanation)\n\nGoogle Search Operators are special commands or symbols that can be added to your search queries to refine and specify the search results. They allow users to search beyond simple keywords, enabling more powerful and targeted information retrieval.\n\nHere are some common operators and their uses:\n\nallinanchor:\n\nUse: Restricts search to words found in the link text on web pages. Useful for finding pages linked by specific terms.\n\nUsage Syntax: allinanchor:keyword1 keyword2 (for multiple keywords).\n\nallintext:\n\nUse: Restricts search to words found only in the body text of web pages, excluding titles, URLs, etc.\n\nUsage Syntax: allintext:keyword1 keyword2 (for multiple keywords).\n\nallintitle:\n\nUse: Restricts search to words found only in the titles of web pages.\n\nUsage Syntax: allintitle:keyword1 keyword2 (for multiple keywords).\n\nallinurl:\n\nUse: Restricts search to words found only in the web page addresses (URLs).\n\nUsage Syntax: allinurl:keyword1 keyword2 (for multiple keywords).\n\nfiletype:\n\nUse: Restricts search to files of a specified type, such as PDF documents, Word files, etc.\n\nUsage Syntax: filetype:extension (e.g., filetype:pdf).\n\ninanchor:\n\nUse: Similar to allinanchor, but allows for single keywords or more complex queries. Restricts search to words in the link text.\n\nUsage Syntax: inanchor:keyword.\n\nintext:\n\nUse: Similar to allintext, but allows for single keywords or more complex queries. Restricts search to words in the body text.\n\nUsage Syntax: intext:keyword.\n\nintitle:\n\nUse: Similar to allintitle, but for single keywords or more complex queries. Restricts search to words in the titles only.\n\nUsage Syntax: intitle:keyword.\n\ninurl:\n\nUse: Similar to allinurl, but for single keywords or more complex queries. Restricts search to words in web page addresses.\n\nUsage Syntax: inurl:keyword.\n\nsite:\n\nUse: Restricts search to a specific domain or website.\n\nUsage Syntax: site:domain (e.g., site:wikipedia.org).\n\nMore at: https://www.google.com/advanced_search (This URL leads to Google's advanced search interface, which often provides guidance on using these and other operators).",
    "enhanced_text": "[NLP] Google Search Operators (Conceptual Table Explanation)\n\nGoogle Search Operators are special commands or symbols that can be added to your search queries to refine and specify the search results. They allow users to search beyond simple keywords, enabling more powerful and targeted information retrieval.\n\nHere are some common operators and their uses:\n\nallinanchor:\n\nUse: Restricts search to words found in the link text on web pages. Useful for finding pages linked by specific terms.\n\nUsage Syntax: allinanchor:keyword1 keyword2 (for multiple keywords).\n\nallintext:\n\nUse: Restricts search to words found only in the body text of web pages, excluding titles, URLs, etc.\n\nUsage Syntax: allintext:keyword1 keyword2 (for multiple keywords).\n\nallintitle:\n\nUse: Restricts search to words found only in the titles of web pages.\n\nUsage Syntax: allintitle:keyword1 keyword2 (for multiple keywords).\n\nallinurl:\n\nUse: Restricts search to words found only in the web page addresses (URLs).\n\nUsage Syntax: allinurl:keyword1 keyword2 (for multiple keywords).\n\nfiletype:\n\nUse: Restricts search to files of a specified type, such as PDF documents, Word files, etc.\n\nUsage Syntax: filetype:extension (e.g., filetype:pdf).\n\ninanchor:\n\nUse: Similar to allinanchor, but allows for single keywords or more complex queries. Restricts search to words in the link text.\n\nUsage Syntax: inanchor:keyword.\n\nintext:\n\nUse: Similar to allintext, but allows for single keywords or more complex queries. Restricts search to words in the body text.\n\nUsage Syntax: intext:keyword.\n\nintitle:\n\nUse: Similar to allintitle, but for single keywords or more complex queries. Restricts search to words in the titles only.\n\nUsage Syntax: intitle:keyword.\n\ninurl:\n\nUse: Similar to allinurl, but for single keywords or more complex queries. Restricts search to words in web page addresses.\n\nUsage Syntax: inurl:keyword.\n\nsite:\n\nUse: Restricts search to a specific domain or website.\n\nUsage Syntax: site:domain (e.g., site:wikipedia.org).\n\nMore at: https://www.google.com/advanced_search (This URL leads to Google's advanced search interface, which often provides guidance on using these and other operators).",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec4-(h) Google Search Operators.txt",
    "file_name": "lec4-(h) Google Search Operators.txt",
    "filename_keywords": [
      "lec4",
      "search",
      "google",
      "operators"
    ],
    "content_keywords": [
      "restricts",
      "they",
      "similar",
      "pdf",
      "more",
      "useful",
      "this url",
      "here",
      "url",
      "word",
      "google search operators",
      "conceptual table explanation",
      "usage syntax",
      "urls",
      "use",
      "google"
    ],
    "technical_terms": [
      "restricts",
      "they",
      "similar",
      "pdf",
      "more",
      "useful",
      "this url",
      "here",
      "url",
      "word",
      "google search operators",
      "conceptual table explanation",
      "usage syntax",
      "urls",
      "use",
      "google"
    ],
    "all_keywords": [
      "lec4",
      "google search operators",
      "similar",
      "pdf",
      "more",
      "search",
      "this url",
      "url",
      "conceptual table explanation",
      "urls",
      "google",
      "restricts",
      "they",
      "useful",
      "word",
      "operators",
      "here",
      "usage syntax",
      "use"
    ],
    "keyword_string": "lec4 google search operators similar pdf more search this url url conceptual table explanation urls google restricts they useful word operators here usage syntax use",
    "token_count": 546,
    "word_count": 305,
    "sentence_count": 28,
    "paragraph_count": 34,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5586080586080586,
    "avg_sentence_length": 10.892857142857142,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 129,
    "document_hash": "70dca68d261a",
    "content": "Low-Level Information Extraction (IE)\n\nLow-level IE refers to the automatic detection and extraction of structured information—like dates, names, places, events—from unstructured text (e.g., emails, web pages).\n\nIt’s foundational in many modern applications that need to understand text content and provide actionable insights or direct answers.\n\nKey Characteristics\n\nWorks on raw text data without requiring manual tagging.\n\nExtracts named entities (dates, locations, organizations, events, phone numbers, etc.).\n\nEnables contextual understanding and integration with other tools (calendars, maps, knowledge graphs).\n\nEnhances user experience by offering relevant, context-aware suggestions or direct answers.\n\nExample 1: Email Event Extraction (Apple/Google Mail)\n\nEmail client scans your message for date/time expressions and event-related phrases.\n\nDetects entities like \"Friday January 6, 2012\".\n\nRecognizes this as an event date and links it to calendar functionality.\n\nPop-up menu offers actions:\n\nCreate New Calendar Event\n\nShow Date in Calendar\n\nCopy Date\n\nThis saves user time by transforming unstructured text into actionable data automatically.\n\nExample 2: Google Search Entity Extraction\n\nWhen you search for \"bhp billiton headquarters\", Google’s system:\n\nExtracts the entity “BHP Billiton Ltd.” and the attribute “Headquarters”.\n\nIdentifies relevant structured facts (Melbourne, London).\n\nDisplays a direct answer box at the top of results (sometimes called a knowledge panel or featured snippet).\n\nHighlights extracted entities in search snippets.\n\nThis improves search relevancy and user satisfaction by reducing need to sift through multiple links.\n\nUnderlying Techniques (brief)\n\nNamed Entity Recognition (NER): Finds entities like dates, people, places.\n\nRelation Extraction: Links entities with their attributes or relations (e.g., company → headquarters).\n\nParsing & Contextual Analysis: Understands sentence structure to correctly identify entities.\n\nKnowledge Graphs: Connect extracted entities to a large graph of known facts.\n\nPattern Matching & Rules: For common formats like dates and times.\n\nMachine Learning / Deep Learning Models: To improve accuracy and generalization.\n\nWhy Is Low-Level IE Important?\n\nIt bridges the gap between unstructured text and structured data.\n\nPowers personal assistants, smart email clients, and intelligent search engines.\n\nAutomates tedious manual data entry and enhances productivity.\n\nEnables context-aware applications that proactively assist users.",
    "enhanced_text": "[NLP] Low-Level Information Extraction (IE)\n\nLow-level IE refers to the automatic detection and extraction of structured information—like dates, names, places, events—from unstructured text (e.g., emails, web pages).\n\nIt’s foundational in many modern applications that need to understand text content and provide actionable insights or direct answers.\n\nKey Characteristics\n\nWorks on raw text data without requiring manual tagging.\n\nExtracts named entities (dates, locations, organizations, events, phone numbers, etc.).\n\nEnables contextual understanding and integration with other tools (calendars, maps, knowledge graphs).\n\nEnhances user experience by offering relevant, context-aware suggestions or direct answers.\n\nExample 1: Email Event Extraction (Apple/Google Mail)\n\nEmail client scans your message for date/time expressions and event-related phrases.\n\nDetects entities like \"Friday January 6, 2012\".\n\nRecognizes this as an event date and links it to calendar functionality.\n\nPop-up menu offers actions:\n\nCreate New Calendar Event\n\nShow Date in Calendar\n\nCopy Date\n\nThis saves user time by transforming unstructured text into actionable data automatically.\n\nExample 2: Google Search Entity Extraction\n\nWhen you search for \"bhp billiton headquarters\", Google’s system:\n\nExtracts the entity “BHP Billiton Ltd.” and the attribute “Headquarters”.\n\nIdentifies relevant structured facts (Melbourne, London).\n\nDisplays a direct answer box at the top of results (sometimes called a knowledge panel or featured snippet).\n\nHighlights extracted entities in search snippets.\n\nThis improves search relevancy and user satisfaction by reducing need to sift through multiple links.\n\nUnderlying Techniques (brief)\n\nNamed Entity Recognition (NER): Finds entities like dates, people, places.\n\nRelation Extraction: Links entities with their attributes or relations (e.g., company → headquarters).\n\nParsing & Contextual Analysis: Understands sentence structure to correctly identify entities.\n\nKnowledge Graphs: Connect extracted entities to a large graph of known facts.\n\nPattern Matching & Rules: For common formats like dates and times.\n\nMachine Learning / Deep Learning Models: To improve accuracy and generalization.\n\nWhy Is Low-Level IE Important?\n\nIt bridges the gap between unstructured text and structured data.\n\nPowers personal assistants, smart email clients, and intelligent search engines.\n\nAutomates tedious manual data entry and enhances productivity.\n\nEnables context-aware applications that proactively assist users.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec5-(a) Low-level Information Extraction.txt",
    "file_name": "lec5-(a) Low-level Information Extraction.txt",
    "filename_keywords": [
      "information",
      "level",
      "lec5",
      "extraction",
      "low"
    ],
    "content_keywords": [
      "underlying techniques",
      "london",
      "level ie important",
      "extracts",
      "identifies",
      "friday january",
      "highlights",
      "detects",
      "calendar\n\ncopy date\n\nthis",
      "recognizes",
      "google mail",
      "for",
      "level information extraction",
      "parsing",
      "connect",
      "rules",
      "powers",
      "example",
      "create new calendar event\n\nshow date",
      "contextual analysis",
      "machine learning",
      "finds",
      "google search entity extraction\n\nwhen",
      "google",
      "ner",
      "key characteristics\n\nworks",
      "bhp billiton headquarters",
      "friday january 6, 2012",
      "pattern matching",
      "bhp",
      "this",
      "relation extraction",
      "deep learning models",
      "bhp billiton ltd",
      "low",
      "links",
      "headquarters",
      "email",
      "pop",
      "named entity recognition",
      "enhances",
      "enables",
      "email event extraction",
      "automates",
      "melbourne",
      "knowledge graphs",
      "displays",
      "understands",
      "apple",
      "why is low"
    ],
    "technical_terms": [
      "underlying techniques",
      "london",
      "level ie important",
      "extracts",
      "identifies",
      "friday january",
      "highlights",
      "detects",
      "calendar\n\ncopy date\n\nthis",
      "recognizes",
      "google mail",
      "for",
      "level information extraction",
      "parsing",
      "connect",
      "rules",
      "powers",
      "example",
      "create new calendar event\n\nshow date",
      "contextual analysis",
      "machine learning",
      "finds",
      "google search entity extraction\n\nwhen",
      "google",
      "ner",
      "key characteristics\n\nworks",
      "pattern matching",
      "bhp",
      "this",
      "relation extraction",
      "deep learning models",
      "bhp billiton ltd",
      "low",
      "links",
      "headquarters",
      "email",
      "pop",
      "named entity recognition",
      "enhances",
      "enables",
      "email event extraction",
      "automates",
      "melbourne",
      "knowledge graphs",
      "displays",
      "understands",
      "apple",
      "why is low"
    ],
    "all_keywords": [
      "information",
      "underlying techniques",
      "london",
      "level ie important",
      "extracts",
      "identifies",
      "friday january",
      "highlights",
      "detects",
      "calendar\n\ncopy date\n\nthis",
      "recognizes",
      "google mail",
      "for",
      "level information extraction",
      "parsing",
      "connect",
      "rules",
      "powers",
      "example",
      "create new calendar event\n\nshow date",
      "understands",
      "contextual analysis",
      "machine learning",
      "finds",
      "google search entity extraction\n\nwhen",
      "google",
      "ner",
      "key characteristics\n\nworks",
      "lec5",
      "bhp billiton headquarters",
      "friday january 6, 2012",
      "pattern matching",
      "bhp",
      "this",
      "relation extraction",
      "deep learning models",
      "bhp billiton ltd",
      "low",
      "links",
      "headquarters",
      "level",
      "email",
      "pop",
      "named entity recognition",
      "enhances",
      "enables",
      "email event extraction",
      "automates",
      "melbourne",
      "knowledge graphs",
      "displays",
      "extraction",
      "apple",
      "why is low"
    ],
    "keyword_string": "information underlying techniques london level ie important extracts identifies friday january highlights detects calendar\n\ncopy date\n\nthis recognizes google mail for level information extraction parsing connect rules powers example create new calendar event\n\nshow date understands contextual analysis machine learning finds google search entity extraction\n\nwhen google ner key characteristics\n\nworks lec5 bhp billiton headquarters friday january 6, 2012 pattern matching bhp this relation extraction deep learning models bhp billiton ltd low links headquarters level email pop named entity recognition enhances enables email event extraction automates melbourne knowledge graphs displays extraction apple why is low",
    "token_count": 487,
    "word_count": 333,
    "sentence_count": 26,
    "paragraph_count": 36,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6837782340862423,
    "avg_sentence_length": 12.807692307692308,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 130,
    "document_hash": "696468244619",
    "content": "Named Entity Recognition (NER) — Detailed Overview\n\nWhat is NER?\n\nNER is a fundamental sub-task of Information Extraction (IE) that identifies and classifies proper names (entities) in text.\n\nEntities are classified into predefined categories like person, location, organization, and others.\n\nIt does not identify events or relations — only entities themselves.\n\nIt’s more than just matching against lists — it involves linguistic and contextual understanding.\n\nCommon Categories in NER\n\nPerson: Names of people (e.g., Andrew Wilkie)\n\nOrganization: Companies, parties, institutions (e.g., Labor, Greens)\n\nLocation: Geographic names (cities, countries)\n\nDate/Time: Dates, years (e.g., 2010)\n\nOthers: Measures, email addresses, domain-specific entities like drug names, ships, etc",
    "enhanced_text": "[NLP] Named Entity Recognition (NER) — Detailed Overview\n\nWhat is NER?\n\nNER is a fundamental sub-task of Information Extraction (IE) that identifies and classifies proper names (entities) in text.\n\nEntities are classified into predefined categories like person, location, organization, and others.\n\nIt does not identify events or relations — only entities themselves.\n\nIt’s more than just matching against lists — it involves linguistic and contextual understanding.\n\nCommon Categories in NER\n\nPerson: Names of people (e.g., Andrew Wilkie)\n\nOrganization: Companies, parties, institutions (e.g., Labor, Greens)\n\nLocation: Geographic names (cities, countries)\n\nDate/Time: Dates, years (e.g., 2010)\n\nOthers: Measures, email addresses, domain-specific entities like drug names, ships, etc",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec5-(b) Named Entity Recognition (NER).txt",
    "file_name": "lec5-(b) Named Entity Recognition (NER).txt",
    "filename_keywords": [
      "entity",
      "recognition",
      "ner",
      "lec5",
      "named"
    ],
    "content_keywords": [
      "others",
      "location",
      "detailed overview\n\nwhat",
      "ner\n\nperson",
      "time",
      "dates",
      "measures",
      "information extraction",
      "greens",
      "date",
      "ner",
      "andrew wilkie",
      "common categories",
      "companies",
      "labor",
      "geographic",
      "entities",
      "named entity recognition",
      "names",
      "organization"
    ],
    "technical_terms": [
      "others",
      "location",
      "detailed overview\n\nwhat",
      "ner\n\nperson",
      "time",
      "dates",
      "measures",
      "information extraction",
      "greens",
      "date",
      "ner",
      "andrew wilkie",
      "common categories",
      "companies",
      "labor",
      "geographic",
      "entities",
      "named entity recognition",
      "names",
      "organization"
    ],
    "all_keywords": [
      "others",
      "recognition",
      "location",
      "named",
      "detailed overview\n\nwhat",
      "ner\n\nperson",
      "time",
      "dates",
      "measures",
      "information extraction",
      "greens",
      "date",
      "ner",
      "lec5",
      "andrew wilkie",
      "common categories",
      "companies",
      "labor",
      "entity",
      "geographic",
      "entities",
      "named entity recognition",
      "names",
      "organization"
    ],
    "keyword_string": "others recognition location named detailed overview\n\nwhat ner\n\nperson time dates measures information extraction greens date ner lec5 andrew wilkie common categories companies labor entity geographic entities named entity recognition names organization",
    "token_count": 170,
    "word_count": 104,
    "sentence_count": 6,
    "paragraph_count": 12,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.611764705882353,
    "avg_sentence_length": 17.333333333333332,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 131,
    "document_hash": "977195eb674b",
    "content": "What NER is NOT\n\nNot event recognition (doesn't identify events or actions).\n\nDoes not perform co-reference (linking different mentions of the same entity).\n\nNot just dictionary or list lookups — needs to handle variants and ambiguity.",
    "enhanced_text": "[NLP] What NER is NOT\n\nNot event recognition (doesn't identify events or actions).\n\nDoes not perform co-reference (linking different mentions of the same entity).\n\nNot just dictionary or list lookups — needs to handle variants and ambiguity.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec5-(c) NE is NOT.txt",
    "file_name": "lec5-(c) NE is NOT.txt",
    "filename_keywords": [
      "lec5",
      "not"
    ],
    "content_keywords": [
      "not\n\nnot",
      "does",
      "ner",
      "not",
      "what ner"
    ],
    "technical_terms": [
      "not\n\nnot",
      "does",
      "ner",
      "not",
      "what ner"
    ],
    "all_keywords": [
      "not\n\nnot",
      "does",
      "ner",
      "lec5",
      "not",
      "what ner"
    ],
    "keyword_string": "not\n\nnot does ner lec5 not what ner",
    "token_count": 49,
    "word_count": 36,
    "sentence_count": 3,
    "paragraph_count": 4,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7346938775510204,
    "avg_sentence_length": 12.0,
    "readability_score": 100,
    "has_technical_terms": false,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "General"
  },
  {
    "document_id": 132,
    "document_hash": "c3be48170de4",
    "content": "Challenges in NER\n\nAmbiguity: Same word/entity may refer to different categories depending on context.\n\nExample: Washington (person or location)\n\nMay (person or month)\n\nMetonymy (one entity name used for related things):\n\n\"The ham sandwich wants his bill\" (person, not food)\n\n\"West Indies won the World Cup\" (team = organization vs. place)\n\nEntity variation: Different forms of the same entity (John Smith, J. Smith)\n\nBoundary detection: Correctly determining start/end of multi-word entities.\n\nDomain and genre variability: Formal vs. informal text, punctuation, formatting affect detection.",
    "enhanced_text": "[NLP] Challenges in NER\n\nAmbiguity: Same word/entity may refer to different categories depending on context.\n\nExample: Washington (person or location)\n\nMay (person or month)\n\nMetonymy (one entity name used for related things):\n\n\"The ham sandwich wants his bill\" (person, not food)\n\n\"West Indies won the World Cup\" (team = organization vs. place)\n\nEntity variation: Different forms of the same entity (John Smith, J. Smith)\n\nBoundary detection: Correctly determining start/end of multi-word entities.\n\nDomain and genre variability: Formal vs. informal text, punctuation, formatting affect detection.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec5-(d) Challenges in NER.txt",
    "file_name": "lec5-(d) Challenges in NER.txt",
    "filename_keywords": [
      "lec5",
      "challenges",
      "ner"
    ],
    "content_keywords": [
      "boundary",
      "john smith",
      "same",
      "domain",
      "may",
      "example",
      "correctly",
      "smith",
      "washington",
      "metonymy",
      "ner",
      "different",
      "world cup",
      "(person, not food)",
      "challenges",
      "entity",
      "west indies",
      "the",
      "ner\n\nambiguity",
      "formal"
    ],
    "technical_terms": [
      "boundary",
      "john smith",
      "same",
      "domain",
      "may",
      "example",
      "correctly",
      "smith",
      "washington",
      "metonymy",
      "ner",
      "different",
      "world cup",
      "challenges",
      "entity",
      "west indies",
      "the",
      "ner\n\nambiguity",
      "formal"
    ],
    "all_keywords": [
      "boundary",
      "john smith",
      "same",
      "domain",
      "may",
      "example",
      "correctly",
      "smith",
      "washington",
      "ner",
      "metonymy",
      "lec5",
      "different",
      "world cup",
      "(person, not food)",
      "challenges",
      "entity",
      "west indies",
      "the",
      "ner\n\nambiguity",
      "formal"
    ],
    "keyword_string": "boundary john smith same domain may example correctly smith washington ner metonymy lec5 different world cup (person, not food) challenges entity west indies the ner\n\nambiguity formal",
    "token_count": 127,
    "word_count": 83,
    "sentence_count": 3,
    "paragraph_count": 10,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6535433070866141,
    "avg_sentence_length": 27.666666666666668,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 133,
    "document_hash": "a6c247ec5955",
    "content": "Approaches to NER\n\nList Lookup (Gazetteers)\n\nPros: Simple, fast, language-independent.\n\nCons: Limited to known entities, no handling of variants or ambiguity.\n\nShallow Parsing + Contextual Rules\n\nUses linguistic clues: capitalization, internal word structure, and local context patterns.\n\nExamples:\n\nCapitalized word followed by “Forest” or “Street” → likely location.\n\nContext phrases like “based in [Location]”.\n\nChallenges: Ambiguous capitalization, semantic ambiguity, structural ambiguity.\n\nStatistical and ML-based Approaches\n\nModels like CRF, HMM, or deep learning models (LSTM, Transformer-based).\n\nLearn from annotated corpora, use context and word features.",
    "enhanced_text": "[NLP] Approaches to NER\n\nList Lookup (Gazetteers)\n\nPros: Simple, fast, language-independent.\n\nCons: Limited to known entities, no handling of variants or ambiguity.\n\nShallow Parsing + Contextual Rules\n\nUses linguistic clues: capitalization, internal word structure, and local context patterns.\n\nExamples:\n\nCapitalized word followed by “Forest” or “Street” → likely location.\n\nContext phrases like “based in [Location]”.\n\nChallenges: Ambiguous capitalization, semantic ambiguity, structural ambiguity.\n\nStatistical and ML-based Approaches\n\nModels like CRF, HMM, or deep learning models (LSTM, Transformer-based).\n\nLearn from annotated corpora, use context and word features.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec5-(e) Approaches to NER.txt",
    "file_name": "lec5-(e) Approaches to NER.txt",
    "filename_keywords": [
      "lec5",
      "ner",
      "approaches"
    ],
    "content_keywords": [
      "shallow parsing",
      "examples",
      "location",
      "simple",
      "gazetteers",
      "hmm",
      "learn",
      "ambiguous",
      "pros",
      "street",
      "ner",
      "transformer",
      "forest",
      "crf",
      "ner\n\nlist lookup",
      "contextual rules\n\nuses",
      "limited",
      "statistical",
      "challenges",
      "approaches\n\nmodels",
      "lstm",
      "context",
      "cons",
      "approaches",
      "capitalized"
    ],
    "technical_terms": [
      "shallow parsing",
      "examples",
      "location",
      "simple",
      "gazetteers",
      "hmm",
      "learn",
      "ambiguous",
      "pros",
      "street",
      "ner",
      "transformer",
      "forest",
      "crf",
      "ner\n\nlist lookup",
      "contextual rules\n\nuses",
      "limited",
      "statistical",
      "challenges",
      "approaches\n\nmodels",
      "lstm",
      "context",
      "cons",
      "approaches",
      "capitalized"
    ],
    "all_keywords": [
      "shallow parsing",
      "examples",
      "location",
      "simple",
      "gazetteers",
      "hmm",
      "learn",
      "ambiguous",
      "pros",
      "street",
      "ner",
      "transformer",
      "lec5",
      "forest",
      "crf",
      "ner\n\nlist lookup",
      "contextual rules\n\nuses",
      "limited",
      "statistical",
      "challenges",
      "approaches\n\nmodels",
      "lstm",
      "context",
      "cons",
      "approaches",
      "capitalized"
    ],
    "keyword_string": "shallow parsing examples location simple gazetteers hmm learn ambiguous pros street ner transformer lec5 forest crf ner\n\nlist lookup contextual rules\n\nuses limited statistical challenges approaches\n\nmodels lstm context cons approaches capitalized",
    "token_count": 143,
    "word_count": 84,
    "sentence_count": 8,
    "paragraph_count": 13,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5874125874125874,
    "avg_sentence_length": 10.5,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 134,
    "document_hash": "b9227dd16683",
    "content": "Example NER Output (BIO Tagging)\n\nText:\n\nForeign Ministry spokesman Shen Guofang told Reuters...\n\nAnnotated tokens:\n\nForeign (ORG)\n\nMinistry (ORG)\n\nspokesman (O)\n\nShen (PER)\n\nGuofang (PER)\n\ntold (O)\n\nReuters (ORG)",
    "enhanced_text": "[NLP] Example NER Output (BIO Tagging)\n\nText:\n\nForeign Ministry spokesman Shen Guofang told Reuters...\n\nAnnotated tokens:\n\nForeign (ORG)\n\nMinistry (ORG)\n\nspokesman (O)\n\nShen (PER)\n\nGuofang (PER)\n\ntold (O)\n\nReuters (ORG)",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec5-(f) Example NER.txt",
    "file_name": "lec5-(f) Example NER.txt",
    "filename_keywords": [
      "lec5",
      "ner",
      "example"
    ],
    "content_keywords": [
      "example ner output",
      "foreign",
      "ner",
      "text",
      "bio",
      "per",
      "bio tagging",
      "shen guofang",
      "foreign ministry",
      "reuters",
      "ministry",
      "org",
      "guofang",
      "annotated",
      "shen"
    ],
    "technical_terms": [
      "example ner output",
      "foreign",
      "ner",
      "text",
      "bio",
      "per",
      "bio tagging",
      "shen guofang",
      "foreign ministry",
      "reuters",
      "ministry",
      "org",
      "guofang",
      "annotated",
      "shen"
    ],
    "all_keywords": [
      "example ner output",
      "foreign",
      "ner",
      "text",
      "bio",
      "lec5",
      "per",
      "bio tagging",
      "shen guofang",
      "example",
      "foreign ministry",
      "reuters",
      "ministry",
      "org",
      "guofang",
      "annotated",
      "shen"
    ],
    "keyword_string": "example ner output foreign ner text bio lec5 per bio tagging shen guofang example foreign ministry reuters ministry org guofang annotated shen",
    "token_count": 59,
    "word_count": 29,
    "sentence_count": 1,
    "paragraph_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.4915254237288136,
    "avg_sentence_length": 29.0,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 135,
    "document_hash": "0e43a69eb941",
    "content": "Evaluation Metrics for NER\n\nPrecision: Correct entities found / all entities found\n\nRecall: Correct entities found / all entities that should be found\n\nF1 Score: Harmonic mean of precision and recall\n\nBoundary errors matter a lot:\n\nPartial matches count as false positives and false negatives.\n\nThis makes evaluation more complex compared to other IR tasks.",
    "enhanced_text": "[NLP] Evaluation Metrics for NER\n\nPrecision: Correct entities found / all entities found\n\nRecall: Correct entities found / all entities that should be found\n\nF1 Score: Harmonic mean of precision and recall\n\nBoundary errors matter a lot:\n\nPartial matches count as false positives and false negatives.\n\nThis makes evaluation more complex compared to other IR tasks.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec5-(g) Evaluation Metrics for NER.txt",
    "file_name": "lec5-(g) Evaluation Metrics for NER.txt",
    "filename_keywords": [
      "lec5",
      "metrics",
      "ner",
      "evaluation"
    ],
    "content_keywords": [
      "boundary",
      "ner",
      "recall",
      "correct",
      "score",
      "ner\n\nprecision",
      "partial",
      "evaluation metrics",
      "this",
      "harmonic"
    ],
    "technical_terms": [
      "boundary",
      "ner",
      "recall",
      "correct",
      "score",
      "ner\n\nprecision",
      "partial",
      "evaluation metrics",
      "this",
      "harmonic"
    ],
    "all_keywords": [
      "boundary",
      "ner",
      "evaluation",
      "recall",
      "correct",
      "lec5",
      "score",
      "ner\n\nprecision",
      "partial",
      "evaluation metrics",
      "metrics",
      "this",
      "harmonic"
    ],
    "keyword_string": "boundary ner evaluation recall correct lec5 score ner\n\nprecision partial evaluation metrics metrics this harmonic",
    "token_count": 65,
    "word_count": 55,
    "sentence_count": 2,
    "paragraph_count": 7,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.8461538461538461,
    "avg_sentence_length": 27.5,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 136,
    "document_hash": "310df5f67bd2",
    "content": "Applications of NER\n\nIndexing and linking entities in documents.\n\nSentiment analysis focused on companies/products.\n\nPopulating knowledge bases and relations.\n\nImproving search engines and question answering.\n\nEnhancing features like smart email replies, calendar event extraction.",
    "enhanced_text": "[NLP] Applications of NER\n\nIndexing and linking entities in documents.\n\nSentiment analysis focused on companies/products.\n\nPopulating knowledge bases and relations.\n\nImproving search engines and question answering.\n\nEnhancing features like smart email replies, calendar event extraction.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec5-(h) Applications of NER.txt",
    "file_name": "lec5-(h) Applications of NER.txt",
    "filename_keywords": [
      "lec5",
      "ner",
      "applications"
    ],
    "content_keywords": [
      "ner",
      "populating",
      "applications",
      "improving",
      "sentiment",
      "ner\n\nindexing",
      "enhancing"
    ],
    "technical_terms": [
      "ner",
      "populating",
      "applications",
      "improving",
      "sentiment",
      "ner\n\nindexing",
      "enhancing"
    ],
    "all_keywords": [
      "ner",
      "lec5",
      "applications",
      "populating",
      "improving",
      "sentiment",
      "ner\n\nindexing",
      "enhancing"
    ],
    "keyword_string": "ner lec5 applications populating improving sentiment ner\n\nindexing enhancing",
    "token_count": 45,
    "word_count": 34,
    "sentence_count": 5,
    "paragraph_count": 6,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.7555555555555555,
    "avg_sentence_length": 6.8,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 137,
    "document_hash": "eec0cfeb74fe",
    "content": "Similarity Measure — Overview\n\nA similarity measure is a mathematical function or algorithm that quantifies how alike two objects are. In the context of information retrieval (IR) and natural language processing (NLP), these objects are often represented as vectors (lists of numbers). For example, a user's search query and a document from a collection can both be transformed into vector representations, and a similarity measure can then be used to determine how closely related they are.\n\n________________________________________\n\nWhy use similarity measures?\n\nSimilarity measures are crucial in various applications for several reasons:\n\n* To rank documents: They allow systems to order documents based on how relevant they are to a given query. Documents with higher similarity scores to the query are ranked higher.\n\n* To filter results: A similarity threshold can be set, so only documents exceeding that level of similarity to a query or another document are considered or returned.\n\n* To improve search accuracy: By providing a quantitative way to assess relevance, similarity measures help in delivering more accurate and pertinent search results.\n\n* For clustering: Grouping similar items together.\n\n* For recommendations: Suggesting items similar to what a user has liked or viewed.\n\n* For anomaly detection: Identifying items that are dissimilar to the norm.\n\n________________________________________\n\nCommon Similarity Measures\n\n1. Jaccard Similarity\n\n* Measures the overlap between two sets of items. It is calculated as the size of the intersection of the sets divided by the size of their union.\n\n* Formula:\n\n Jaccard(Set_A, Set_B) = |Set_A intersect Set_B| / |Set_A union Set_B|\n\n Where:\n\n `|Set_A intersect Set_B|` is the number of elements common to both Set_A and Set_B.\n\n `|Set_A union Set_B|` is the total number of unique elements in either Set_A or Set_B.\n\n* Useful for comparing sets of words (e.g., unique keywords in documents, items in user profiles).\n\n* Range: 0 (indicating no overlap between the sets) to 1 (indicating the sets are identical).\n\n* Example:\n\n Doc1 words = {apple, orange, banana}\n\n Doc2 words = {apple, banana, kiwi}\n\n Intersection (Doc1, Doc2) = {apple, banana} -> Size = 2\n\n Union (Doc1, Doc2) = {apple, orange, banana, kiwi} -> Size = 4\n\n Jaccard Similarity = 2 / 4 = 0.5\n\n________________________________________\n\n2. Cosine Similarity\n\n* Measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. It focuses on the orientation of the vectors, not their magnitude.\n\n* Formula:\n\n Cosine_Similarity(Vector_A, Vector_B) = (Vector_A . Vector_B) / (||Vector_A|| * ||Vector_B||)\n\n Where:\n\n `Vector_A . Vector_B` is the dot product of Vector_A and Vector_B.\n\n `||Vector_A||` is the magnitude (or Euclidean norm/length) of Vector_A.\n\n `||Vector_B||` is the magnitude of Vector_B.\n\n* Values range from -1 (exactly opposite) to 1 (exactly the same direction). For vectors with non-negative components (like TF-IDF scores or word counts), the range is typically 0 (orthogonal, no similarity) to 1 (same direction, maximal similarity).\n\n* Popular in text similarity tasks, especially with TF-IDF vectors or word embedding vectors.\n\n* Example:\n\n Vector A = [1, 2, 3]\n\n Vector B = [2, 3, 4]\n\n Dot Product (A . B) = (1*2) + (2*3) + (3*4) = 2 + 6 + 12 = 20\n\n Magnitude of A (||A||) = sqrt(1^2 + 2^2 + 3^2) = sqrt(1 + 4 + 9) = sqrt(14)\n\n Magnitude of B (||B||) = sqrt(2^2 + 3^2 + 4^2) = sqrt(4 + 9 + 16) = sqrt(29)\n\n Cosine Similarity = 20 / (sqrt(14) * sqrt(29))\n\n = 20 / (approx 3.742 * approx 5.385)\n\n = 20 / (approx 20.15)\n\n which is approximately 0.992\n\n________________________________________\n\n3. Euclidean Distance\n\n* Measures the straight-line distance (or \"as the crow flies\" distance) between two points (or vectors) in Euclidean space.\n\n* Formula:\n\n Euclidean_Distance(Vector_A, Vector_B) = sqrt( sum_for_each_dimension_i ( (A_i - B_i)^2 ) )\n\n Where A_i and B_i are the components of vectors A and B in the i-th dimension.\n\n* A lower distance indicates higher similarity (i.e., the points are closer). A distance of 0 means the points are identical.\n\n* Often used in clustering algorithms (like K-means) and for comparing vectors with continuous-valued features. To use it as a similarity measure (where higher is better), it often needs to be transformed (e.g., 1 / (1 + distance) or exp(-distance)).\n\n________________________________________\n\n4. Edit Distance (Specifically Levenshtein Distance)\n\n* Measures the similarity between two strings by counting the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.\n\n* Useful for comparing short texts, names, identifying spelling errors, or in bioinformatics for DNA sequences.\n\n* A lower edit distance means the strings are more similar. An edit distance of 0 means the strings are identical.\n\n* Example:\n\n To transform \"kitten\" into \"sitting\":\n\n 1. k -> s (substitution: \"sitten\")\n\n 2. e -> i (substitution: \"sittin\")\n\n 3. -> g (insertion: \"sitting\")\n\n The Levenshtein distance is 3.\n\n________________________________________\n\n5. Word Embedding Similarity\n\n* Compares word or document vectors that are generated from word embedding models like Word2Vec, GloVe, FastText, or sentence/document embedding models like Sentence-BERT.\n\n* These models learn dense vector representations where words or texts with similar meanings are located close to each other in the vector space.\n\n* The similarity between these embedding vectors is usually measured using Cosine Similarity.\n\n* This approach captures semantic similarity (meaning-based similarity) that goes beyond simple surface-level word overlap. For example, \"king\" and \"queen\" would have high similarity.\n\n________________________________________\n\n6. Semantic Similarity Measures\n\n* A broader category of measures that aim to capture the similarity in meaning or semantic content between pieces of text, rather than just lexical overlap.\n\n* Example: Two sentences that use different words but convey a similar idea can achieve a high semantic similarity score.\n\n* Methods include:\n\n * Knowledge-based measures: Utilizing structured knowledge bases like WordNet (which groups words into sets of synonyms called synsets and defines relationships between them).\n\n * Corpus-based measures: Statistical methods derived from large text corpora, including distributional similarity (words appearing in similar contexts are similar) and techniques like Latent Semantic Analysis (LSA).\n\n * Embedding-based measures: As described in \"Word Embedding Similarity,\" using vectors from models like BERT, ELMo, Universal Sentence Encoder.\n\n * Transformer models: Modern deep learning models like BERT can directly provide similarity scores or embeddings for comparison that capture rich contextual meaning.\n\n________________________________________\n\n7. Topic Modeling-based Similarity\n\n* Uses topic distributions derived from topic models like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF).\n\n* In these models, documents are represented as a mixture of underlying topics, and each topic is a distribution over words.\n\n* So, each document can be represented as a vector where each component is the probability or proportion of a particular topic in that document.\n\n* Similarity between documents is then computed by comparing their topic distributions (e.g., using Cosine Similarity, Jensen-Shannon divergence, or Hellinger distance on the topic vectors).\n\n* Useful for comparing documents at a conceptual or thematic level rather than relying on exact word matches.",
    "enhanced_text": "[NLP] Similarity Measure — Overview\n\nA similarity measure is a mathematical function or algorithm that quantifies how alike two objects are. In the context of information retrieval (IR) and natural language processing (NLP), these objects are often represented as vectors (lists of numbers). For example, a user's search query and a document from a collection can both be transformed into vector representations, and a similarity measure can then be used to determine how closely related they are.\n\n________________________________________\n\nWhy use similarity measures?\n\nSimilarity measures are crucial in various applications for several reasons:\n\n* To rank documents: They allow systems to order documents based on how relevant they are to a given query. Documents with higher similarity scores to the query are ranked higher.\n\n* To filter results: A similarity threshold can be set, so only documents exceeding that level of similarity to a query or another document are considered or returned.\n\n* To improve search accuracy: By providing a quantitative way to assess relevance, similarity measures help in delivering more accurate and pertinent search results.\n\n* For clustering: Grouping similar items together.\n\n* For recommendations: Suggesting items similar to what a user has liked or viewed.\n\n* For anomaly detection: Identifying items that are dissimilar to the norm.\n\n________________________________________\n\nCommon Similarity Measures\n\n1. Jaccard Similarity\n\n* Measures the overlap between two sets of items. It is calculated as the size of the intersection of the sets divided by the size of their union.\n\n* Formula:\n\n Jaccard(Set_A, Set_B) = |Set_A intersect Set_B| / |Set_A union Set_B|\n\n Where:\n\n `|Set_A intersect Set_B|` is the number of elements common to both Set_A and Set_B.\n\n `|Set_A union Set_B|` is the total number of unique elements in either Set_A or Set_B.\n\n* Useful for comparing sets of words (e.g., unique keywords in documents, items in user profiles).\n\n* Range: 0 (indicating no overlap between the sets) to 1 (indicating the sets are identical).\n\n* Example:\n\n Doc1 words = {apple, orange, banana}\n\n Doc2 words = {apple, banana, kiwi}\n\n Intersection (Doc1, Doc2) = {apple, banana} -> Size = 2\n\n Union (Doc1, Doc2) = {apple, orange, banana, kiwi} -> Size = 4\n\n Jaccard Similarity = 2 / 4 = 0.5\n\n________________________________________\n\n2. Cosine Similarity\n\n* Measures the cosine of the angle between two non-zero vectors in a multi-dimensional space. It focuses on the orientation of the vectors, not their magnitude.\n\n* Formula:\n\n Cosine_Similarity(Vector_A, Vector_B) = (Vector_A . Vector_B) / (||Vector_A|| * ||Vector_B||)\n\n Where:\n\n `Vector_A . Vector_B` is the dot product of Vector_A and Vector_B.\n\n `||Vector_A||` is the magnitude (or Euclidean norm/length) of Vector_A.\n\n `||Vector_B||` is the magnitude of Vector_B.\n\n* Values range from -1 (exactly opposite) to 1 (exactly the same direction). For vectors with non-negative components (like TF-IDF scores or word counts), the range is typically 0 (orthogonal, no similarity) to 1 (same direction, maximal similarity).\n\n* Popular in text similarity tasks, especially with TF-IDF vectors or word embedding vectors.\n\n* Example:\n\n Vector A = [1, 2, 3]\n\n Vector B = [2, 3, 4]\n\n Dot Product (A . B) = (1*2) + (2*3) + (3*4) = 2 + 6 + 12 = 20\n\n Magnitude of A (||A||) = sqrt(1^2 + 2^2 + 3^2) = sqrt(1 + 4 + 9) = sqrt(14)\n\n Magnitude of B (||B||) = sqrt(2^2 + 3^2 + 4^2) = sqrt(4 + 9 + 16) = sqrt(29)\n\n Cosine Similarity = 20 / (sqrt(14) * sqrt(29))\n\n = 20 / (approx 3.742 * approx 5.385)\n\n = 20 / (approx 20.15)\n\n which is approximately 0.992\n\n________________________________________\n\n3. Euclidean Distance\n\n* Measures the straight-line distance (or \"as the crow flies\" distance) between two points (or vectors) in Euclidean space.\n\n* Formula:\n\n Euclidean_Distance(Vector_A, Vector_B) = sqrt( sum_for_each_dimension_i ( (A_i - B_i)^2 ) )\n\n Where A_i and B_i are the components of vectors A and B in the i-th dimension.\n\n* A lower distance indicates higher similarity (i.e., the points are closer). A distance of 0 means the points are identical.\n\n* Often used in clustering algorithms (like K-means) and for comparing vectors with continuous-valued features. To use it as a similarity measure (where higher is better), it often needs to be transformed (e.g., 1 / (1 + distance) or exp(-distance)).\n\n________________________________________\n\n4. Edit Distance (Specifically Levenshtein Distance)\n\n* Measures the similarity between two strings by counting the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other.\n\n* Useful for comparing short texts, names, identifying spelling errors, or in bioinformatics for DNA sequences.\n\n* A lower edit distance means the strings are more similar. An edit distance of 0 means the strings are identical.\n\n* Example:\n\n To transform \"kitten\" into \"sitting\":\n\n 1. k -> s (substitution: \"sitten\")\n\n 2. e -> i (substitution: \"sittin\")\n\n 3. -> g (insertion: \"sitting\")\n\n The Levenshtein distance is 3.\n\n________________________________________\n\n5. Word Embedding Similarity\n\n* Compares word or document vectors that are generated from word embedding models like Word2Vec, GloVe, FastText, or sentence/document embedding models like Sentence-BERT.\n\n* These models learn dense vector representations where words or texts with similar meanings are located close to each other in the vector space.\n\n* The similarity between these embedding vectors is usually measured using Cosine Similarity.\n\n* This approach captures semantic similarity (meaning-based similarity) that goes beyond simple surface-level word overlap. For example, \"king\" and \"queen\" would have high similarity.\n\n________________________________________\n\n6. Semantic Similarity Measures\n\n* A broader category of measures that aim to capture the similarity in meaning or semantic content between pieces of text, rather than just lexical overlap.\n\n* Example: Two sentences that use different words but convey a similar idea can achieve a high semantic similarity score.\n\n* Methods include:\n\n * Knowledge-based measures: Utilizing structured knowledge bases like WordNet (which groups words into sets of synonyms called synsets and defines relationships between them).\n\n * Corpus-based measures: Statistical methods derived from large text corpora, including distributional similarity (words appearing in similar contexts are similar) and techniques like Latent Semantic Analysis (LSA).\n\n * Embedding-based measures: As described in \"Word Embedding Similarity,\" using vectors from models like BERT, ELMo, Universal Sentence Encoder.\n\n * Transformer models: Modern deep learning models like BERT can directly provide similarity scores or embeddings for comparison that capture rich contextual meaning.\n\n________________________________________\n\n7. Topic Modeling-based Similarity\n\n* Uses topic distributions derived from topic models like Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF).\n\n* In these models, documents are represented as a mixture of underlying topics, and each topic is a distribution over words.\n\n* So, each document can be represented as a vector where each component is the probability or proportion of a particular topic in that document.\n\n* Similarity between documents is then computed by comparing their topic distributions (e.g., using Cosine Similarity, Jensen-Shannon divergence, or Hellinger distance on the topic vectors).\n\n* Useful for comparing documents at a conceptual or thematic level rather than relying on exact word matches.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(a) Similarity Measure.txt",
    "file_name": "lec6-(a) Similarity Measure.txt",
    "filename_keywords": [
      "lec6",
      "similarity",
      "measure"
    ],
    "content_keywords": [
      "cosine similarity",
      "specifically levenshtein distance",
      "vector b",
      "values",
      "for",
      "these",
      "dna",
      "measures",
      "queen",
      "wordnet",
      "embedding",
      "size",
      "modern",
      "statistical",
      "utilizing",
      "word embedding similarity,",
      "why",
      "latent dirichlet allocation",
      "nmf",
      "compares",
      "documents",
      "topic modeling-based similarity",
      "idf",
      "corpus",
      "lda",
      "jensen",
      "non",
      "example",
      "overview\n\na",
      "transformer",
      "often",
      "sitten",
      "where",
      "shannon",
      "the",
      "semantic similarity measures",
      "jaccard",
      "popular",
      "formula",
      "magnitude",
      "euclidean distance",
      "matrix factorization",
      "uses",
      "universal sentence encoder",
      "dot product",
      "kitten",
      "jaccard similarity",
      "latent semantic analysis",
      "they",
      "useful",
      "this",
      "range",
      "hellinger",
      "lsa",
      "king",
      "grouping",
      "similarity",
      "sitting",
      "sentence",
      "knowledge",
      "edit distance",
      "the levenshtein",
      "methods",
      "identifying",
      "intersection",
      "word embedding similarity",
      "euclidean",
      "common similarity measures",
      "vector a",
      "glove",
      "fasttext",
      "topic modeling",
      "elmo",
      "similarity measure",
      "union",
      "nlp",
      "as the crow flies",
      "suggesting",
      "bert",
      "edit distance (specifically levenshtein distance)",
      "two",
      "sittin"
    ],
    "technical_terms": [
      "magnitude",
      "compares",
      "euclidean distance",
      "cosine similarity",
      "matrix factorization",
      "edit distance",
      "specifically levenshtein distance",
      "the levenshtein",
      "documents",
      "vector b",
      "idf",
      "methods",
      "uses",
      "values",
      "for",
      "these",
      "corpus",
      "universal sentence encoder",
      "lda",
      "dot product",
      "identifying",
      "dna",
      "jensen",
      "intersection",
      "measures",
      "non",
      "jaccard similarity",
      "example",
      "word embedding similarity",
      "euclidean",
      "latent semantic analysis",
      "embedding",
      "common similarity measures",
      "size",
      "vector a",
      "glove",
      "fasttext",
      "topic modeling",
      "they",
      "overview\n\na",
      "elmo",
      "similarity measure",
      "transformer",
      "useful",
      "often",
      "modern",
      "union",
      "nlp",
      "range",
      "this",
      "hellinger",
      "statistical",
      "lsa",
      "where",
      "utilizing",
      "suggesting",
      "shannon",
      "the",
      "grouping",
      "bert",
      "why",
      "latent dirichlet allocation",
      "similarity",
      "semantic similarity measures",
      "jaccard",
      "sentence",
      "popular",
      "two",
      "wordnet",
      "nmf",
      "formula",
      "knowledge"
    ],
    "all_keywords": [
      "sittin",
      "cosine similarity",
      "specifically levenshtein distance",
      "vector b",
      "values",
      "for",
      "these",
      "dna",
      "measures",
      "queen",
      "embedding",
      "size",
      "modern",
      "statistical",
      "utilizing",
      "word embedding similarity,",
      "why",
      "latent dirichlet allocation",
      "nmf",
      "compares",
      "documents",
      "topic modeling-based similarity",
      "idf",
      "corpus",
      "lda",
      "jensen",
      "non",
      "example",
      "overview\n\na",
      "transformer",
      "often",
      "sitten",
      "where",
      "shannon",
      "the",
      "semantic similarity measures",
      "jaccard",
      "popular",
      "formula",
      "magnitude",
      "euclidean distance",
      "matrix factorization",
      "uses",
      "universal sentence encoder",
      "dot product",
      "kitten",
      "jaccard similarity",
      "latent semantic analysis",
      "they",
      "useful",
      "this",
      "range",
      "hellinger",
      "lsa",
      "king",
      "grouping",
      "similarity",
      "sitting",
      "sentence",
      "knowledge",
      "edit distance",
      "the levenshtein",
      "methods",
      "lec6",
      "identifying",
      "intersection",
      "word embedding similarity",
      "euclidean",
      "common similarity measures",
      "vector a",
      "glove",
      "fasttext",
      "topic modeling",
      "elmo",
      "similarity measure",
      "measure",
      "union",
      "nlp",
      "as the crow flies",
      "suggesting",
      "bert",
      "edit distance (specifically levenshtein distance)",
      "two",
      "wordnet"
    ],
    "keyword_string": "sittin cosine similarity specifically levenshtein distance vector b values for these dna measures queen embedding size modern statistical utilizing word embedding similarity, why latent dirichlet allocation nmf compares documents topic modeling-based similarity idf corpus lda jensen non example overview\n\na transformer often sitten where shannon the semantic similarity measures jaccard popular formula magnitude euclidean distance matrix factorization uses universal sentence encoder dot product kitten jaccard similarity latent semantic analysis they useful this range hellinger lsa king grouping similarity sitting sentence knowledge edit distance the levenshtein methods lec6 identifying intersection word embedding similarity euclidean common similarity measures vector a glove fasttext topic modeling elmo similarity measure measure union nlp as the crow flies suggesting bert edit distance (specifically levenshtein distance) two wordnet",
    "token_count": 2069,
    "word_count": 1135,
    "sentence_count": 63,
    "paragraph_count": 90,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5485741904301595,
    "avg_sentence_length": 18.015873015873016,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": true,
    "content_type": "Technical, Structured, Definitions"
  },
  {
    "document_id": 138,
    "document_hash": "0573f7a22840",
    "content": "Jaccard Similarity Coefficient\n\nDefinition\n\nThe Jaccard Coefficient (also known as Jaccard Index or sometimes referred to as the Tanimoto Coefficient in some contexts, though Tanimoto often applies to binary attributes in vectors) is a statistic used to measure the similarity and diversity between two finite sets. It quantifies the degree of overlap between two sets, let's call them set A and set B.\n\n________________________________________\n\nFormula\n\nJaccard(A, B) = |A intersect B| / |A union B|\n\nWhere:\n\n* `|A intersect B|` represents the number of elements common to both set A and set B (i.e., the cardinality of the intersection of A and B).\n\n* `|A union B|` represents the total number of unique elements present in either set A or set B (or both) (i.e., the cardinality of the union of A and B).\n\n It can also be expressed as: |A union B| = |A| + |B| - |A intersect B|\n\n________________________________________\n\nProperties\n\n* Jaccard(A, A) = 1\n\n This means a set is perfectly similar to itself. The intersection and union are the set itself, so |A| / |A| = 1.\n\n* Jaccard(A, B) = 0 if (A intersect B) is an empty set (∅).\n\n This means if there are no common elements between set A and set B, their Jaccard similarity is zero.\n\n* The Jaccard score is always between 0 and 1, inclusive (0 <= Jaccard(A, B) <= 1).\n\n A score of 1 indicates perfect similarity, and a score of 0 indicates no similarity.\n\n________________________________________\n\nConceptual Diagram Explanation\n\nImagine a Venn diagram with two overlapping circles representing set A and set B.\n\n* Intersection (A intersect B): This is the overlapping area in the Venn diagram. It contains all the elements that are present in BOTH set A AND set B. The Jaccard formula uses the count of these common elements in its numerator.\n\n* Union (A union B): This is the total area covered by both circles in the Venn diagram, including the overlapping part. It contains all unique elements that are present in set A OR set B OR both. The Jaccard formula uses the count of these total unique elements in its denominator.\n\nThese visual components directly correspond to the numerator (intersection size) and the denominator (union size) of the Jaccard formula.\n\n________________________________________\n\nExample Calculation\n\nSentences:\n\n1. S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\"\n\n2. S2: \"The Bachelors of Computer Science Program is offered at Bahria University, Islamabad.\"\n\nTokenization (treating words as elements of sets, case-sensitive for this example, though typically lowercasing is done):\n\n* Set A (words from S1):\n\n A = {'I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad'}\n\n Number of elements in A, |A| = 14\n\n* Set B (words from S2):\n\n B = {'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'is', 'offered', 'at', 'Bahria', 'University', 'Islamabad'}\n\n Number of elements in B, |B| = 12\n\n________________________________________\n\nCalculate:\n\n* Union (A union B):\n\n This set contains all unique words from both A and B.\n\n A union B = {'I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered'}\n\n Number of elements in (A union B), |A union B| = 16\n\n* Intersection (A intersect B):\n\n This set contains only the words common to both A and B.\n\n A intersect B = {'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad'}\n\n Number of elements in (A intersect B), |A intersect B| = 10\n\n________________________________________\n\nJaccard similarity:\n\nJaccard(A, B) = |A intersect B| / |A union B|\n\nJaccard(A, B) = 10 / 16\n\nJaccard(A, B) = 0.625\n\n________________________________________\n\nAlternate Formula (Often called Tanimoto Coefficient for binary attributes, equivalent to Jaccard for sets)\n\nT = Nc / (Na + Nb - Nc)\n\nWhere:\n\n* Nc = |A intersect B| (number of common elements)\n\n* Na = |A| (number of elements in set A)\n\n* Nb = |B| (number of elements in set B)\n\nUsing the example:\n\nNa = 14\n\nNb = 12\n\nNc = 10\n\nT = 10 / (14 + 12 - 10)\n\nT = 10 / (26 - 10)\n\nT = 10 / 16\n\nT = 0.625\n\nThis confirms that Na + Nb - Nc is an alternative way to calculate |A union B|.\n\n________________________________________\n\nPros and Cons\n\nPros:\n\n* Simple and intuitive: The concept of overlap relative to total unique items is easy to understand.\n\n* Efficient to calculate: Requires basic set operations (intersection, union) and counts.\n\n* Not sensitive to differences in set sizes in the same way some other metrics might be; it focuses on the proportion of shared items.\n\nCons:\n\n* Ignores term frequency: It only considers the presence or absence of an element (e.g., a word). It doesn't care if a word appears once or multiple times within a document when documents are converted to sets of unique words.\n\n* Doesn’t inherently weight rare terms more than common terms: All shared elements contribute equally to the intersection count.\n\n* Needs normalization or careful consideration for length variations when term frequency or the magnitude of elements is important. For instance, if comparing very short texts to very long texts, the Jaccard index might be misleading if one is not just interested in shared vocabulary.\n\n________________________________________\n\nUseful Online Calculator\n\nFor experimenting with Jaccard similarity calculations, you can find online tools. One such example is: planetcalc.com/1664/",
    "enhanced_text": "[NLP] Jaccard Similarity Coefficient\n\nDefinition\n\nThe Jaccard Coefficient (also known as Jaccard Index or sometimes referred to as the Tanimoto Coefficient in some contexts, though Tanimoto often applies to binary attributes in vectors) is a statistic used to measure the similarity and diversity between two finite sets. It quantifies the degree of overlap between two sets, let's call them set A and set B.\n\n________________________________________\n\nFormula\n\nJaccard(A, B) = |A intersect B| / |A union B|\n\nWhere:\n\n* `|A intersect B|` represents the number of elements common to both set A and set B (i.e., the cardinality of the intersection of A and B).\n\n* `|A union B|` represents the total number of unique elements present in either set A or set B (or both) (i.e., the cardinality of the union of A and B).\n\n It can also be expressed as: |A union B| = |A| + |B| - |A intersect B|\n\n________________________________________\n\nProperties\n\n* Jaccard(A, A) = 1\n\n This means a set is perfectly similar to itself. The intersection and union are the set itself, so |A| / |A| = 1.\n\n* Jaccard(A, B) = 0 if (A intersect B) is an empty set (∅).\n\n This means if there are no common elements between set A and set B, their Jaccard similarity is zero.\n\n* The Jaccard score is always between 0 and 1, inclusive (0 <= Jaccard(A, B) <= 1).\n\n A score of 1 indicates perfect similarity, and a score of 0 indicates no similarity.\n\n________________________________________\n\nConceptual Diagram Explanation\n\nImagine a Venn diagram with two overlapping circles representing set A and set B.\n\n* Intersection (A intersect B): This is the overlapping area in the Venn diagram. It contains all the elements that are present in BOTH set A AND set B. The Jaccard formula uses the count of these common elements in its numerator.\n\n* Union (A union B): This is the total area covered by both circles in the Venn diagram, including the overlapping part. It contains all unique elements that are present in set A OR set B OR both. The Jaccard formula uses the count of these total unique elements in its denominator.\n\nThese visual components directly correspond to the numerator (intersection size) and the denominator (union size) of the Jaccard formula.\n\n________________________________________\n\nExample Calculation\n\nSentences:\n\n1. S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\"\n\n2. S2: \"The Bachelors of Computer Science Program is offered at Bahria University, Islamabad.\"\n\nTokenization (treating words as elements of sets, case-sensitive for this example, though typically lowercasing is done):\n\n* Set A (words from S1):\n\n A = {'I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad'}\n\n Number of elements in A, |A| = 14\n\n* Set B (words from S2):\n\n B = {'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'is', 'offered', 'at', 'Bahria', 'University', 'Islamabad'}\n\n Number of elements in B, |B| = 12\n\n________________________________________\n\nCalculate:\n\n* Union (A union B):\n\n This set contains all unique words from both A and B.\n\n A union B = {'I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered'}\n\n Number of elements in (A union B), |A union B| = 16\n\n* Intersection (A intersect B):\n\n This set contains only the words common to both A and B.\n\n A intersect B = {'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad'}\n\n Number of elements in (A intersect B), |A intersect B| = 10\n\n________________________________________\n\nJaccard similarity:\n\nJaccard(A, B) = |A intersect B| / |A union B|\n\nJaccard(A, B) = 10 / 16\n\nJaccard(A, B) = 0.625\n\n________________________________________\n\nAlternate Formula (Often called Tanimoto Coefficient for binary attributes, equivalent to Jaccard for sets)\n\nT = Nc / (Na + Nb - Nc)\n\nWhere:\n\n* Nc = |A intersect B| (number of common elements)\n\n* Na = |A| (number of elements in set A)\n\n* Nb = |B| (number of elements in set B)\n\nUsing the example:\n\nNa = 14\n\nNb = 12\n\nNc = 10\n\nT = 10 / (14 + 12 - 10)\n\nT = 10 / (26 - 10)\n\nT = 10 / 16\n\nT = 0.625\n\nThis confirms that Na + Nb - Nc is an alternative way to calculate |A union B|.\n\n________________________________________\n\nPros and Cons\n\nPros:\n\n* Simple and intuitive: The concept of overlap relative to total unique items is easy to understand.\n\n* Efficient to calculate: Requires basic set operations (intersection, union) and counts.\n\n* Not sensitive to differences in set sizes in the same way some other metrics might be; it focuses on the proportion of shared items.\n\nCons:\n\n* Ignores term frequency: It only considers the presence or absence of an element (e.g., a word). It doesn't care if a word appears once or multiple times within a document when documents are converted to sets of unique words.\n\n* Doesn’t inherently weight rare terms more than common terms: All shared elements contribute equally to the intersection count.\n\n* Needs normalization or careful consideration for length variations when term frequency or the magnitude of elements is important. For instance, if comparing very short texts to very long texts, the Jaccard index might be misleading if one is not just interested in shared vocabulary.\n\n________________________________________\n\nUseful Online Calculator\n\nFor experimenting with Jaccard similarity calculations, you can find online tools. One such example is: planetcalc.com/1664/",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(b) Jaccard Similarity Coefficient.txt",
    "file_name": "lec6-(b) Jaccard Similarity Coefficient.txt",
    "filename_keywords": [
      "lec6",
      "jaccard",
      "coefficient",
      "similarity"
    ],
    "content_keywords": [
      "islamabad",
      "bahria university",
      "simple",
      "a and",
      "formula\n\njaccard",
      "venn",
      "b or",
      "efficient",
      "computer science program",
      "both",
      "useful online calculator\n\nfor",
      "calculate",
      "example calculation\n\nsentences",
      "for",
      "these",
      "set a",
      "the bachelors",
      "tanimoto coefficient",
      "jaccard index",
      "university",
      "intersection",
      "ignores",
      "pros",
      "all",
      "tokenization",
      "enrolled",
      "bachelors",
      "number",
      "one",
      "alternate formula",
      "doesn",
      "and",
      "using",
      "the jaccard",
      "program",
      "bahria",
      "often",
      "needs",
      "jaccard similarity coefficient\n\ndefinition\n\nthe jaccard coefficient",
      "requires",
      "a or",
      "s1: \"i am enrolled in the bachelors of computer sci",
      "this",
      "union",
      "tanimoto",
      "cons\n\npros",
      "s2: \"the bachelors of computer science program is o",
      "science",
      "where",
      "computer",
      "the",
      "not",
      "set b",
      "cons",
      "jaccard",
      "conceptual diagram explanation\n\nimagine",
      "2. s2:",
      "offered",
      "properties"
    ],
    "technical_terms": [
      "islamabad",
      "bahria university",
      "simple",
      "a and",
      "formula\n\njaccard",
      "venn",
      "b or",
      "efficient",
      "computer science program",
      "both",
      "useful online calculator\n\nfor",
      "calculate",
      "example calculation\n\nsentences",
      "for",
      "these",
      "set a",
      "the bachelors",
      "tanimoto coefficient",
      "jaccard index",
      "university",
      "intersection",
      "ignores",
      "pros",
      "all",
      "tokenization",
      "bachelors",
      "number",
      "one",
      "alternate formula",
      "doesn",
      "and",
      "using",
      "the jaccard",
      "program",
      "bahria",
      "often",
      "needs",
      "jaccard similarity coefficient\n\ndefinition\n\nthe jaccard coefficient",
      "requires",
      "a or",
      "this",
      "union",
      "tanimoto",
      "cons\n\npros",
      "science",
      "where",
      "computer",
      "the",
      "not",
      "set b",
      "cons",
      "jaccard",
      "conceptual diagram explanation\n\nimagine",
      "properties"
    ],
    "all_keywords": [
      "islamabad",
      "bahria university",
      "simple",
      "a and",
      "formula\n\njaccard",
      "venn",
      "b or",
      "efficient",
      "computer science program",
      "both",
      "useful online calculator\n\nfor",
      "calculate",
      "example calculation\n\nsentences",
      "for",
      "these",
      "set a",
      "the bachelors",
      "jaccard index",
      "tanimoto coefficient",
      "lec6",
      "university",
      "intersection",
      "ignores",
      "pros",
      "all",
      "tokenization",
      "enrolled",
      "bachelors",
      "number",
      "one",
      "alternate formula",
      "doesn",
      "and",
      "using",
      "the jaccard",
      "program",
      "bahria",
      "often",
      "needs",
      "jaccard similarity coefficient\n\ndefinition\n\nthe jaccard coefficient",
      "requires",
      "a or",
      "s1: \"i am enrolled in the bachelors of computer sci",
      "this",
      "union",
      "tanimoto",
      "cons\n\npros",
      "s2: \"the bachelors of computer science program is o",
      "science",
      "where",
      "computer",
      "the",
      "not",
      "set b",
      "similarity",
      "cons",
      "coefficient",
      "jaccard",
      "conceptual diagram explanation\n\nimagine",
      "2. s2:",
      "offered",
      "properties"
    ],
    "keyword_string": "islamabad bahria university simple a and formula\n\njaccard venn b or efficient computer science program both useful online calculator\n\nfor calculate example calculation\n\nsentences for these set a the bachelors jaccard index tanimoto coefficient lec6 university intersection ignores pros all tokenization enrolled bachelors number one alternate formula doesn and using the jaccard program bahria often needs jaccard similarity coefficient\n\ndefinition\n\nthe jaccard coefficient requires a or s1: \"i am enrolled in the bachelors of computer sci this union tanimoto cons\n\npros s2: \"the bachelors of computer science program is o science where computer the not set b similarity cons coefficient jaccard conceptual diagram explanation\n\nimagine 2. s2: offered properties",
    "token_count": 1742,
    "word_count": 893,
    "sentence_count": 35,
    "paragraph_count": 80,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5126291618828932,
    "avg_sentence_length": 25.514285714285716,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": true,
    "content_type": "Technical, Structured, Definitions"
  },
  {
    "document_id": 139,
    "document_hash": "5adf91036082",
    "content": "Cosine Similarity\n\nDefinition\n\nCosine Similarity measures the similarity between two non-zero vectors by calculating the cosine of the angle between them in an inner product space. It focuses on the orientation of the vectors, ignoring their magnitudes (lengths).\n\n* Range: Cosine similarity values range from -1 to 1. However, when vectors have only non-negative components (like word counts or TF-IDF scores), the similarity typically ranges from 0 to 1.\n\n* Interpretation:\n\n * A value of 1 means the vectors point in the exact same direction (perfectly similar orientation).\n\n * A value of 0 means the vectors are orthogonal (perpendicular), indicating no similarity in their orientation.\n\n * A value of -1 (possible with vectors containing negative values) means the vectors point in opposite directions.\n\n________________________________________\n\nFormula\n\nCosine Similarity = (A . B) / (||A|| * ||B||)\n\nWhere:\n\n* `A . B` represents the dot product of vectors A and B.\n\n This is calculated by multiplying corresponding elements of the two vectors and then summing all those products. For vectors A = [a1, a2, ..., an] and B = [b1, b2, ..., bn], the dot product is:\n\n A . B = (a1*b1) + (a2*b2) + ... + (an*bn)\n\n* `||A||` represents the magnitude (also known as the Euclidean norm or length) of vector A.\n\n This is calculated as the square root of the sum of the squares of its components. For vector A = [a1, a2, ..., an], the magnitude is:\n\n ||A|| = sqrt(a1^2 + a2^2 + ... + an^2)\n\n* `||B||` represents the magnitude of vector B.\n\n Calculated similarly to ||A||:\n\n ||B|| = sqrt(b1^2 + b2^2 + ... + bn^2)\n\n________________________________________\n\nExample\n\nSentences:\n\n1. S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\"\n\n2. S2: \"The Bachelors of Computer Science Program is offered at Bahria University, Islamabad.\"\n\n________________________________________\n\nStep 1: Tokenize and Create Vocabulary\n\nCombine all unique words from both sentences into a vocabulary. Let's assume a fixed order for the vocabulary:\n\nVocabulary = ['I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered']\n\n(The vocabulary has 16 unique words.)\n\n________________________________________\n\nStep 2: Construct Binary Vectors\n\nRepresent each sentence as a vector where each element corresponds to a word in the vocabulary. A '1' indicates the word is present in the sentence, and a '0' indicates it is absent.\n\n* Vector for Sentence 1 (V1):\n\n Based on the vocabulary order: ['I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered']\n\n V1 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n\n (The words 'is' and 'offered' are absent from S1, so their corresponding positions are 0.)\n\n* Vector for Sentence 2 (V2):\n\n V2 = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n (The words 'I', 'am', 'enrolled', 'in' are absent from S2, so their corresponding positions are 0.)\n\n________________________________________\n\nStep 3: Compute Magnitudes\n\nMagnitude of V1 (||V1||):\n\n||V1|| = sqrt(sum of the squares of each element in V1)\n\nV1 has 14 ones and 2 zeros.\n\n||V1|| = sqrt( (1^2 * 14) + (0^2 * 2) )\n\n||V1|| = sqrt( (1 * 14) + (0 * 2) )\n\n||V1|| = sqrt(14 + 0)\n\n||V1|| = sqrt(14)\n\nMagnitude of V2 (||V2||):\n\n||V2|| = sqrt(sum of the squares of each element in V2)\n\nV2 has 12 ones and 4 zeros.\n\n||V2|| = sqrt( (1^2 * 12) + (0^2 * 4) )\n\n||V2|| = sqrt( (1 * 12) + (0 * 4) )\n\n||V2|| = sqrt(12 + 0)\n\n||V2|| = sqrt(12)\n\n________________________________________\n\nStep 4: Compute Dot Product (V1 . V2)\n\nThe dot product V1 . V2 is the sum of the products of corresponding elements.\n\nV1 . V2 = (1*0) + (1*0) + (1*0) + (1*0) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (0*1) + (0*1)\n\nV1 . V2 = 0 + 0 + 0 + 0 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 0 + 0\n\nV1 . V2 = 10\n\n(Alternatively, for binary vectors, the dot product is simply the count of common words, i.e., positions where both vectors have a 1. There are 10 such common words.)\n\n________________________________________\n\nStep 5: Calculate Cosine Similarity\n\nCosine Similarity = (V1 . V2) / (||V1|| * ||V2||)\n\nCosine Similarity = 10 / (sqrt(14) * sqrt(12))\n\nCosine Similarity = 10 / sqrt(14 * 12)\n\nCosine Similarity = 10 / sqrt(168)\n\nUsing approximations: sqrt(168) is approximately 12.96\n\nCosine Similarity approx 10 / 12.96\n\nCosine Similarity approx 0.771\n\n________________________________________\n\nInterpretation\n\n* A cosine similarity of approximately 0.771 indicates a fairly strong similarity in terms of shared words (and thus, orientation of their vector representations) between the two sentences.\n\n* It effectively ignores differences in the length of the sentences (number of words) if those different words are not shared, and focuses on the proportion of shared content relative to their individual complexities (magnitudes).\n\n________________________________________\n\nSummary: Why Use Cosine Similarity?\n\n* Captures similarity of orientation (direction) of vectors, regardless of their magnitude (length). This is useful when the length of the document (e.g., word count) is less important than the content.\n\n* Works well for text similarity, especially when using term frequency vectors (like TF-IDF), where the magnitude can vary significantly between documents of different lengths but the relative importance of terms (direction) is key.\n\n* Commonly used in information retrieval for document similarity, in recommendation systems (e.g., finding similar users or items), and in clustering tasks.",
    "enhanced_text": "[NLP] Cosine Similarity\n\nDefinition\n\nCosine Similarity measures the similarity between two non-zero vectors by calculating the cosine of the angle between them in an inner product space. It focuses on the orientation of the vectors, ignoring their magnitudes (lengths).\n\n* Range: Cosine similarity values range from -1 to 1. However, when vectors have only non-negative components (like word counts or TF-IDF scores), the similarity typically ranges from 0 to 1.\n\n* Interpretation:\n\n * A value of 1 means the vectors point in the exact same direction (perfectly similar orientation).\n\n * A value of 0 means the vectors are orthogonal (perpendicular), indicating no similarity in their orientation.\n\n * A value of -1 (possible with vectors containing negative values) means the vectors point in opposite directions.\n\n________________________________________\n\nFormula\n\nCosine Similarity = (A . B) / (||A|| * ||B||)\n\nWhere:\n\n* `A . B` represents the dot product of vectors A and B.\n\n This is calculated by multiplying corresponding elements of the two vectors and then summing all those products. For vectors A = [a1, a2, ..., an] and B = [b1, b2, ..., bn], the dot product is:\n\n A . B = (a1*b1) + (a2*b2) + ... + (an*bn)\n\n* `||A||` represents the magnitude (also known as the Euclidean norm or length) of vector A.\n\n This is calculated as the square root of the sum of the squares of its components. For vector A = [a1, a2, ..., an], the magnitude is:\n\n ||A|| = sqrt(a1^2 + a2^2 + ... + an^2)\n\n* `||B||` represents the magnitude of vector B.\n\n Calculated similarly to ||A||:\n\n ||B|| = sqrt(b1^2 + b2^2 + ... + bn^2)\n\n________________________________________\n\nExample\n\nSentences:\n\n1. S1: \"I am enrolled in the Bachelors of Computer Science Program at Bahria University, Islamabad.\"\n\n2. S2: \"The Bachelors of Computer Science Program is offered at Bahria University, Islamabad.\"\n\n________________________________________\n\nStep 1: Tokenize and Create Vocabulary\n\nCombine all unique words from both sentences into a vocabulary. Let's assume a fixed order for the vocabulary:\n\nVocabulary = ['I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered']\n\n(The vocabulary has 16 unique words.)\n\n________________________________________\n\nStep 2: Construct Binary Vectors\n\nRepresent each sentence as a vector where each element corresponds to a word in the vocabulary. A '1' indicates the word is present in the sentence, and a '0' indicates it is absent.\n\n* Vector for Sentence 1 (V1):\n\n Based on the vocabulary order: ['I', 'am', 'enrolled', 'in', 'the', 'Bachelors', 'of', 'Computer', 'Science', 'Program', 'at', 'Bahria', 'University', 'Islamabad', 'is', 'offered']\n\n V1 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n\n (The words 'is' and 'offered' are absent from S1, so their corresponding positions are 0.)\n\n* Vector for Sentence 2 (V2):\n\n V2 = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n (The words 'I', 'am', 'enrolled', 'in' are absent from S2, so their corresponding positions are 0.)\n\n________________________________________\n\nStep 3: Compute Magnitudes\n\nMagnitude of V1 (||V1||):\n\n||V1|| = sqrt(sum of the squares of each element in V1)\n\nV1 has 14 ones and 2 zeros.\n\n||V1|| = sqrt( (1^2 * 14) + (0^2 * 2) )\n\n||V1|| = sqrt( (1 * 14) + (0 * 2) )\n\n||V1|| = sqrt(14 + 0)\n\n||V1|| = sqrt(14)\n\nMagnitude of V2 (||V2||):\n\n||V2|| = sqrt(sum of the squares of each element in V2)\n\nV2 has 12 ones and 4 zeros.\n\n||V2|| = sqrt( (1^2 * 12) + (0^2 * 4) )\n\n||V2|| = sqrt( (1 * 12) + (0 * 4) )\n\n||V2|| = sqrt(12 + 0)\n\n||V2|| = sqrt(12)\n\n________________________________________\n\nStep 4: Compute Dot Product (V1 . V2)\n\nThe dot product V1 . V2 is the sum of the products of corresponding elements.\n\nV1 . V2 = (1*0) + (1*0) + (1*0) + (1*0) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (1*1) + (0*1) + (0*1)\n\nV1 . V2 = 0 + 0 + 0 + 0 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 0 + 0\n\nV1 . V2 = 10\n\n(Alternatively, for binary vectors, the dot product is simply the count of common words, i.e., positions where both vectors have a 1. There are 10 such common words.)\n\n________________________________________\n\nStep 5: Calculate Cosine Similarity\n\nCosine Similarity = (V1 . V2) / (||V1|| * ||V2||)\n\nCosine Similarity = 10 / (sqrt(14) * sqrt(12))\n\nCosine Similarity = 10 / sqrt(14 * 12)\n\nCosine Similarity = 10 / sqrt(168)\n\nUsing approximations: sqrt(168) is approximately 12.96\n\nCosine Similarity approx 10 / 12.96\n\nCosine Similarity approx 0.771\n\n________________________________________\n\nInterpretation\n\n* A cosine similarity of approximately 0.771 indicates a fairly strong similarity in terms of shared words (and thus, orientation of their vector representations) between the two sentences.\n\n* It effectively ignores differences in the length of the sentences (number of words) if those different words are not shared, and focuses on the proportion of shared content relative to their individual complexities (magnitudes).\n\n________________________________________\n\nSummary: Why Use Cosine Similarity?\n\n* Captures similarity of orientation (direction) of vectors, regardless of their magnitude (length). This is useful when the length of the document (e.g., word count) is less important than the content.\n\n* Works well for text similarity, especially when using term frequency vectors (like TF-IDF), where the magnitude can vary significantly between documents of different lengths but the relative importance of terms (direction) is key.\n\n* Commonly used in information retrieval for document similarity, in recommendation systems (e.g., finding similar users or items), and in clustering tasks.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(c) Cosine Similarity.txt",
    "file_name": "lec6-(c) Cosine Similarity.txt",
    "filename_keywords": [
      "lec6",
      "similarity",
      "cosine"
    ],
    "content_keywords": [
      "magnitude",
      "islamabad",
      "works",
      "bahria university",
      "cosine similarity",
      "construct binary vectors\n\nrepresent",
      "cosine similarity\n\ndefinition\n\ncosine similarity",
      "cosine",
      "step",
      "computer science program",
      "summary",
      "idf",
      "calculated",
      "for",
      "let",
      "captures",
      "the bachelors",
      "vocabulary",
      "there",
      "university",
      "create vocabulary\n\ncombine",
      "calculate cosine similarity\n\ncosine similarity",
      "bachelors",
      "enrolled",
      "euclidean",
      "alternatively",
      "vector",
      "and",
      "using",
      "interpretation",
      "program",
      "bahria",
      "s1: \"i am enrolled in the bachelors of computer sci",
      "this",
      "compute dot product",
      "range",
      "s2: \"the bachelors of computer science program is o",
      "compute magnitudes\n\nmagnitude",
      "example\n\nsentences",
      "where",
      "science",
      "tokenize",
      "computer",
      "the",
      "however",
      "why use cosine similarity",
      "based",
      "commonly",
      "formula\n\ncosine similarity",
      "sentence",
      "2. s2:",
      "offered"
    ],
    "technical_terms": [
      "magnitude",
      "islamabad",
      "works",
      "bahria university",
      "cosine similarity",
      "construct binary vectors\n\nrepresent",
      "cosine similarity\n\ndefinition\n\ncosine similarity",
      "cosine",
      "step",
      "computer science program",
      "summary",
      "idf",
      "calculated",
      "for",
      "let",
      "captures",
      "the bachelors",
      "vocabulary",
      "there",
      "university",
      "create vocabulary\n\ncombine",
      "calculate cosine similarity\n\ncosine similarity",
      "bachelors",
      "euclidean",
      "alternatively",
      "vector",
      "using",
      "interpretation",
      "program",
      "bahria",
      "this",
      "compute dot product",
      "range",
      "compute magnitudes\n\nmagnitude",
      "example\n\nsentences",
      "where",
      "science",
      "tokenize",
      "computer",
      "the",
      "however",
      "why use cosine similarity",
      "based",
      "commonly",
      "formula\n\ncosine similarity",
      "sentence"
    ],
    "all_keywords": [
      "magnitude",
      "islamabad",
      "works",
      "bahria university",
      "cosine similarity",
      "construct binary vectors\n\nrepresent",
      "cosine similarity\n\ndefinition\n\ncosine similarity",
      "cosine",
      "step",
      "computer science program",
      "summary",
      "idf",
      "calculated",
      "for",
      "let",
      "captures",
      "the bachelors",
      "vocabulary",
      "lec6",
      "there",
      "university",
      "create vocabulary\n\ncombine",
      "calculate cosine similarity\n\ncosine similarity",
      "bachelors",
      "enrolled",
      "euclidean",
      "alternatively",
      "vector",
      "and",
      "using",
      "interpretation",
      "program",
      "bahria",
      "s1: \"i am enrolled in the bachelors of computer sci",
      "this",
      "compute dot product",
      "range",
      "s2: \"the bachelors of computer science program is o",
      "compute magnitudes\n\nmagnitude",
      "example\n\nsentences",
      "where",
      "science",
      "tokenize",
      "computer",
      "the",
      "however",
      "why use cosine similarity",
      "similarity",
      "based",
      "commonly",
      "formula\n\ncosine similarity",
      "sentence",
      "2. s2:",
      "offered"
    ],
    "keyword_string": "magnitude islamabad works bahria university cosine similarity construct binary vectors\n\nrepresent cosine similarity\n\ndefinition\n\ncosine similarity cosine step computer science program summary idf calculated for let captures the bachelors vocabulary lec6 there university create vocabulary\n\ncombine calculate cosine similarity\n\ncosine similarity bachelors enrolled euclidean alternatively vector and using interpretation program bahria s1: \"i am enrolled in the bachelors of computer sci this compute dot product range s2: \"the bachelors of computer science program is o compute magnitudes\n\nmagnitude example\n\nsentences where science tokenize computer the however why use cosine similarity similarity based commonly formula\n\ncosine similarity sentence 2. s2: offered",
    "token_count": 1999,
    "word_count": 938,
    "sentence_count": 43,
    "paragraph_count": 82,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.46923461730865434,
    "avg_sentence_length": 21.813953488372093,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": true,
    "content_type": "Technical, Structured, Definitions"
  },
  {
    "document_id": 140,
    "document_hash": "bea18ae79624",
    "content": "Pros & Cons of Cosine Similarity\n\nPros:\n\nScale-invariant:\nNot affected by vector magnitudes, only their direction, which helps when document lengths vary.\n\nEffective for high-dimensional data:\nHandles the large number of dimensions common in text data (e.g., thousands of unique words).\n\nSuitable for large datasets:\nComputationally efficient and scalable for comparing many documents.\n\nEasy interpretability:\nSimilarity ranges from -1 (opposite) to 1 (identical), with 0 meaning orthogonal (no similarity).\n\nInvariant to vector length:\nFocuses on similarity in terms of presence or frequency patterns rather than raw word counts (especially with binary vectors).\n\nCons:\n\nNo semantic understanding:\nTreats each word independently without grasping synonyms or related meanings—unless combined with word embeddings.\n\nNot ideal for very sparse data:\nWhen documents share few common words, vectors mostly contain zeros, which may lead to unreliable similarity scores.\n\nSensitive to outliers:\nA small number of rare or unique words might disproportionately influence similarity if not managed (e.g., through weighting).",
    "enhanced_text": "[NLP] Pros & Cons of Cosine Similarity\n\nPros:\n\nScale-invariant:\nNot affected by vector magnitudes, only their direction, which helps when document lengths vary.\n\nEffective for high-dimensional data:\nHandles the large number of dimensions common in text data (e.g., thousands of unique words).\n\nSuitable for large datasets:\nComputationally efficient and scalable for comparing many documents.\n\nEasy interpretability:\nSimilarity ranges from -1 (opposite) to 1 (identical), with 0 meaning orthogonal (no similarity).\n\nInvariant to vector length:\nFocuses on similarity in terms of presence or frequency patterns rather than raw word counts (especially with binary vectors).\n\nCons:\n\nNo semantic understanding:\nTreats each word independently without grasping synonyms or related meanings—unless combined with word embeddings.\n\nNot ideal for very sparse data:\nWhen documents share few common words, vectors mostly contain zeros, which may lead to unreliable similarity scores.\n\nSensitive to outliers:\nA small number of rare or unique words might disproportionately influence similarity if not managed (e.g., through weighting).",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(d) Pros & Cons of Cosine Similarity.txt",
    "file_name": "lec6-(d) Pros & Cons of Cosine Similarity.txt",
    "filename_keywords": [
      "lec6",
      "similarity",
      "cosine",
      "pros",
      "cons"
    ],
    "content_keywords": [
      "computationally",
      "when",
      "handles",
      "not",
      "scale",
      "pros",
      "cons",
      "similarity",
      "suitable",
      "focuses",
      "cosine similarity\n\npros",
      "treats",
      "easy",
      "effective",
      "invariant",
      "sensitive"
    ],
    "technical_terms": [
      "computationally",
      "when",
      "handles",
      "not",
      "scale",
      "pros",
      "cons",
      "similarity",
      "suitable",
      "focuses",
      "cosine similarity\n\npros",
      "treats",
      "easy",
      "effective",
      "invariant",
      "sensitive"
    ],
    "all_keywords": [
      "computationally",
      "when",
      "handles",
      "lec6",
      "not",
      "scale",
      "similarity",
      "cosine",
      "pros",
      "cons",
      "suitable",
      "focuses",
      "cosine similarity\n\npros",
      "treats",
      "easy",
      "effective",
      "invariant",
      "sensitive"
    ],
    "keyword_string": "computationally when handles lec6 not scale similarity cosine pros cons suitable focuses cosine similarity\n\npros treats easy effective invariant sensitive",
    "token_count": 229,
    "word_count": 154,
    "sentence_count": 8,
    "paragraph_count": 11,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6724890829694323,
    "avg_sentence_length": 19.25,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": false,
    "content_type": "Technical"
  },
  {
    "document_id": 141,
    "document_hash": "45753147c8b3",
    "content": "TF-IDF (Term Frequency - Inverse Document Frequency)\n\nTF-IDF measures the importance of a term in a document relative to a corpus (a collection of documents). It balances how frequent a term is in a document (TF) with how rare it is across all documents (IDF).\n\n________________________________________\n\n1. Term Frequency (TF)\n\n* Measures how often a term appears in a document relative to the total number of terms in that document.\n\n* Formula:\n\n TF(t) = (Number of times term t appears in document) / (Total number of terms in document)\n\n________________________________________\n\n2. Inverse Document Frequency (IDF)\n\n* Measures how rare or common a term is across all documents in the corpus.\n\n* Formula:\n\n IDF(t) = log((Total number of documents) / (Number of documents containing term t))\n\n (Note: The 'log' here is often the natural logarithm (ln), but other bases can be used consistently.)\n\n* Terms common to many documents (like \"the\", \"and\") have low IDF.\n\n* Rare terms have high IDF, giving them more weight.\n\n________________________________________\n\n3. TF-IDF Score\n\n* The TF-IDF score is the product of Term Frequency and Inverse Document Frequency.\n\n* Formula:\n\n TF-IDF(t) = TF(t) * IDF(t)\n\n* A high TF-IDF score means the term is important in the specific document and rare in the corpus.\n\n________________________________________\n\nExample Calculation\n\nSentences:\n\n1. S1: \"I love Pakistan\"\n\n2. S2: \"Pakistan Zindabad\"\n\n3. S3: \"I am Pakistan\"\n\nStep 1: Tokenize\n\n* S1 = [\"I\", \"love\", \"Pakistan\"]\n\n* S2 = [\"Pakistan\", \"Zindabad\"]\n\n* S3 = [\"I\", \"am\", \"Pakistan\"]\n\nStep 2: Calculate Term Frequency (TF) — What is it?\n\nTerm Frequency is just a way to count how often a word shows up in a sentence (or document) compared to the total number of words in that sentence.\n\nHow to calculate:\n\n* Count how many times the word appears in the sentence.\n\n* Divide that by the total number of words in the sentence.\n\nExample:\n\nSentence 1: \"I love Pakistan\"\n\n* Total words = 3\n\n* \"I\" appears 1 time -> TF(\"I\", S1) = 1 / 3 = 0.33 (approximately)\n\n* \"love\" appears 1 time -> TF(\"love\", S1) = 1 / 3 = 0.33 (approximately)\n\n* \"Pakistan\" appears 1 time -> TF(\"Pakistan\", S1) = 1 / 3 = 0.33 (approximately)\n\nThis means each word has equal frequency in this sentence because they appear once and the sentence has three words.\n\nStep 3: Calculate IDF (Assuming natural log ln, corpus size = 3 documents)\n\n* The term \"I\" appears in 2 documents (S1, S3).\n\n IDF(\"I\") = ln(Total number of documents / Number of documents containing \"I\")\n\n IDF(\"I\") = ln(3 / 2) which is approximately 0.405\n\n* The term \"love\" appears in 1 document (S1).\n\n IDF(\"love\") = ln(Total number of documents / Number of documents containing \"love\")\n\n IDF(\"love\") = ln(3 / 1) which is approximately 1.099\n\n* The term \"Pakistan\" appears in all 3 documents (S1, S2, S3).\n\n IDF(\"Pakistan\") = ln(Total number of documents / Number of documents containing \"Pakistan\")\n\n IDF(\"Pakistan\") = ln(3 / 3) = ln(1) = 0\n\n* The term \"Zindabad\" appears in 1 document (S2).\n\n IDF(\"Zindabad\") = ln(Total number of documents / Number of documents containing \"Zindabad\")\n\n IDF(\"Zindabad\") = ln(3 / 1) which is approximately 1.099\n\n* The term \"am\" appears in 1 document (S3).\n\n IDF(\"am\") = ln(Total number of documents / Number of documents containing \"am\")\n\n IDF(\"am\") = ln(3 / 1) which is approximately 1.099\n\nStep 4: Calculate TF-IDF — What is it?\n\nNow, TF-IDF takes the TF (how often the word appears in one sentence) and multiplies it by the IDF (how unique or rare the word is across all sentences).\n\nSimple meaning:\n\n* If a word appears a lot in the sentence AND is rare in other sentences, it gets a high score.\n\n* If a word is common across all sentences (like \"Pakistan\" in this example), the score will be low or zero.\n\nHow to calculate TF-IDF:\n\nTF-IDF(term, document) = Term Frequency(term, document) * Inverse Document Frequency(term)\n\nExample:\n\nIn Sentence 1, for the term \"love\":\n\n* TF(\"love\", S1) = 0.33 (approximately, from step 2)\n\n* IDF(\"love\") = 1.099 (approximately, because it appears in only 1 sentence out of 3)\n\n* So, TF-IDF(\"love\", S1) = 0.33 * 1.099 which is approximately 0.36\n\nThat means \"love\" is important in Sentence 1 and unique to it (in this small corpus), so it gets a higher score.\n\nFor the term \"Pakistan\" in Sentence 1:\n\n* TF(\"Pakistan\", S1) = 0.33 (approximately)\n\n* IDF(\"Pakistan\") = 0 (since \"Pakistan\" appears in all sentences)\n\n* So, TF-IDF(\"Pakistan\", S1) = 0.33 * 0 = 0, meaning it’s not useful to differentiate sentences based on this term in this corpus.\n\nPros & Cons of TF-IDF\n\nPros:\n\n* Weights terms by importance: Gives higher importance to terms that are significant to a document but not common across all documents.\n\n* Downweights common words: Common stop words like \"the\", \"a\", etc., get very low scores (due to high document frequency, leading to low IDF).\n\n* Good for vector representation: Turns text into numerical vectors which can be used for Machine Learning models or similarity calculations.\n\n* Language independent: Based purely on statistical counts, no language-specific rules are required for its basic calculation.\n\nCons:\n\n* No semantic understanding: Treats each word independently; doesn't understand synonyms (e.g., \"car\" and \"automobile\" are treated as different) or context.\n\n* Sensitive to rare terms: Rare misspellings or very specific jargon may get high scores even if not broadly meaningful or if they are errors.\n\n* Assumes documents are bags of words: Ignores word order and grammar, which can be important for understanding meaning.",
    "enhanced_text": "[NLP] TF-IDF (Term Frequency - Inverse Document Frequency)\n\nTF-IDF measures the importance of a term in a document relative to a corpus (a collection of documents). It balances how frequent a term is in a document (TF) with how rare it is across all documents (IDF).\n\n________________________________________\n\n1. Term Frequency (TF)\n\n* Measures how often a term appears in a document relative to the total number of terms in that document.\n\n* Formula:\n\n TF(t) = (Number of times term t appears in document) / (Total number of terms in document)\n\n________________________________________\n\n2. Inverse Document Frequency (IDF)\n\n* Measures how rare or common a term is across all documents in the corpus.\n\n* Formula:\n\n IDF(t) = log((Total number of documents) / (Number of documents containing term t))\n\n (Note: The 'log' here is often the natural logarithm (ln), but other bases can be used consistently.)\n\n* Terms common to many documents (like \"the\", \"and\") have low IDF.\n\n* Rare terms have high IDF, giving them more weight.\n\n________________________________________\n\n3. TF-IDF Score\n\n* The TF-IDF score is the product of Term Frequency and Inverse Document Frequency.\n\n* Formula:\n\n TF-IDF(t) = TF(t) * IDF(t)\n\n* A high TF-IDF score means the term is important in the specific document and rare in the corpus.\n\n________________________________________\n\nExample Calculation\n\nSentences:\n\n1. S1: \"I love Pakistan\"\n\n2. S2: \"Pakistan Zindabad\"\n\n3. S3: \"I am Pakistan\"\n\nStep 1: Tokenize\n\n* S1 = [\"I\", \"love\", \"Pakistan\"]\n\n* S2 = [\"Pakistan\", \"Zindabad\"]\n\n* S3 = [\"I\", \"am\", \"Pakistan\"]\n\nStep 2: Calculate Term Frequency (TF) — What is it?\n\nTerm Frequency is just a way to count how often a word shows up in a sentence (or document) compared to the total number of words in that sentence.\n\nHow to calculate:\n\n* Count how many times the word appears in the sentence.\n\n* Divide that by the total number of words in the sentence.\n\nExample:\n\nSentence 1: \"I love Pakistan\"\n\n* Total words = 3\n\n* \"I\" appears 1 time -> TF(\"I\", S1) = 1 / 3 = 0.33 (approximately)\n\n* \"love\" appears 1 time -> TF(\"love\", S1) = 1 / 3 = 0.33 (approximately)\n\n* \"Pakistan\" appears 1 time -> TF(\"Pakistan\", S1) = 1 / 3 = 0.33 (approximately)\n\nThis means each word has equal frequency in this sentence because they appear once and the sentence has three words.\n\nStep 3: Calculate IDF (Assuming natural log ln, corpus size = 3 documents)\n\n* The term \"I\" appears in 2 documents (S1, S3).\n\n IDF(\"I\") = ln(Total number of documents / Number of documents containing \"I\")\n\n IDF(\"I\") = ln(3 / 2) which is approximately 0.405\n\n* The term \"love\" appears in 1 document (S1).\n\n IDF(\"love\") = ln(Total number of documents / Number of documents containing \"love\")\n\n IDF(\"love\") = ln(3 / 1) which is approximately 1.099\n\n* The term \"Pakistan\" appears in all 3 documents (S1, S2, S3).\n\n IDF(\"Pakistan\") = ln(Total number of documents / Number of documents containing \"Pakistan\")\n\n IDF(\"Pakistan\") = ln(3 / 3) = ln(1) = 0\n\n* The term \"Zindabad\" appears in 1 document (S2).\n\n IDF(\"Zindabad\") = ln(Total number of documents / Number of documents containing \"Zindabad\")\n\n IDF(\"Zindabad\") = ln(3 / 1) which is approximately 1.099\n\n* The term \"am\" appears in 1 document (S3).\n\n IDF(\"am\") = ln(Total number of documents / Number of documents containing \"am\")\n\n IDF(\"am\") = ln(3 / 1) which is approximately 1.099\n\nStep 4: Calculate TF-IDF — What is it?\n\nNow, TF-IDF takes the TF (how often the word appears in one sentence) and multiplies it by the IDF (how unique or rare the word is across all sentences).\n\nSimple meaning:\n\n* If a word appears a lot in the sentence AND is rare in other sentences, it gets a high score.\n\n* If a word is common across all sentences (like \"Pakistan\" in this example), the score will be low or zero.\n\nHow to calculate TF-IDF:\n\nTF-IDF(term, document) = Term Frequency(term, document) * Inverse Document Frequency(term)\n\nExample:\n\nIn Sentence 1, for the term \"love\":\n\n* TF(\"love\", S1) = 0.33 (approximately, from step 2)\n\n* IDF(\"love\") = 1.099 (approximately, because it appears in only 1 sentence out of 3)\n\n* So, TF-IDF(\"love\", S1) = 0.33 * 1.099 which is approximately 0.36\n\nThat means \"love\" is important in Sentence 1 and unique to it (in this small corpus), so it gets a higher score.\n\nFor the term \"Pakistan\" in Sentence 1:\n\n* TF(\"Pakistan\", S1) = 0.33 (approximately)\n\n* IDF(\"Pakistan\") = 0 (since \"Pakistan\" appears in all sentences)\n\n* So, TF-IDF(\"Pakistan\", S1) = 0.33 * 0 = 0, meaning it’s not useful to differentiate sentences based on this term in this corpus.\n\nPros & Cons of TF-IDF\n\nPros:\n\n* Weights terms by importance: Gives higher importance to terms that are significant to a document but not common across all documents.\n\n* Downweights common words: Common stop words like \"the\", \"a\", etc., get very low scores (due to high document frequency, leading to low IDF).\n\n* Good for vector representation: Turns text into numerical vectors which can be used for Machine Learning models or similarity calculations.\n\n* Language independent: Based purely on statistical counts, no language-specific rules are required for its basic calculation.\n\nCons:\n\n* No semantic understanding: Treats each word independently; doesn't understand synonyms (e.g., \"car\" and \"automobile\" are treated as different) or context.\n\n* Sensitive to rare terms: Rare misspellings or very specific jargon may get high scores even if not broadly meaningful or if they are errors.\n\n* Assumes documents are bags of words: Ignores word order and grammar, which can be important for understanding meaning.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(e) TF-IDF.txt",
    "file_name": "lec6-(e) TF-IDF.txt",
    "filename_keywords": [
      "lec6",
      "idf"
    ],
    "content_keywords": [
      "language",
      "assumes",
      "count",
      "idf score",
      "simple",
      "s2: \"pakistan zindabad\"",
      "s3: \"i am pakistan\"",
      "how",
      "pakistan",
      "step",
      "weights",
      "note",
      "idf",
      "i love pakistan",
      "s1: \"i love pakistan\"",
      "example calculation\n\nsentences",
      "for",
      "assuming",
      "rare",
      "idf\n\npros",
      ")\n\n idf(",
      "calculate term frequency",
      "measures",
      "term frequency",
      "pakistan zindabad",
      "example",
      "now",
      "number",
      "inverse document frequency",
      "calculate tf",
      "in sentence",
      "pros",
      "downweights",
      "common",
      "machine learning",
      "that",
      "and",
      "sensitive",
      "calculate idf",
      "tf-idf score",
      "the tf",
      "zindabad",
      "i am pakistan",
      "divide",
      "this",
      "good",
      "treats",
      "gives",
      "t understand synonyms (e.g.,",
      "tokenize",
      "love",
      "ignores",
      "terms",
      "the",
      "turns",
      "log",
      "based",
      "cons",
      "term frequency (tf)",
      "sentence",
      "appears 1 time -> tf(",
      "inverse document frequency (idf)",
      "total",
      "what",
      "formula"
    ],
    "technical_terms": [
      "language",
      "assumes",
      "count",
      "idf score",
      "simple",
      "how",
      "pakistan",
      "step",
      "weights",
      "note",
      "idf",
      "example calculation\n\nsentences",
      "for",
      "assuming",
      "rare",
      "idf\n\npros",
      "calculate term frequency",
      "measures",
      "term frequency",
      "pakistan zindabad",
      "example",
      "now",
      "number",
      "inverse document frequency",
      "calculate tf",
      "in sentence",
      "pros",
      "downweights",
      "common",
      "machine learning",
      "that",
      "and",
      "sensitive",
      "calculate idf",
      "the tf",
      "zindabad",
      "divide",
      "this",
      "good",
      "treats",
      "gives",
      "tokenize",
      "ignores",
      "terms",
      "the",
      "turns",
      "based",
      "cons",
      "sentence",
      "total",
      "what",
      "formula"
    ],
    "all_keywords": [
      "language",
      "assumes",
      "count",
      "idf score",
      "simple",
      "s2: \"pakistan zindabad\"",
      "s3: \"i am pakistan\"",
      "how",
      "pakistan",
      "step",
      "weights",
      "note",
      "idf",
      "i love pakistan",
      "s1: \"i love pakistan\"",
      "example calculation\n\nsentences",
      "for",
      "assuming",
      "rare",
      "idf\n\npros",
      ")\n\n idf(",
      "lec6",
      "calculate term frequency",
      "measures",
      "term frequency",
      "pakistan zindabad",
      "example",
      "now",
      "number",
      "inverse document frequency",
      "calculate tf",
      "in sentence",
      "pros",
      "downweights",
      "common",
      "machine learning",
      "that",
      "and",
      "sensitive",
      "calculate idf",
      "tf-idf score",
      "the tf",
      "zindabad",
      "i am pakistan",
      "divide",
      "this",
      "good",
      "treats",
      "gives",
      "t understand synonyms (e.g.,",
      "tokenize",
      "love",
      "ignores",
      "terms",
      "the",
      "turns",
      "log",
      "based",
      "cons",
      "term frequency (tf)",
      "sentence",
      "appears 1 time -> tf(",
      "inverse document frequency (idf)",
      "total",
      "what",
      "formula"
    ],
    "keyword_string": "language assumes count idf score simple s2: \"pakistan zindabad\" s3: \"i am pakistan\" how pakistan step weights note idf i love pakistan s1: \"i love pakistan\" example calculation\n\nsentences for assuming rare idf\n\npros )\n\n idf( lec6 calculate term frequency measures term frequency pakistan zindabad example now number inverse document frequency calculate tf in sentence pros downweights common machine learning that and sensitive calculate idf tf-idf score the tf zindabad i am pakistan divide this good treats gives t understand synonyms (e.g., tokenize love ignores terms the turns log based cons term frequency (tf) sentence appears 1 time -> tf( inverse document frequency (idf) total what formula",
    "token_count": 1642,
    "word_count": 918,
    "sentence_count": 38,
    "paragraph_count": 86,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.559074299634592,
    "avg_sentence_length": 24.157894736842106,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": true,
    "content_type": "Technical, Structured, Definitions"
  },
  {
    "document_id": 142,
    "document_hash": "c15923857765",
    "content": "What are Vector Space Models (VSMs)?\n\n* Vector Space Models (VSMs) are a way to represent text documents as mathematical vectors, which are essentially lists of numbers.\n\n* In this model, each unique word found across the entire collection of documents (corpus) acts as a dimension in a high-dimensional space.\n\n* This numerical representation allows text to be compared, analyzed, or manipulated using mathematical operations and algorithms.\n\n________________________________________\n\nWhat are VSMs used for?\n\nVSMs are utilized in a variety of Natural Language Processing (NLP) and Information Retrieval (IR) tasks, including:\n\n* Finding similar documents or texts: Identifying documents that share similar content.\n\n* Classifying documents into categories: Assigning documents to predefined topics or classes.\n\n* Summarizing content: Extracting key information from texts.\n\n* Identifying topics: Discovering underlying themes in a collection of documents (topic modeling).\n\n* Translating languages: As a component in some machine translation systems.\n\n* Any task where understanding the semantic relationship or comparing text content is important.\n\n________________________________________\n\nHow do we build and use a VSM? (Step-by-step)\n\n1. Get the text data: Collect the documents, sentences, or text snippets you want to model. This is your corpus.\n\n2. Clean the text (Preprocessing):\n\n * Tokenization: Break down the text into individual words or tokens.\n\n * Lowercasing: Convert all text to a consistent case, usually lowercase, to treat words like \"The\" and \"the\" as the same.\n\n * Stop word removal: Remove common words (e.g., \"is\", \"a\", \"the\", \"in\") that typically don't carry significant meaning for differentiating documents.\n\n * Stemming/Lemmatization: Reduce words to their root or base form (e.g., \"running\" to \"run\", \"studies\" to \"study\"). Lemmatization is generally more linguistically accurate than stemming.\n\n3. Build a vocabulary: Create a comprehensive list of all unique words (terms) that remain after the cleaning process across all documents in the corpus. The order of words in this vocabulary will define the order of components in the vectors.\n\n4. Create vectors (Document Representation): Represent each document as a numerical vector. Each element in the vector corresponds to a word in the vocabulary. The value of each element can be determined in several ways:\n\n * Binary: 1 if the word is present in the document, 0 if absent.\n\n * Term Frequency (TF): The count of how often a word appears in the document, possibly normalized by the total number of words in that document.\n\n * TF-IDF (Term Frequency-Inverse Document Frequency): A weighted score that gives higher importance to words that are frequent in a specific document but rare across the entire corpus.\n\n5. Calculate similarity: Use a similarity measure (like cosine similarity) to quantify how close or related document vectors are to each other in the vector space.\n\n6. Search/query (Information Retrieval): For search applications, a user's query is also converted into a vector. This query vector is then compared against all document vectors in the corpus using the chosen similarity measure to find the most relevant documents.\n\n________________________________________\n\nStrengths of VSMs\n\n* Flexible: Can be applied to a wide range of text analysis tasks and different types of textual data.\n\n* Scalable: Can handle large collections of documents and extensive vocabularies, especially with efficient data structures.\n\n* Effective at measuring similarity: Generally good at determining the degree of relatedness between two pieces of text based on shared terms.\n\n* Language independent (at a basic level): The core mechanism of counting words and creating vectors doesn't rely on deep grammatical rules of a specific language, making it broadly applicable.\n\n* Handles sparse data well: Most document vectors will have many zero entries (since a document usually contains only a small subset of the total vocabulary). VSMs and associated algorithms are often designed to work efficiently with such sparse vectors.\n\n________________________________________\n\nWeaknesses of VSMs\n\n* No context understanding (Bag-of-Words assumption): VSMs typically treat words as independent units, ignoring the order of words and the grammatical structure. This means the meaning derived from word context (e.g., \"apple\" the fruit vs. \"Apple\" the company) can be lost. \"Bank\" in \"river bank\" and \"investment bank\" would be treated as the same dimension.\n\n* Struggles with phrases or sentence meaning: The model doesn't inherently capture the meaning of multi-word expressions, idioms, or the overall semantic meaning of a sentence beyond the sum of its words.\n\n* Semantic drift and polysemy problems:\n\n * Semantic drift: The meaning of words can change over time, which VSMs built on older corpora might not capture.\n\n * Polysemy: Words with multiple meanings can lead to ambiguous or muddled vector representations because the same term contributes to the vector regardless of its intended sense in a particular context.\n\n________________________________________\n\nExample: Vector Space Model with 3 documents\n\nDocuments:\n\n* Doc1: \"I love Pakistan\"\n\n* Doc2: \"Pakistan Zindabad\"\n\n* Doc3: \"I am Pakistan\"\n\n________________________________________\n\nStep 1: Build Vocabulary\n\n(Assuming minimal preprocessing: tokenization and lowercasing. No stop word removal or stemming for this simple example.)\n\nCollect all unique words from these documents:\n\nVocabulary = [\"i\", \"love\", \"pakistan\", \"zindabad\", \"am\"]\n\nThe order in this list determines the dimension of our vectors.\n\n________________________________________\n\nStep 2: Calculate Term Frequency (TF)\n\nTerm Frequency (TF) for a term 't' in a document 'd' is calculated as:\n\nTF(t, d) = (Number of times term 't' appears in document 'd') / (Total number of terms in document 'd')\n\n* Doc1: \"i love pakistan\" (3 words)\n\n * TF(\"i\", Doc1) = 1/3\n\n * TF(\"love\", Doc1) = 1/3\n\n * TF(\"pakistan\", Doc1) = 1/3\n\n * TF(\"zindabad\", Doc1) = 0/3 = 0\n\n * TF(\"am\", Doc1) = 0/3 = 0\n\n* Doc2: \"pakistan zindabad\" (2 words)\n\n * TF(\"i\", Doc2) = 0/2 = 0\n\n * TF(\"love\", Doc2) = 0/2 = 0\n\n * TF(\"pakistan\", Doc2) = 1/2\n\n * TF(\"zindabad\", Doc2) = 1/2\n\n * TF(\"am\", Doc2) = 0/2 = 0\n\n* Doc3: \"i am pakistan\" (3 words)\n\n * TF(\"i\", Doc3) = 1/3\n\n * TF(\"love\", Doc3) = 0/3 = 0\n\n * TF(\"pakistan\", Doc3) = 1/3\n\n * TF(\"zindabad\", Doc3) = 0/3 = 0\n\n * TF(\"am\", Doc3) = 1/3\n\n________________________________________\n\nStep 3: Represent each document as a vector\n\nUsing the vocabulary order [\"i\", \"love\", \"pakistan\", \"zindabad\", \"am\"], the TF vectors are:\n\n* V_Doc1 = [1/3, 1/3, 1/3, 0, 0]\n\n (approx. [0.333, 0.333, 0.333, 0, 0])\n\n* V_Doc2 = [0, 0, 1/2, 1/2, 0]\n\n (approx. [0, 0, 0.5, 0.5, 0])\n\n* V_Doc3 = [1/3, 0, 1/3, 0, 1/3]\n\n (approx. [0.333, 0, 0.333, 0, 0.333])\n\n________________________________________\n\nStep 4: Compare documents using these vectors (e.g., using Cosine Similarity)\n\nWe can now calculate how similar these documents are by comparing their vectors mathematically. Let's calculate the cosine similarity between Doc1 and Doc3.\n\nFormula for Cosine Similarity:\n\nCosine Similarity(A, B) = (A . B) / (||A|| * ||B||)\n\nWhere:\n\nA . B = Dot product of A and B\n\n||A|| = Magnitude of A\n\n||B|| = Magnitude of B\n\nLet A = V_Doc1 = [1/3, 1/3, 1/3, 0, 0]\n\nLet B = V_Doc3 = [1/3, 0, 1/3, 0, 1/3]\n\na. Calculate the Dot Product (V_Doc1 . V_Doc3):\n\nV_Doc1 . V_Doc3 = (1/3 * 1/3) + (1/3 * 0) + (1/3 * 1/3) + (0 * 0) + (0 * 1/3)\n\n = 1/9 + 0 + 1/9 + 0 + 0\n\n = 2/9 (approx. 0.222)\n\nb. Calculate the Magnitude of V_Doc1 (||V_Doc1||):\n\n||V_Doc1|| = sqrt((1/3)^2 + (1/3)^2 + (1/3)^2 + 0^2 + 0^2)\n\n = sqrt(1/9 + 1/9 + 1/9 + 0 + 0)\n\n = sqrt(3/9)\n\n = sqrt(1/3) (approx. 0.577)\n\nc. Calculate the Magnitude of V_Doc3 (||V_Doc3||):\n\n||V_Doc3|| = sqrt((1/3)^2 + 0^2 + (1/3)^2 + 0^2 + (1/3)^2)\n\n = sqrt(1/9 + 0 + 1/9 + 0 + 1/9)\n\n = sqrt(3/9)\n\n = sqrt(1/3) (approx. 0.577)\n\nd. Calculate Cosine Similarity:\n\nCosine Similarity(V_Doc1, V_Doc3) = (2/9) / (sqrt(1/3) * sqrt(1/3))\n\n = (2/9) / (1/3)\n\n = (2/9) * 3\n\n = 6/9\n\n = 2/3 (approx. 0.667)\n\nInterpretation of this example:\n\nThe cosine similarity between Doc1 (\"I love Pakistan\") and Doc3 (\"I am Pakistan\") is approximately 0.667. This indicates a moderate to strong similarity, as they share the terms \"i\" and \"pakistan\" with similar term frequencies within their respective documents. Documents sharing more common words with similar frequencies (or TF-IDF scores) will have higher cosine similarity scores, indicating they are more similar in content.",
    "enhanced_text": "[NLP] What are Vector Space Models (VSMs)?\n\n* Vector Space Models (VSMs) are a way to represent text documents as mathematical vectors, which are essentially lists of numbers.\n\n* In this model, each unique word found across the entire collection of documents (corpus) acts as a dimension in a high-dimensional space.\n\n* This numerical representation allows text to be compared, analyzed, or manipulated using mathematical operations and algorithms.\n\n________________________________________\n\nWhat are VSMs used for?\n\nVSMs are utilized in a variety of Natural Language Processing (NLP) and Information Retrieval (IR) tasks, including:\n\n* Finding similar documents or texts: Identifying documents that share similar content.\n\n* Classifying documents into categories: Assigning documents to predefined topics or classes.\n\n* Summarizing content: Extracting key information from texts.\n\n* Identifying topics: Discovering underlying themes in a collection of documents (topic modeling).\n\n* Translating languages: As a component in some machine translation systems.\n\n* Any task where understanding the semantic relationship or comparing text content is important.\n\n________________________________________\n\nHow do we build and use a VSM? (Step-by-step)\n\n1. Get the text data: Collect the documents, sentences, or text snippets you want to model. This is your corpus.\n\n2. Clean the text (Preprocessing):\n\n * Tokenization: Break down the text into individual words or tokens.\n\n * Lowercasing: Convert all text to a consistent case, usually lowercase, to treat words like \"The\" and \"the\" as the same.\n\n * Stop word removal: Remove common words (e.g., \"is\", \"a\", \"the\", \"in\") that typically don't carry significant meaning for differentiating documents.\n\n * Stemming/Lemmatization: Reduce words to their root or base form (e.g., \"running\" to \"run\", \"studies\" to \"study\"). Lemmatization is generally more linguistically accurate than stemming.\n\n3. Build a vocabulary: Create a comprehensive list of all unique words (terms) that remain after the cleaning process across all documents in the corpus. The order of words in this vocabulary will define the order of components in the vectors.\n\n4. Create vectors (Document Representation): Represent each document as a numerical vector. Each element in the vector corresponds to a word in the vocabulary. The value of each element can be determined in several ways:\n\n * Binary: 1 if the word is present in the document, 0 if absent.\n\n * Term Frequency (TF): The count of how often a word appears in the document, possibly normalized by the total number of words in that document.\n\n * TF-IDF (Term Frequency-Inverse Document Frequency): A weighted score that gives higher importance to words that are frequent in a specific document but rare across the entire corpus.\n\n5. Calculate similarity: Use a similarity measure (like cosine similarity) to quantify how close or related document vectors are to each other in the vector space.\n\n6. Search/query (Information Retrieval): For search applications, a user's query is also converted into a vector. This query vector is then compared against all document vectors in the corpus using the chosen similarity measure to find the most relevant documents.\n\n________________________________________\n\nStrengths of VSMs\n\n* Flexible: Can be applied to a wide range of text analysis tasks and different types of textual data.\n\n* Scalable: Can handle large collections of documents and extensive vocabularies, especially with efficient data structures.\n\n* Effective at measuring similarity: Generally good at determining the degree of relatedness between two pieces of text based on shared terms.\n\n* Language independent (at a basic level): The core mechanism of counting words and creating vectors doesn't rely on deep grammatical rules of a specific language, making it broadly applicable.\n\n* Handles sparse data well: Most document vectors will have many zero entries (since a document usually contains only a small subset of the total vocabulary). VSMs and associated algorithms are often designed to work efficiently with such sparse vectors.\n\n________________________________________\n\nWeaknesses of VSMs\n\n* No context understanding (Bag-of-Words assumption): VSMs typically treat words as independent units, ignoring the order of words and the grammatical structure. This means the meaning derived from word context (e.g., \"apple\" the fruit vs. \"Apple\" the company) can be lost. \"Bank\" in \"river bank\" and \"investment bank\" would be treated as the same dimension.\n\n* Struggles with phrases or sentence meaning: The model doesn't inherently capture the meaning of multi-word expressions, idioms, or the overall semantic meaning of a sentence beyond the sum of its words.\n\n* Semantic drift and polysemy problems:\n\n * Semantic drift: The meaning of words can change over time, which VSMs built on older corpora might not capture.\n\n * Polysemy: Words with multiple meanings can lead to ambiguous or muddled vector representations because the same term contributes to the vector regardless of its intended sense in a particular context.\n\n________________________________________\n\nExample: Vector Space Model with 3 documents\n\nDocuments:\n\n* Doc1: \"I love Pakistan\"\n\n* Doc2: \"Pakistan Zindabad\"\n\n* Doc3: \"I am Pakistan\"\n\n________________________________________\n\nStep 1: Build Vocabulary\n\n(Assuming minimal preprocessing: tokenization and lowercasing. No stop word removal or stemming for this simple example.)\n\nCollect all unique words from these documents:\n\nVocabulary = [\"i\", \"love\", \"pakistan\", \"zindabad\", \"am\"]\n\nThe order in this list determines the dimension of our vectors.\n\n________________________________________\n\nStep 2: Calculate Term Frequency (TF)\n\nTerm Frequency (TF) for a term 't' in a document 'd' is calculated as:\n\nTF(t, d) = (Number of times term 't' appears in document 'd') / (Total number of terms in document 'd')\n\n* Doc1: \"i love pakistan\" (3 words)\n\n * TF(\"i\", Doc1) = 1/3\n\n * TF(\"love\", Doc1) = 1/3\n\n * TF(\"pakistan\", Doc1) = 1/3\n\n * TF(\"zindabad\", Doc1) = 0/3 = 0\n\n * TF(\"am\", Doc1) = 0/3 = 0\n\n* Doc2: \"pakistan zindabad\" (2 words)\n\n * TF(\"i\", Doc2) = 0/2 = 0\n\n * TF(\"love\", Doc2) = 0/2 = 0\n\n * TF(\"pakistan\", Doc2) = 1/2\n\n * TF(\"zindabad\", Doc2) = 1/2\n\n * TF(\"am\", Doc2) = 0/2 = 0\n\n* Doc3: \"i am pakistan\" (3 words)\n\n * TF(\"i\", Doc3) = 1/3\n\n * TF(\"love\", Doc3) = 0/3 = 0\n\n * TF(\"pakistan\", Doc3) = 1/3\n\n * TF(\"zindabad\", Doc3) = 0/3 = 0\n\n * TF(\"am\", Doc3) = 1/3\n\n________________________________________\n\nStep 3: Represent each document as a vector\n\nUsing the vocabulary order [\"i\", \"love\", \"pakistan\", \"zindabad\", \"am\"], the TF vectors are:\n\n* V_Doc1 = [1/3, 1/3, 1/3, 0, 0]\n\n (approx. [0.333, 0.333, 0.333, 0, 0])\n\n* V_Doc2 = [0, 0, 1/2, 1/2, 0]\n\n (approx. [0, 0, 0.5, 0.5, 0])\n\n* V_Doc3 = [1/3, 0, 1/3, 0, 1/3]\n\n (approx. [0.333, 0, 0.333, 0, 0.333])\n\n________________________________________\n\nStep 4: Compare documents using these vectors (e.g., using Cosine Similarity)\n\nWe can now calculate how similar these documents are by comparing their vectors mathematically. Let's calculate the cosine similarity between Doc1 and Doc3.\n\nFormula for Cosine Similarity:\n\nCosine Similarity(A, B) = (A . B) / (||A|| * ||B||)\n\nWhere:\n\nA . B = Dot product of A and B\n\n||A|| = Magnitude of A\n\n||B|| = Magnitude of B\n\nLet A = V_Doc1 = [1/3, 1/3, 1/3, 0, 0]\n\nLet B = V_Doc3 = [1/3, 0, 1/3, 0, 1/3]\n\na. Calculate the Dot Product (V_Doc1 . V_Doc3):\n\nV_Doc1 . V_Doc3 = (1/3 * 1/3) + (1/3 * 0) + (1/3 * 1/3) + (0 * 0) + (0 * 1/3)\n\n = 1/9 + 0 + 1/9 + 0 + 0\n\n = 2/9 (approx. 0.222)\n\nb. Calculate the Magnitude of V_Doc1 (||V_Doc1||):\n\n||V_Doc1|| = sqrt((1/3)^2 + (1/3)^2 + (1/3)^2 + 0^2 + 0^2)\n\n = sqrt(1/9 + 1/9 + 1/9 + 0 + 0)\n\n = sqrt(3/9)\n\n = sqrt(1/3) (approx. 0.577)\n\nc. Calculate the Magnitude of V_Doc3 (||V_Doc3||):\n\n||V_Doc3|| = sqrt((1/3)^2 + 0^2 + (1/3)^2 + 0^2 + (1/3)^2)\n\n = sqrt(1/9 + 0 + 1/9 + 0 + 1/9)\n\n = sqrt(3/9)\n\n = sqrt(1/3) (approx. 0.577)\n\nd. Calculate Cosine Similarity:\n\nCosine Similarity(V_Doc1, V_Doc3) = (2/9) / (sqrt(1/3) * sqrt(1/3))\n\n = (2/9) / (1/3)\n\n = (2/9) * 3\n\n = 6/9\n\n = 2/3 (approx. 0.667)\n\nInterpretation of this example:\n\nThe cosine similarity between Doc1 (\"I love Pakistan\") and Doc3 (\"I am Pakistan\") is approximately 0.667. This indicates a moderate to strong similarity, as they share the terms \"i\" and \"pakistan\" with similar term frequencies within their respective documents. Documents sharing more common words with similar frequencies (or TF-IDF scores) will have higher cosine similarity scores, indicating they are more similar in content.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec6-(f) Vector Space Models (VSMs).txt",
    "file_name": "lec6-(f) Vector Space Models (VSMs).txt",
    "filename_keywords": [
      "vsms",
      "models",
      "space",
      "lec6",
      "vector"
    ],
    "content_keywords": [
      "translating",
      "cosine similarity",
      "discovering",
      "strengths",
      "pakistan",
      "dot",
      "binary",
      "create vectors (document representation): represent",
      "most",
      "for",
      "assuming",
      "preprocessing",
      "vocabulary",
      "convert",
      "calculate term frequency",
      "study",
      "love",
      "tokenization",
      ", doc1) = 0/3 = 0\n\n * tf(",
      "bag",
      "in a document",
      "and",
      "investment bank",
      "bank",
      "generally",
      "running",
      "extracting",
      ", doc1) = 1/3\n\n * tf(",
      "documents",
      "run",
      "any",
      ", doc3) = 1/3\n\n * tf(",
      "lowercasing",
      "vector space model",
      "idf",
      "calculate similarity: use a similarity measure (lik",
      "let",
      ", doc2) = 1/2\n\n * tf(",
      "pakistan zindabad",
      "example",
      "clean the text (preprocessing):",
      "represent",
      "words",
      "vsms",
      "using",
      "let b",
      "calculate cosine similarity",
      "b\n\nlet a",
      "where",
      "the",
      ", doc2) = 0/2 = 0\n\n* doc3:",
      "can",
      "total",
      "use",
      "each",
      "formula",
      "(2 words)\n\n * tf(",
      "magnitude",
      "information retrieval",
      "break",
      "reduce",
      "compare",
      "calculate",
      "dot product",
      "search",
      "term frequency",
      "summarizing",
      "number",
      "get the text data: collect the documents, sentences",
      "finding",
      "natural language processing",
      "handles",
      "zindabad",
      "assigning",
      "classifying",
      "river bank",
      "build",
      "this",
      "build a vocabulary: create a comprehensive list of",
      "struggles",
      "vsm",
      ") that typically don",
      "appears in document",
      "studies",
      ", doc1) = 0/3 = 0\n\n* doc2:",
      "apple",
      "flexible",
      "language",
      "weaknesses",
      "how",
      "step",
      "collect",
      "create",
      ", doc3) = 0/3 = 0\n\n * tf(",
      "document representation",
      "i love pakistan",
      ", doc2) = 0/2 = 0\n\n * tf(",
      "scalable",
      "identifying",
      "search/query (information retrieval): for search ap",
      "build vocabulary",
      "inverse document frequency",
      "(3 words)\n\n * tf(",
      "stemming",
      ")\n\n* doc1:",
      "polysemy",
      "get",
      "effective",
      "remove",
      "interpretation",
      "stop",
      "i am pakistan",
      "lemmatization",
      "semantic",
      "nlp",
      "clean",
      "what",
      "vector space models"
    ],
    "technical_terms": [
      "translating",
      "cosine similarity",
      "discovering",
      "strengths",
      "pakistan",
      "dot",
      "binary",
      "most",
      "for",
      "assuming",
      "preprocessing",
      "vocabulary",
      "convert",
      "calculate term frequency",
      "tokenization",
      "bag",
      "bank",
      "generally",
      "extracting",
      "documents",
      "any",
      "lowercasing",
      "vector space model",
      "idf",
      "let",
      "pakistan zindabad",
      "example",
      "represent",
      "words",
      "vsms",
      "using",
      "let b",
      "calculate cosine similarity",
      "b\n\nlet a",
      "where",
      "the",
      "can",
      "total",
      "use",
      "each",
      "formula",
      "magnitude",
      "information retrieval",
      "break",
      "reduce",
      "compare",
      "calculate",
      "dot product",
      "search",
      "term frequency",
      "summarizing",
      "number",
      "finding",
      "natural language processing",
      "handles",
      "assigning",
      "classifying",
      "build",
      "this",
      "struggles",
      "vsm",
      "apple",
      "flexible",
      "language",
      "weaknesses",
      "how",
      "step",
      "collect",
      "create",
      "document representation",
      "scalable",
      "identifying",
      "build vocabulary",
      "inverse document frequency",
      "stemming",
      "polysemy",
      "get",
      "effective",
      "remove",
      "interpretation",
      "stop",
      "lemmatization",
      "semantic",
      "nlp",
      "clean",
      "what",
      "vector space models"
    ],
    "all_keywords": [
      "translating",
      "cosine similarity",
      "discovering",
      "strengths",
      "pakistan",
      "dot",
      "binary",
      "create vectors (document representation): represent",
      "most",
      "for",
      "assuming",
      "preprocessing",
      "vocabulary",
      "convert",
      "calculate term frequency",
      "study",
      "love",
      "tokenization",
      ", doc1) = 0/3 = 0\n\n * tf(",
      "bag",
      "in a document",
      "and",
      "investment bank",
      "bank",
      "generally",
      "running",
      "extracting",
      ", doc1) = 1/3\n\n * tf(",
      "documents",
      "run",
      "any",
      ", doc3) = 1/3\n\n * tf(",
      "lowercasing",
      "vector space model",
      "idf",
      "calculate similarity: use a similarity measure (lik",
      "let",
      ", doc2) = 1/2\n\n * tf(",
      "pakistan zindabad",
      "example",
      "clean the text (preprocessing):",
      "represent",
      "words",
      "vsms",
      "using",
      "let b",
      "calculate cosine similarity",
      "b\n\nlet a",
      "where",
      "the",
      ", doc2) = 0/2 = 0\n\n* doc3:",
      "can",
      "total",
      "use",
      "each",
      "formula",
      "(2 words)\n\n * tf(",
      "magnitude",
      "information retrieval",
      "break",
      "reduce",
      "compare",
      "calculate",
      "dot product",
      "space",
      "search",
      "term frequency",
      "summarizing",
      "number",
      "vector",
      "get the text data: collect the documents, sentences",
      "finding",
      "natural language processing",
      "handles",
      "zindabad",
      "assigning",
      "classifying",
      "river bank",
      "build",
      "this",
      "build a vocabulary: create a comprehensive list of",
      "struggles",
      "vsm",
      ") that typically don",
      "appears in document",
      "studies",
      ", doc1) = 0/3 = 0\n\n* doc2:",
      "apple",
      "flexible",
      "language",
      "weaknesses",
      "how",
      "step",
      "collect",
      "create",
      ", doc3) = 0/3 = 0\n\n * tf(",
      "document representation",
      "i love pakistan",
      ", doc2) = 0/2 = 0\n\n * tf(",
      "scalable",
      "lec6",
      "identifying",
      "search/query (information retrieval): for search ap",
      "build vocabulary",
      "inverse document frequency",
      "(3 words)\n\n * tf(",
      "stemming",
      ")\n\n* doc1:",
      "polysemy",
      "get",
      "effective",
      "models",
      "remove",
      "interpretation",
      "stop",
      "i am pakistan",
      "lemmatization",
      "semantic",
      "nlp",
      "clean",
      "what",
      "vector space models"
    ],
    "keyword_string": "translating cosine similarity discovering strengths pakistan dot binary create vectors (document representation): represent most for assuming preprocessing vocabulary convert calculate term frequency study love tokenization , doc1) = 0/3 = 0\n\n * tf( bag in a document and investment bank bank generally running extracting , doc1) = 1/3\n\n * tf( documents run any , doc3) = 1/3\n\n * tf( lowercasing vector space model idf calculate similarity: use a similarity measure (lik let , doc2) = 1/2\n\n * tf( pakistan zindabad example clean the text (preprocessing): represent words vsms using let b calculate cosine similarity b\n\nlet a where the , doc2) = 0/2 = 0\n\n* doc3: can total use each formula (2 words)\n\n * tf( magnitude information retrieval break reduce compare calculate dot product space search term frequency summarizing number vector get the text data: collect the documents, sentences finding natural language processing handles zindabad assigning classifying river bank build this build a vocabulary: create a comprehensive list of struggles vsm ) that typically don appears in document studies , doc1) = 0/3 = 0\n\n* doc2: apple flexible language weaknesses how step collect create , doc3) = 0/3 = 0\n\n * tf( document representation i love pakistan , doc2) = 0/2 = 0\n\n * tf( scalable lec6 identifying search/query (information retrieval): for search ap build vocabulary inverse document frequency (3 words)\n\n * tf( stemming )\n\n* doc1: polysemy get effective models remove interpretation stop i am pakistan lemmatization semantic nlp clean what vector space models",
    "token_count": 2684,
    "word_count": 1343,
    "sentence_count": 68,
    "paragraph_count": 118,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.5003725782414307,
    "avg_sentence_length": 19.75,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": true,
    "has_definitions": true,
    "content_type": "Technical, Structured, Definitions"
  },
  {
    "document_id": 143,
    "document_hash": "cc20a40eb5e4",
    "content": "What is it?\n\nDefinition of Question Answering (QA):\nQuestion Answering (QA) is a field of computer science that focuses on building systems capable of answering questions posed in natural language.\n\nExamples of QA systems:\n\nAskJeeves (a well-known early example)\n\nAnswerBus (an open-domain question answering system)\n\nIonaut, EasyAsk, AnswerLogic, AnswerFriend, Start, LCC, Quasm, Mulder, Webclopedia, etc.\n\nWhy use it?\n\nFrom AskJeeves: \"Search engines do not speak your language. They make you speak their language; a language that's strange, confusing, and includes words that no one is entirely sure of their meaning.\"\n\nQA engines attempt to let you ask your question the way you'd normally ask it.\n\nQA systems are beneficial for inexperienced users, simplifying the information retrieval process.\n\nThe goal is to move beyond simply finding a document to directly providing an answer (Document=Answer?).\n\nHow does it work?\n\nA typical Question Answering system involves several key components:\n\nNatural Language Processing (NLP):\n\nSemantic Processing: Understanding the meaning of words and sentences.\n\nSyntactic Processing: Analyzing the grammatical structure of sentences.\n\nParsing: Breaking down sentences into their grammatical components.\n\nKnowledge Base: A repository of structured information from which answers can be retrieved or inferred.\n\nAnswer Processing: The final stage where candidate answers are generated, ranked, and presented to the user.\n\nNatural Language Processing (NLP)\n\nQA Engines have unique processes related to NLP.\n\nSTART-Natural Language System (an example of an NLP system used in QA):\n\nParsing: Analyzing sentence structure.\n\nNatural Language Annotation: Tagging and labeling linguistic features.\n\nProcessing Component: Core logic for handling natural language.\n\nAnswer Processing\n\nConceptual Table Explanation (QA System Output Types):\nThis table outlines various Question Answering (QA) systems and the typical format of the output they provide as answers:\n\nAnswerBus: Provides answers in the form of Sentences.\n\nAskJeeves: Primarily returns relevant Documents.\n\nIONAUT: Delivers Passages (sections of text) as answers.\n\nLCC: Outputs answers as Sentences.\n\nMulder: Provides Extracted answers, implying precise snippets.\n\nQuASM: Returns Document blocks, which are larger chunks than passages.\n\nSTART: Outputs a Mixture of answer types.\n\nWebclopedia: Provides answers in the form of Sentences.\nThis table highlights the diversity in how different QA systems present their findings, ranging from full documents to specific sentences or extracted facts.\n\nAskJeeves\n\nAskJeeves (now Ask.com) was a popular early QA system with key features:\n\nIt had its own knowledge base and partnered with other sources to answer questions.\n\nIt catalogued previous questions to improve its understanding and response over time.\n\nIt utilized an answer processing engine, often generating responses based on question templates.\n\nAnswerBus\n\nConceptual Diagram Explanation (AnswerBus Flowchart):\nThe flowchart illustrates the process of the AnswerBus Question Answering system:\n\nUser Question: The process begins with a natural language question from the user.\n\nTranslated Question: The user's question is first translated or transformed into a more structured query.\n\nFrom the translated question, three parallel paths emerge:\n\nQuestion Type: The system analyzes the question to determine its type (e.g., factoid, list).\n\nSearch Engine Specific Query: A query optimized for traditional search engines is generated.\n\nMatching Words: Keywords or phrases are extracted for direct matching.\n\nHit Lists from Search Engines: Results (e.g., documents, passages) are retrieved from various search engines based on the generated queries.\n\nExtracted Sentences: Relevant sentences are extracted from the retrieved hit lists.\n\nCandidate Answers: From these extracted sentences, potential answers are identified.\n\nRanked Answers: Finally, the candidate answers are ranked by relevance and presented to the user.\nThis diagram shows AnswerBus as an open-domain QA system that leverages traditional search engines and then applies further processing to extract and rank specific answers from the search results.\n\nProblems (in Question Answering)\n\nChallenges in Question Answering include:\n\n\"How\" and \"Why\" questions: These often require deeper reasoning and causal understanding beyond simple fact retrieval.\n\n\"What\" questions:\n\n\"What happened?\": Requires understanding events and narratives.\n\n\"What did we do?\": Requires understanding actions and consequences.\n\nAnswer Quality:\n\nCorrectness: Ensuring the provided answer is factually accurate.\n\nAnswer Presentation: Presenting the answer clearly, concisely, and in an understandable format.\n\nCorrect? (From Webclopedia)\n\nExamples of humorous or difficult-to-answer questions from Webclopedia, highlighting challenges in QA:\n\nQuestion: Where do lobsters like to live?\n\nAnswer: on a Canadian airline (Incorrect, likely a result of misinterpretation or a factoid related to a specific event).\n\nQuestion: Where do hyenas live?\n\nAnswer: in Saudi Arabia (Plausible but incomplete).\n\nAnswer: in the back of pick-up trucks (Incorrect, likely a humorous or out-of-context misinterpretation).\n\nQuestion: Where are zebras most likely found?\n\nAnswer: near dumps (Incorrect, likely a misinterpretation of a specific text).\n\nAnswer: in the dictionary (True, but not the desired geographical answer for a \"where\" question, highlighting semantic understanding issues).\n\nQuestion: Why can't ostriches fly?\n\nAnswer: Because of American economic sanctions (Completely incorrect and absurd, demonstrating a failure in causal reasoning or semantic relevance).\n\nThese examples were collected by Ulf Hermjakob on November 29, 2001, showcasing the limitations of QA systems in dealing with common sense, ambiguity, and real-world knowledge at the time.",
    "enhanced_text": "[NLP] What is it?\n\nDefinition of Question Answering (QA):\nQuestion Answering (QA) is a field of computer science that focuses on building systems capable of answering questions posed in natural language.\n\nExamples of QA systems:\n\nAskJeeves (a well-known early example)\n\nAnswerBus (an open-domain question answering system)\n\nIonaut, EasyAsk, AnswerLogic, AnswerFriend, Start, LCC, Quasm, Mulder, Webclopedia, etc.\n\nWhy use it?\n\nFrom AskJeeves: \"Search engines do not speak your language. They make you speak their language; a language that's strange, confusing, and includes words that no one is entirely sure of their meaning.\"\n\nQA engines attempt to let you ask your question the way you'd normally ask it.\n\nQA systems are beneficial for inexperienced users, simplifying the information retrieval process.\n\nThe goal is to move beyond simply finding a document to directly providing an answer (Document=Answer?).\n\nHow does it work?\n\nA typical Question Answering system involves several key components:\n\nNatural Language Processing (NLP):\n\nSemantic Processing: Understanding the meaning of words and sentences.\n\nSyntactic Processing: Analyzing the grammatical structure of sentences.\n\nParsing: Breaking down sentences into their grammatical components.\n\nKnowledge Base: A repository of structured information from which answers can be retrieved or inferred.\n\nAnswer Processing: The final stage where candidate answers are generated, ranked, and presented to the user.\n\nNatural Language Processing (NLP)\n\nQA Engines have unique processes related to NLP.\n\nSTART-Natural Language System (an example of an NLP system used in QA):\n\nParsing: Analyzing sentence structure.\n\nNatural Language Annotation: Tagging and labeling linguistic features.\n\nProcessing Component: Core logic for handling natural language.\n\nAnswer Processing\n\nConceptual Table Explanation (QA System Output Types):\nThis table outlines various Question Answering (QA) systems and the typical format of the output they provide as answers:\n\nAnswerBus: Provides answers in the form of Sentences.\n\nAskJeeves: Primarily returns relevant Documents.\n\nIONAUT: Delivers Passages (sections of text) as answers.\n\nLCC: Outputs answers as Sentences.\n\nMulder: Provides Extracted answers, implying precise snippets.\n\nQuASM: Returns Document blocks, which are larger chunks than passages.\n\nSTART: Outputs a Mixture of answer types.\n\nWebclopedia: Provides answers in the form of Sentences.\nThis table highlights the diversity in how different QA systems present their findings, ranging from full documents to specific sentences or extracted facts.\n\nAskJeeves\n\nAskJeeves (now Ask.com) was a popular early QA system with key features:\n\nIt had its own knowledge base and partnered with other sources to answer questions.\n\nIt catalogued previous questions to improve its understanding and response over time.\n\nIt utilized an answer processing engine, often generating responses based on question templates.\n\nAnswerBus\n\nConceptual Diagram Explanation (AnswerBus Flowchart):\nThe flowchart illustrates the process of the AnswerBus Question Answering system:\n\nUser Question: The process begins with a natural language question from the user.\n\nTranslated Question: The user's question is first translated or transformed into a more structured query.\n\nFrom the translated question, three parallel paths emerge:\n\nQuestion Type: The system analyzes the question to determine its type (e.g., factoid, list).\n\nSearch Engine Specific Query: A query optimized for traditional search engines is generated.\n\nMatching Words: Keywords or phrases are extracted for direct matching.\n\nHit Lists from Search Engines: Results (e.g., documents, passages) are retrieved from various search engines based on the generated queries.\n\nExtracted Sentences: Relevant sentences are extracted from the retrieved hit lists.\n\nCandidate Answers: From these extracted sentences, potential answers are identified.\n\nRanked Answers: Finally, the candidate answers are ranked by relevance and presented to the user.\nThis diagram shows AnswerBus as an open-domain QA system that leverages traditional search engines and then applies further processing to extract and rank specific answers from the search results.\n\nProblems (in Question Answering)\n\nChallenges in Question Answering include:\n\n\"How\" and \"Why\" questions: These often require deeper reasoning and causal understanding beyond simple fact retrieval.\n\n\"What\" questions:\n\n\"What happened?\": Requires understanding events and narratives.\n\n\"What did we do?\": Requires understanding actions and consequences.\n\nAnswer Quality:\n\nCorrectness: Ensuring the provided answer is factually accurate.\n\nAnswer Presentation: Presenting the answer clearly, concisely, and in an understandable format.\n\nCorrect? (From Webclopedia)\n\nExamples of humorous or difficult-to-answer questions from Webclopedia, highlighting challenges in QA:\n\nQuestion: Where do lobsters like to live?\n\nAnswer: on a Canadian airline (Incorrect, likely a result of misinterpretation or a factoid related to a specific event).\n\nQuestion: Where do hyenas live?\n\nAnswer: in Saudi Arabia (Plausible but incomplete).\n\nAnswer: in the back of pick-up trucks (Incorrect, likely a humorous or out-of-context misinterpretation).\n\nQuestion: Where are zebras most likely found?\n\nAnswer: near dumps (Incorrect, likely a misinterpretation of a specific text).\n\nAnswer: in the dictionary (True, but not the desired geographical answer for a \"where\" question, highlighting semantic understanding issues).\n\nQuestion: Why can't ostriches fly?\n\nAnswer: Because of American economic sanctions (Completely incorrect and absurd, demonstrating a failure in causal reasoning or semantic relevance).\n\nThese examples were collected by Ulf Hermjakob on November 29, 2001, showcasing the limitations of QA systems in dealing with common sense, ambiguity, and real-world knowledge at the time.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec7-(a).txt",
    "file_name": "lec7-(a).txt",
    "filename_keywords": [
      "lec7"
    ],
    "content_keywords": [
      "syntactic processing",
      "user question",
      "ask",
      "hit lists",
      "these",
      "outputs",
      "candidate answers",
      "mulder",
      "answer processing",
      "askjeeves",
      "answer",
      "matching words",
      "qa engines",
      "core",
      "presenting",
      "from",
      "why",
      "document",
      "completely",
      "true",
      "what happened?",
      "what did we do?",
      "delivers passages",
      "documents",
      "extracted sentences",
      "question type",
      "provides extracted",
      "from webclopedia",
      "november",
      "answerbus",
      "plausible",
      "askjeeves\n\naskjeeves",
      "processing component",
      "american",
      "answerlogic",
      "semantic processing",
      "answerbus\n\nconceptual diagram explanation",
      "keywords",
      "where",
      "answerfriend",
      "understanding",
      "the",
      "incorrect",
      "answerbus flowchart",
      "easyask",
      "lcc",
      "because",
      "question answering",
      "relevant",
      "knowledge base",
      "parsing",
      "search",
      "ensuring",
      "search engines",
      "results",
      "they",
      "natural language processing",
      "requires",
      "tagging",
      "provides",
      "analyzing",
      "this",
      "question",
      "challenges",
      "start",
      "breaking",
      "correct",
      "natural language system",
      "sentences",
      "examples",
      "problems",
      "how",
      "mixture",
      "finally",
      "answerbus question answering",
      "ranked answers",
      "answer processing\n\nconceptual table explanation",
      "answer quality",
      "ionaut",
      "webclopedia",
      "translated question",
      "search engine specific query",
      "ulf hermjakob",
      "natural language annotation",
      "returns document",
      "saudi arabia",
      "quasm",
      "nlp",
      "from askjeeves",
      "primarily",
      "definition",
      "qa system output types",
      "answer presentation",
      "canadian",
      "what",
      "correctness"
    ],
    "technical_terms": [
      "syntactic processing",
      "user question",
      "ask",
      "hit lists",
      "these",
      "outputs",
      "candidate answers",
      "mulder",
      "answer processing",
      "askjeeves",
      "answer",
      "matching words",
      "qa engines",
      "core",
      "presenting",
      "from",
      "why",
      "document",
      "completely",
      "true",
      "delivers passages",
      "documents",
      "extracted sentences",
      "question type",
      "provides extracted",
      "from webclopedia",
      "november",
      "answerbus",
      "plausible",
      "askjeeves\n\naskjeeves",
      "processing component",
      "american",
      "answerlogic",
      "semantic processing",
      "answerbus\n\nconceptual diagram explanation",
      "keywords",
      "where",
      "answerfriend",
      "understanding",
      "the",
      "incorrect",
      "answerbus flowchart",
      "easyask",
      "lcc",
      "because",
      "question answering",
      "relevant",
      "knowledge base",
      "parsing",
      "search",
      "ensuring",
      "search engines",
      "results",
      "they",
      "natural language processing",
      "requires",
      "tagging",
      "provides",
      "analyzing",
      "this",
      "question",
      "challenges",
      "start",
      "breaking",
      "correct",
      "natural language system",
      "sentences",
      "examples",
      "problems",
      "how",
      "mixture",
      "finally",
      "answerbus question answering",
      "ranked answers",
      "answer processing\n\nconceptual table explanation",
      "answer quality",
      "ionaut",
      "webclopedia",
      "translated question",
      "search engine specific query",
      "ulf hermjakob",
      "natural language annotation",
      "returns document",
      "saudi arabia",
      "quasm",
      "nlp",
      "from askjeeves",
      "primarily",
      "definition",
      "qa system output types",
      "answer presentation",
      "canadian",
      "what",
      "correctness"
    ],
    "all_keywords": [
      "syntactic processing",
      "user question",
      "ask",
      "hit lists",
      "these",
      "outputs",
      "candidate answers",
      "mulder",
      "answer processing",
      "askjeeves",
      "answer",
      "matching words",
      "qa engines",
      "core",
      "presenting",
      "from",
      "why",
      "document",
      "completely",
      "true",
      "what happened?",
      "what did we do?",
      "delivers passages",
      "documents",
      "extracted sentences",
      "question type",
      "provides extracted",
      "from webclopedia",
      "lec7",
      "november",
      "answerbus",
      "plausible",
      "askjeeves\n\naskjeeves",
      "processing component",
      "american",
      "answerlogic",
      "semantic processing",
      "answerbus\n\nconceptual diagram explanation",
      "keywords",
      "where",
      "answerfriend",
      "understanding",
      "the",
      "incorrect",
      "answerbus flowchart",
      "easyask",
      "lcc",
      "because",
      "question answering",
      "relevant",
      "knowledge base",
      "parsing",
      "search",
      "ensuring",
      "search engines",
      "results",
      "they",
      "natural language processing",
      "requires",
      "tagging",
      "provides",
      "analyzing",
      "this",
      "question",
      "challenges",
      "start",
      "breaking",
      "correct",
      "natural language system",
      "sentences",
      "examples",
      "problems",
      "how",
      "mixture",
      "finally",
      "answerbus question answering",
      "ranked answers",
      "answer processing\n\nconceptual table explanation",
      "answer quality",
      "ionaut",
      "webclopedia",
      "translated question",
      "search engine specific query",
      "ulf hermjakob",
      "natural language annotation",
      "returns document",
      "saudi arabia",
      "quasm",
      "nlp",
      "from askjeeves",
      "primarily",
      "definition",
      "qa system output types",
      "answer presentation",
      "canadian",
      "what",
      "correctness"
    ],
    "keyword_string": "syntactic processing user question ask hit lists these outputs candidate answers mulder answer processing askjeeves answer matching words qa engines core presenting from why document completely true what happened? what did we do? delivers passages documents extracted sentences question type provides extracted from webclopedia lec7 november answerbus plausible askjeeves\n\naskjeeves processing component american answerlogic semantic processing answerbus\n\nconceptual diagram explanation keywords where answerfriend understanding the incorrect answerbus flowchart easyask lcc because question answering relevant knowledge base parsing search ensuring search engines results they natural language processing requires tagging provides analyzing this question challenges start breaking correct natural language system sentences examples problems how mixture finally answerbus question answering ranked answers answer processing\n\nconceptual table explanation answer quality ionaut webclopedia translated question search engine specific query ulf hermjakob natural language annotation returns document saudi arabia quasm nlp from askjeeves primarily definition qa system output types answer presentation canadian what correctness",
    "token_count": 1179,
    "word_count": 808,
    "sentence_count": 60,
    "paragraph_count": 74,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6853265479219678,
    "avg_sentence_length": 13.466666666666667,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  },
  {
    "document_id": 144,
    "document_hash": "6addb3f4cbaa",
    "content": "(TREC) -- Text Retrieval Conference\n\nTREC is a yearly information retrieval competition.\n\nBegan in 1992, with the Question Answering (QA) track introduced in 1999.\n\nIts purpose is to encourage research into systems that return direct answers rather than just lists of documents.\n\nQuestions (Q's) are typically \"open domain\" (can be about any topic) and \"closed class\" (expect a specific type of answer, like a name, date, or number).\n\nAnswers (A's) are constrained to be less than 50 characters and usually entities or noun phrases.\n\n(TREC) -- Text Retrieval Conference (Continued)\n\nTREC QA Track in 2001:\n\nIncluded 500 questions.\n\nSome answers were \"nil\" (no answer found) or presented large difficulty.\n\nMany questions were \"definition questions\" (e.g., \"What is X?\").\n\nQA list tasks (Example):\n\n\"Name 4 cities that have a 'Shubert' theater.\" (Requires extracting multiple entities of a specific type).\n\nQA context tasks (Examples):\n\n\"How many species of spiders are there?\"\n\n\"How many are poisonous to humans?\"\n\n\"What percentage of spider bites in the US are fatal?\"\nThese questions require extracting specific numerical or factual information, often requiring aggregation or calculation from text.\n\nExample Questions and Results\n\nExample Questions and the systems that would attempt to answer them:\n\nQuestion: \"What river in the US is known as the Big Muddy?\"\n\nSystems: AskJeeves, AnswerBus, Google\n\nQuestion: \"What person’s head is on a dime?\"\n\nSystems: AskJeeves, AnswerBus, AltaVista\n\nQuestion: \"Show some paintings by Claude Monet\"\n\nSystem: START\n\nLooking Ahead\n\nThe field of Question Answering shows:\n\nStrong User Demand: Users increasingly expect direct answers.\n\nEnormous Interest in the Problem: Significant research and development focus.\n\nSuccesses: Continuous advancements in QA system capabilities.\n\nSources\n\nAskMSR: Question Answering Using the Worldwide Web\n\nAuthors: Michele Banko, Eric Brill, Susan Dumais, Jimmy Lin\n\nPaper URL: http://www.ai.mit.edu/people/jimmylin/publications/Banko-etal-AAAI02.pdf\n\nPublished in: Proceedings of 2002 AAAI SYMPOSIUM on Mining Answers from Text and Knowledge Bases, March 2002\n\nWeb Question Answering: Is More Always Better?\n\nAuthors: Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, Andrew Ng\n\nPaper URL: http://research.microsoft.com/~sdumais/SIGIR2002-QA-Submit-Conf.pdf\n\nAnswerBus\n\nWebsite: www.answerbus.com\n\nRelated resources:\n\nhttp://misshoover.si.umich.edu/~zzheng/qa-new/\n\nhttp://www2002.org/CDROM/poster/203/\n\nAskJeeves\n\nAbout page: http://www.ask.co.uk/docs/about/what_is.asp\n\nWebclopedia\n\nTREC paper: http://trec.nist.gov/pubs/trec9/papers/webclopedia.pdf\n\nProject page: http://www.isi.edu/natural-language/projects/webclopedia/\n\nSTART\n\nProject page: http://www.ai.mit.edu/projects/infolab/ailab.html\n\nText Retrieval Conference (TREC)\n\nIntroduction (Revisited)\n\nNatural Language Processing (NLP)\n\nField of Computer Science.\n\nIncludes tasks like Sentiment Analysis, Word prediction, Translation, and Question Answering.\n\nQuestion Answering (QA)\n\nAn important sub-domain of NLP.\n\nApproaches used to develop QA Systems:\n\nConventional (Machine learning & IR)\n\nDeep learning\n\nAnswer Selection (a major component of QA systems) is a focus of research.\n\nTypes of QA Systems\n\nConceptual Diagram Explanation (Categories of QA Systems):\nThe diagram categorizes QA systems into three main types based on their domain, approach, and underlying mechanisms:\n\nDomain-Based Categorization:\n\nClosed Domain: QA systems designed to answer questions within a very specific, limited topic area (e.g., medical QA system).\n\nOpen Domain: QA systems that can answer questions about a wide range of topics, often leveraging large text corpora like the web.\n\nLinguistic/Pattern-Based Approaches:\n\nLinguistic: Systems heavily relying on linguistic rules, parsing, and semantic analysis.\n\nStatistical: Systems employing statistical models derived from large datasets.\n\nPattern Matching: Systems that identify answers by matching predefined patterns in text.\n\nRetrieval and Reasoning-Based Approaches:\n\nIR/IE based QA System: Systems that primarily use Information Retrieval (IR) to find relevant documents and Information Extraction (IE) to extract answers from them.\n\nRestricted domain QA system: Similar to closed domain, emphasizing a narrow focus for better accuracy.\n\nRule based QA system: Systems that use a set of predefined rules and ontologies to derive answers.\n\nCategories of QA systems\n\nQA Systems based on NLP and IR:\n\nThese systems utilize syntax processing, Named Entity Tagging, and Information Retrieval (IR) techniques.\n\nThey often use free text documents from open-domain sources as their data resource.\n\nThey typically deal with \"wh-type\" questions (who, what, when, where, why) and provide query responses in the form of extracted snippets from documents.\n\nQA Systems Reasoning with NLP:\n\nThese systems employ Semantic Analysis or high-level reasoning techniques.\n\nThey perform tasks on Knowledge Base data resources and are often domain-oriented.\n\nThey are not restricted to \"wh-type\" questions and can provide synthesized responses that go beyond simple text extraction.\n\nQAS Architecture\n\nConceptual Diagram Explanation (QAS Architecture):\nThe diagram illustrates a common architecture for a Question Answering System (QAS), highlighting its main components and their sub-components. The architecture generally involves a pipeline of processing stages:\n\nHigh-Level Components (Left and Right Bubbles):\n\nQuestion Processing (1): Handles the user's input question.\n\nDocument Processing (2): Manages the collection of documents (corpus) from which answers are sought.\n\nAnswer Processing (3): Generates and refines the final answers.\n\nDetailed Sub-Components (Bottom Panel):\n\nQuestion Processing / Analysis: This phase analyzes the input query to understand its intent and identify key elements.\n\nQuestion Analysis: Initial understanding of the question.\n\nQuestion Classification: Categorizing the question type (e.g., factoid, list).\n\nQuestion Reformulation: Rephrasing the question for better retrieval.\n\nDocument Processing / Information Retrieval: This phase finds relevant information from the corpus.\n\nInformation Retrieval: Retrieving documents or passages relevant to the processed question.\n\nParagraph Filtering: Selecting paragraphs or smaller units likely to contain answers.\n\nParagraph Ordering: Ranking the selected paragraphs by relevance.\n\nAnswer Processing / Answer Identification: This final phase extracts and validates the answer.\n\nAnswer Identification: Pinpointing specific candidate answers within the retrieved text.\n\nAnswer Extraction: Extracting the actual answer text.\n\nAnswer Validation: Verifying the correctness and quality of the extracted answer.\n\nThis layered architecture shows a typical flow from understanding a user's question to delivering a verified answer, emphasizing that QA is a complex task involving multiple NLP and IR sub-tasks.\n\nQuestion Types\n\nQuestion types commonly handled by QA systems:\n\nFactoid:\n\nSimple and fact-based.\n\nGenerally starts with \"Wh-\" words (Who, What, When, Where).\n\nList:\n\nRequires the answer to be in a list of facts or entities.\n\nCasual:\n\nRequires explanations, reasons, or elaborations about an entity/event.\n\nTypically \"How\" or \"Why\" type questions.\n\nConfirmation:\n\nRequires answers in the form of \"yes\" or \"no\".\n\nSystems need inference mechanisms, world knowledge, and common sense reasoning to reply.\n\nHypothetical:\n\nRelated to any hypothetical event.\n\nGenerally begins with \"what would happen if\".\n\nThere are no specific correct answers for these questions, making them challenging for QA.\n\nAnswer Selection\n\nAnswer selection is one of the important tasks of Question Answering.\n\nIt deals with the selection of the correct and suitable answer from a set of candidate answers retrieved based upon a user query in natural language.\n\nResearch work has been done in Answer selection using:\n\nConventional Machine learning & IR methods\n\nDeep learning methods\n\nConventional Approaches for Answer Selection\n\nConventional approaches for answer selection use linguistic tools, feature engineering, and external resources. These include:\n\nTree-edit distance: Measures the similarity between linguistic parse trees.\n\nSupport Vector Machine (SVM): A supervised machine learning model used for classification.\n\nLexical semantic features calculation using WordNet: Leveraging a lexical database to understand word meanings and relationships.\n\nMatching between the words using their semantic relevance: Comparing words based on their meaning rather than just exact matches.\n\nDeep Learning\n\nDeep Learning is a sub-field of Machine Learning that mimics the human brain's structure and function.\n\nNeural Networks are a core component of deep learning. Common types used in NLP for tasks like answer selection include:\n\nRecurrent Neural Network (RNN):\n\nLong-Short Term Memory (LSTM)\n\nBidirectional LSTM (BiLSTM)\n\nConvolutional Neural Network (CNN):\n\nAttention-based CNN\n\nLiterature Review (Deep Learning Approaches for Answer Selection)\n\nConceptual Table Explanation (Literature Review - Page 26):\nThis table summarizes several research papers on applying Deep Learning (DL) approaches to Answer Selection, providing an overview of the author(s), title, DL approach used, dataset, question types addressed, and the reported results (often in metrics like MAP, MRR, or Precision).\n\nFeng et al. (2015): Applied CNN to InsuranceQA dataset for Non-factoid answer selection, achieving Precision 62.8.\n\nWang et al. (2015): Used Stacked BiLSTM on TREC QA dataset for Factoid questions, reporting MAP 0.7134 and MRR 0.7913.\n\nLei et al. (2014): Employed CNN on TREC QA for Factoid answer selection, with results MAP 0.7113 and MRR 0.7846.\n\nTan et al. (2015): Utilized BiLSTM then CNN (with Attention) on TREC QA & InsuranceQA for Non-factoid questions, yielding MAP 72.79 and MRR 82.40.\n\nWang et al. (2016): Developed Inner Attention based Recurrent Neural Networks (RNN), tested on WikiQA and TREC QA, with results MAP 0.7341/0.7369 and MRR 0.7418/0.8208.\n\nNie et al. (2017): Implemented an Attention-based encoder-decoder model (BiLSTM) on TREC QA for Factoid questions, reporting MAP 0.7261 and MRR 0.8018.\n\nThis table demonstrates the evolution and effectiveness of different neural network architectures for improving answer selection in QA systems across various datasets and question complexities.\n\nLiterature Review (Continued)\n\nConceptual Table Explanation (Literature Review - Page 27):\nThis table continues the summary of deep learning literature in Question Answering, focusing on more recent work and hybrid approaches.\n\nShao et al. (2019): Explored Transformer-Based Neural Networks (BiLSTM) for answer selection in question answering, using WikiQA data. Reported MAP 0.6941 and MRR 0.7077.\n\nWang et al. (2017): Proposed a Hybrid Framework for Text Modeling with Convolutional RNN (Conv-RNN), evaluated on WikiQA and InsuranceQA. Achieved Accuracy 71.7, MAP 0.7427, and MRR 0.7504.\n\nT. Shao et al. (2019): Presented Collaborative Learning for Answer Selection using CNN and BiLSTM in parallel on Insurance QA. Reported MAP 0.7219 and MRR 0.6756.\n\nThe section also includes a reference to a paper on putting QA systems into practice:\n\nB. Kratzwald and S. Feuerriegel, \"Putting Question-Answering Systems into Practice: Transfer Learning for Efficient Domain Customization,\" ACM Trans. Manage. Inf. Syst., vol. 9, pp. 15:1-15:20, Feb. 2019. This highlights the practical challenges and solutions, such as transfer learning for domain customization, in deploying QA systems.",
    "enhanced_text": "[NLP] (TREC) -- Text Retrieval Conference\n\nTREC is a yearly information retrieval competition.\n\nBegan in 1992, with the Question Answering (QA) track introduced in 1999.\n\nIts purpose is to encourage research into systems that return direct answers rather than just lists of documents.\n\nQuestions (Q's) are typically \"open domain\" (can be about any topic) and \"closed class\" (expect a specific type of answer, like a name, date, or number).\n\nAnswers (A's) are constrained to be less than 50 characters and usually entities or noun phrases.\n\n(TREC) -- Text Retrieval Conference (Continued)\n\nTREC QA Track in 2001:\n\nIncluded 500 questions.\n\nSome answers were \"nil\" (no answer found) or presented large difficulty.\n\nMany questions were \"definition questions\" (e.g., \"What is X?\").\n\nQA list tasks (Example):\n\n\"Name 4 cities that have a 'Shubert' theater.\" (Requires extracting multiple entities of a specific type).\n\nQA context tasks (Examples):\n\n\"How many species of spiders are there?\"\n\n\"How many are poisonous to humans?\"\n\n\"What percentage of spider bites in the US are fatal?\"\nThese questions require extracting specific numerical or factual information, often requiring aggregation or calculation from text.\n\nExample Questions and Results\n\nExample Questions and the systems that would attempt to answer them:\n\nQuestion: \"What river in the US is known as the Big Muddy?\"\n\nSystems: AskJeeves, AnswerBus, Google\n\nQuestion: \"What person’s head is on a dime?\"\n\nSystems: AskJeeves, AnswerBus, AltaVista\n\nQuestion: \"Show some paintings by Claude Monet\"\n\nSystem: START\n\nLooking Ahead\n\nThe field of Question Answering shows:\n\nStrong User Demand: Users increasingly expect direct answers.\n\nEnormous Interest in the Problem: Significant research and development focus.\n\nSuccesses: Continuous advancements in QA system capabilities.\n\nSources\n\nAskMSR: Question Answering Using the Worldwide Web\n\nAuthors: Michele Banko, Eric Brill, Susan Dumais, Jimmy Lin\n\nPaper URL: http://www.ai.mit.edu/people/jimmylin/publications/Banko-etal-AAAI02.pdf\n\nPublished in: Proceedings of 2002 AAAI SYMPOSIUM on Mining Answers from Text and Knowledge Bases, March 2002\n\nWeb Question Answering: Is More Always Better?\n\nAuthors: Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, Andrew Ng\n\nPaper URL: http://research.microsoft.com/~sdumais/SIGIR2002-QA-Submit-Conf.pdf\n\nAnswerBus\n\nWebsite: www.answerbus.com\n\nRelated resources:\n\nhttp://misshoover.si.umich.edu/~zzheng/qa-new/\n\nhttp://www2002.org/CDROM/poster/203/\n\nAskJeeves\n\nAbout page: http://www.ask.co.uk/docs/about/what_is.asp\n\nWebclopedia\n\nTREC paper: http://trec.nist.gov/pubs/trec9/papers/webclopedia.pdf\n\nProject page: http://www.isi.edu/natural-language/projects/webclopedia/\n\nSTART\n\nProject page: http://www.ai.mit.edu/projects/infolab/ailab.html\n\nText Retrieval Conference (TREC)\n\nIntroduction (Revisited)\n\nNatural Language Processing (NLP)\n\nField of Computer Science.\n\nIncludes tasks like Sentiment Analysis, Word prediction, Translation, and Question Answering.\n\nQuestion Answering (QA)\n\nAn important sub-domain of NLP.\n\nApproaches used to develop QA Systems:\n\nConventional (Machine learning & IR)\n\nDeep learning\n\nAnswer Selection (a major component of QA systems) is a focus of research.\n\nTypes of QA Systems\n\nConceptual Diagram Explanation (Categories of QA Systems):\nThe diagram categorizes QA systems into three main types based on their domain, approach, and underlying mechanisms:\n\nDomain-Based Categorization:\n\nClosed Domain: QA systems designed to answer questions within a very specific, limited topic area (e.g., medical QA system).\n\nOpen Domain: QA systems that can answer questions about a wide range of topics, often leveraging large text corpora like the web.\n\nLinguistic/Pattern-Based Approaches:\n\nLinguistic: Systems heavily relying on linguistic rules, parsing, and semantic analysis.\n\nStatistical: Systems employing statistical models derived from large datasets.\n\nPattern Matching: Systems that identify answers by matching predefined patterns in text.\n\nRetrieval and Reasoning-Based Approaches:\n\nIR/IE based QA System: Systems that primarily use Information Retrieval (IR) to find relevant documents and Information Extraction (IE) to extract answers from them.\n\nRestricted domain QA system: Similar to closed domain, emphasizing a narrow focus for better accuracy.\n\nRule based QA system: Systems that use a set of predefined rules and ontologies to derive answers.\n\nCategories of QA systems\n\nQA Systems based on NLP and IR:\n\nThese systems utilize syntax processing, Named Entity Tagging, and Information Retrieval (IR) techniques.\n\nThey often use free text documents from open-domain sources as their data resource.\n\nThey typically deal with \"wh-type\" questions (who, what, when, where, why) and provide query responses in the form of extracted snippets from documents.\n\nQA Systems Reasoning with NLP:\n\nThese systems employ Semantic Analysis or high-level reasoning techniques.\n\nThey perform tasks on Knowledge Base data resources and are often domain-oriented.\n\nThey are not restricted to \"wh-type\" questions and can provide synthesized responses that go beyond simple text extraction.\n\nQAS Architecture\n\nConceptual Diagram Explanation (QAS Architecture):\nThe diagram illustrates a common architecture for a Question Answering System (QAS), highlighting its main components and their sub-components. The architecture generally involves a pipeline of processing stages:\n\nHigh-Level Components (Left and Right Bubbles):\n\nQuestion Processing (1): Handles the user's input question.\n\nDocument Processing (2): Manages the collection of documents (corpus) from which answers are sought.\n\nAnswer Processing (3): Generates and refines the final answers.\n\nDetailed Sub-Components (Bottom Panel):\n\nQuestion Processing / Analysis: This phase analyzes the input query to understand its intent and identify key elements.\n\nQuestion Analysis: Initial understanding of the question.\n\nQuestion Classification: Categorizing the question type (e.g., factoid, list).\n\nQuestion Reformulation: Rephrasing the question for better retrieval.\n\nDocument Processing / Information Retrieval: This phase finds relevant information from the corpus.\n\nInformation Retrieval: Retrieving documents or passages relevant to the processed question.\n\nParagraph Filtering: Selecting paragraphs or smaller units likely to contain answers.\n\nParagraph Ordering: Ranking the selected paragraphs by relevance.\n\nAnswer Processing / Answer Identification: This final phase extracts and validates the answer.\n\nAnswer Identification: Pinpointing specific candidate answers within the retrieved text.\n\nAnswer Extraction: Extracting the actual answer text.\n\nAnswer Validation: Verifying the correctness and quality of the extracted answer.\n\nThis layered architecture shows a typical flow from understanding a user's question to delivering a verified answer, emphasizing that QA is a complex task involving multiple NLP and IR sub-tasks.\n\nQuestion Types\n\nQuestion types commonly handled by QA systems:\n\nFactoid:\n\nSimple and fact-based.\n\nGenerally starts with \"Wh-\" words (Who, What, When, Where).\n\nList:\n\nRequires the answer to be in a list of facts or entities.\n\nCasual:\n\nRequires explanations, reasons, or elaborations about an entity/event.\n\nTypically \"How\" or \"Why\" type questions.\n\nConfirmation:\n\nRequires answers in the form of \"yes\" or \"no\".\n\nSystems need inference mechanisms, world knowledge, and common sense reasoning to reply.\n\nHypothetical:\n\nRelated to any hypothetical event.\n\nGenerally begins with \"what would happen if\".\n\nThere are no specific correct answers for these questions, making them challenging for QA.\n\nAnswer Selection\n\nAnswer selection is one of the important tasks of Question Answering.\n\nIt deals with the selection of the correct and suitable answer from a set of candidate answers retrieved based upon a user query in natural language.\n\nResearch work has been done in Answer selection using:\n\nConventional Machine learning & IR methods\n\nDeep learning methods\n\nConventional Approaches for Answer Selection\n\nConventional approaches for answer selection use linguistic tools, feature engineering, and external resources. These include:\n\nTree-edit distance: Measures the similarity between linguistic parse trees.\n\nSupport Vector Machine (SVM): A supervised machine learning model used for classification.\n\nLexical semantic features calculation using WordNet: Leveraging a lexical database to understand word meanings and relationships.\n\nMatching between the words using their semantic relevance: Comparing words based on their meaning rather than just exact matches.\n\nDeep Learning\n\nDeep Learning is a sub-field of Machine Learning that mimics the human brain's structure and function.\n\nNeural Networks are a core component of deep learning. Common types used in NLP for tasks like answer selection include:\n\nRecurrent Neural Network (RNN):\n\nLong-Short Term Memory (LSTM)\n\nBidirectional LSTM (BiLSTM)\n\nConvolutional Neural Network (CNN):\n\nAttention-based CNN\n\nLiterature Review (Deep Learning Approaches for Answer Selection)\n\nConceptual Table Explanation (Literature Review - Page 26):\nThis table summarizes several research papers on applying Deep Learning (DL) approaches to Answer Selection, providing an overview of the author(s), title, DL approach used, dataset, question types addressed, and the reported results (often in metrics like MAP, MRR, or Precision).\n\nFeng et al. (2015): Applied CNN to InsuranceQA dataset for Non-factoid answer selection, achieving Precision 62.8.\n\nWang et al. (2015): Used Stacked BiLSTM on TREC QA dataset for Factoid questions, reporting MAP 0.7134 and MRR 0.7913.\n\nLei et al. (2014): Employed CNN on TREC QA for Factoid answer selection, with results MAP 0.7113 and MRR 0.7846.\n\nTan et al. (2015): Utilized BiLSTM then CNN (with Attention) on TREC QA & InsuranceQA for Non-factoid questions, yielding MAP 72.79 and MRR 82.40.\n\nWang et al. (2016): Developed Inner Attention based Recurrent Neural Networks (RNN), tested on WikiQA and TREC QA, with results MAP 0.7341/0.7369 and MRR 0.7418/0.8208.\n\nNie et al. (2017): Implemented an Attention-based encoder-decoder model (BiLSTM) on TREC QA for Factoid questions, reporting MAP 0.7261 and MRR 0.8018.\n\nThis table demonstrates the evolution and effectiveness of different neural network architectures for improving answer selection in QA systems across various datasets and question complexities.\n\nLiterature Review (Continued)\n\nConceptual Table Explanation (Literature Review - Page 27):\nThis table continues the summary of deep learning literature in Question Answering, focusing on more recent work and hybrid approaches.\n\nShao et al. (2019): Explored Transformer-Based Neural Networks (BiLSTM) for answer selection in question answering, using WikiQA data. Reported MAP 0.6941 and MRR 0.7077.\n\nWang et al. (2017): Proposed a Hybrid Framework for Text Modeling with Convolutional RNN (Conv-RNN), evaluated on WikiQA and InsuranceQA. Achieved Accuracy 71.7, MAP 0.7427, and MRR 0.7504.\n\nT. Shao et al. (2019): Presented Collaborative Learning for Answer Selection using CNN and BiLSTM in parallel on Insurance QA. Reported MAP 0.7219 and MRR 0.6756.\n\nThe section also includes a reference to a paper on putting QA systems into practice:\n\nB. Kratzwald and S. Feuerriegel, \"Putting Question-Answering Systems into Practice: Transfer Learning for Efficient Domain Customization,\" ACM Trans. Manage. Inf. Syst., vol. 9, pp. 15:1-15:20, Feb. 2019. This highlights the practical challenges and solutions, such as transfer learning for domain customization, in deploying QA systems.",
    "category": "NLP",
    "source_file": "documents\\NLP_text_files\\lec7-(b).txt",
    "file_name": "lec7-(b).txt",
    "filename_keywords": [
      "lec7"
    ],
    "content_keywords": [
      "rule",
      "simple",
      "askjeeves\n\nabout",
      "answer selection",
      "practice",
      "matching",
      "who",
      "qa systems",
      "convolutional neural network",
      "convolutional rnn",
      "these",
      "conventional machine",
      "support vector machine",
      "similar",
      "recurrent neural networks",
      "significant",
      "computer science",
      "worldwide web\n\nauthors",
      "measures",
      "feb",
      "retrieving",
      "eric brill",
      "information extraction",
      "conceptual table explanation",
      "utilized bilstm",
      "confirmation",
      "typically",
      "lexical",
      "reported map",
      "enormous interest",
      "answer processing",
      "introduction",
      "factoid",
      "question answering system",
      "question processing",
      "cnn\n\nliterature review",
      "some",
      "askjeeves",
      "bidirectional lstm",
      "answerbus\n\nwebsite",
      "generates",
      "authors",
      "explored transformer",
      "answer",
      "types",
      "putting question",
      "related",
      "manages",
      "statistical",
      "answering systems",
      "system",
      "knowledge bases",
      "generally",
      "trec",
      "lstm",
      "example questions",
      "question answering using",
      "recurrent neural network",
      "wh-type",
      "extracting",
      "results\n\nexample questions",
      "why",
      "banko",
      "aaai symposium",
      "level components",
      "insurance qa",
      "deep",
      "literature review",
      "what is x?",
      "sources\n\naskmsr",
      "continuous",
      "deep learning\n\ndeep learning",
      "applied cnn",
      "text",
      "developed inner attention",
      "shao",
      "yes",
      "answer selection\n\nanswer",
      "named entity tagging",
      "answer extraction",
      "michele banko",
      "answerbus",
      "is more always better",
      "mining answers",
      "list",
      "proposed",
      "precision",
      "aaai",
      "tan",
      "definition questions",
      "big muddy",
      "non",
      "example",
      "trec qa track",
      "verifying",
      "common",
      "right bubbles",
      "svm",
      "answer selection\n\nconventional",
      "questions",
      "bottom panel",
      "google\n\nquestion",
      "employed cnn",
      "when",
      "lei",
      "text modeling",
      "webclopedia\n\ntrec",
      "conventional approaches",
      "linguistic",
      "submit",
      "categories",
      "pattern matching",
      "qas architecture",
      "hybrid framework",
      "conventional",
      "syst",
      "answers",
      "text retrieval conference\n\ntrec",
      "wikiqa",
      "susan dumais",
      "where",
      "reasoning",
      "text retrieval conference",
      "rephrasing",
      "deep learning",
      "claude monet",
      "map",
      "presented collaborative learning",
      "the",
      "acm trans",
      "manage",
      "attention",
      "acm",
      "nil",
      "proceedings",
      "andrew ng\n\npaper url",
      "s) are typically",
      "approaches",
      "information retrieval",
      "initial",
      "categorizing",
      "march",
      "users",
      "answer validation",
      "question types\n\nquestion",
      "leveraging",
      "deep learning approaches",
      "continued",
      "based approaches",
      "sentiment analysis",
      "began",
      "question answering",
      "web question answering",
      "question analysis",
      "insuranceqa",
      "many",
      "shubert",
      "symposium",
      "ranking",
      "knowledge base",
      "domain",
      "closed domain",
      "achieved accuracy",
      "trec qa",
      "machine learning",
      "left",
      "problem",
      "its",
      "comparing",
      "qas",
      "casual",
      "rnn",
      "translation",
      "they",
      "hypothetical",
      "natural language processing",
      "handles",
      "name 4 cities that have a",
      "qas architecture\n\nconceptual diagram explanation",
      "conv",
      "requires",
      "implemented",
      "this",
      "wang",
      "restricted",
      "question",
      "machine",
      "nie",
      "page",
      "includes",
      "based categorization",
      "qa systems reasoning",
      "start",
      "conf",
      "question classification",
      "short term memory",
      "systems",
      "published",
      "paragraph ordering",
      "inf",
      "field",
      "name",
      "project",
      "research",
      "detailed sub",
      "examples",
      "how",
      "analysis",
      "jimmy lin\n\npaper url",
      "semantic analysis",
      "feuerriegel",
      "document processing",
      "included",
      "pattern",
      "there",
      "successes",
      "cnn",
      "retrieval",
      "url",
      "question reformulation",
      "altavista\n\nquestion",
      "components",
      "paragraph filtering",
      "pinpointing",
      "revisited",
      "what would happen if",
      "wh-",
      "jimmy lin",
      "qa systems\n\nconceptual diagram explanation",
      "bilstm",
      "high",
      "based neural networks",
      "word",
      "theater.",
      "nlp",
      "(can be about any topic) and",
      "long",
      "mrr",
      "strong user demand",
      "used stacked bilstm",
      "show",
      "start\n\nproject",
      "cdrom",
      "tree",
      "neural networks",
      "feng",
      "transfer learning",
      "answer identification",
      "kratzwald",
      "qa system",
      "what",
      "start\n\nlooking ahead\n\nthe",
      "efficient domain customization",
      "open domain",
      "wordnet",
      "selecting"
    ],
    "technical_terms": [
      "rule",
      "simple",
      "askjeeves\n\nabout",
      "answer selection",
      "practice",
      "matching",
      "who",
      "qa systems",
      "convolutional neural network",
      "convolutional rnn",
      "these",
      "conventional machine",
      "support vector machine",
      "similar",
      "recurrent neural networks",
      "significant",
      "computer science",
      "worldwide web\n\nauthors",
      "measures",
      "feb",
      "retrieving",
      "eric brill",
      "information extraction",
      "conceptual table explanation",
      "utilized bilstm",
      "confirmation",
      "typically",
      "lexical",
      "reported map",
      "enormous interest",
      "answer processing",
      "introduction",
      "factoid",
      "question answering system",
      "question processing",
      "cnn\n\nliterature review",
      "some",
      "askjeeves",
      "bidirectional lstm",
      "answerbus\n\nwebsite",
      "generates",
      "authors",
      "explored transformer",
      "answer",
      "types",
      "putting question",
      "related",
      "manages",
      "statistical",
      "answering systems",
      "system",
      "knowledge bases",
      "generally",
      "trec",
      "lstm",
      "example questions",
      "question answering using",
      "recurrent neural network",
      "extracting",
      "results\n\nexample questions",
      "why",
      "banko",
      "aaai symposium",
      "level components",
      "insurance qa",
      "deep",
      "literature review",
      "sources\n\naskmsr",
      "continuous",
      "deep learning\n\ndeep learning",
      "applied cnn",
      "text",
      "developed inner attention",
      "shao",
      "answer selection\n\nanswer",
      "named entity tagging",
      "answer extraction",
      "michele banko",
      "answerbus",
      "is more always better",
      "mining answers",
      "list",
      "proposed",
      "precision",
      "aaai",
      "tan",
      "big muddy",
      "non",
      "example",
      "trec qa track",
      "verifying",
      "common",
      "right bubbles",
      "svm",
      "answer selection\n\nconventional",
      "questions",
      "bottom panel",
      "google\n\nquestion",
      "employed cnn",
      "when",
      "lei",
      "text modeling",
      "webclopedia\n\ntrec",
      "conventional approaches",
      "linguistic",
      "submit",
      "categories",
      "pattern matching",
      "qas architecture",
      "hybrid framework",
      "conventional",
      "syst",
      "answers",
      "text retrieval conference\n\ntrec",
      "wikiqa",
      "susan dumais",
      "where",
      "reasoning",
      "text retrieval conference",
      "rephrasing",
      "deep learning",
      "claude monet",
      "map",
      "presented collaborative learning",
      "the",
      "acm trans",
      "manage",
      "attention",
      "acm",
      "proceedings",
      "andrew ng\n\npaper url",
      "approaches",
      "information retrieval",
      "initial",
      "categorizing",
      "march",
      "users",
      "answer validation",
      "question types\n\nquestion",
      "leveraging",
      "deep learning approaches",
      "continued",
      "based approaches",
      "sentiment analysis",
      "began",
      "question answering",
      "web question answering",
      "question analysis",
      "insuranceqa",
      "many",
      "shubert",
      "symposium",
      "ranking",
      "knowledge base",
      "domain",
      "closed domain",
      "achieved accuracy",
      "trec qa",
      "machine learning",
      "left",
      "problem",
      "its",
      "comparing",
      "qas",
      "casual",
      "rnn",
      "translation",
      "they",
      "hypothetical",
      "natural language processing",
      "handles",
      "qas architecture\n\nconceptual diagram explanation",
      "conv",
      "requires",
      "implemented",
      "this",
      "wang",
      "restricted",
      "question",
      "machine",
      "nie",
      "page",
      "includes",
      "based categorization",
      "qa systems reasoning",
      "start",
      "conf",
      "question classification",
      "short term memory",
      "systems",
      "published",
      "paragraph ordering",
      "inf",
      "field",
      "name",
      "project",
      "research",
      "detailed sub",
      "examples",
      "how",
      "analysis",
      "jimmy lin\n\npaper url",
      "semantic analysis",
      "feuerriegel",
      "document processing",
      "included",
      "pattern",
      "there",
      "successes",
      "cnn",
      "retrieval",
      "url",
      "question reformulation",
      "altavista\n\nquestion",
      "components",
      "paragraph filtering",
      "pinpointing",
      "revisited",
      "jimmy lin",
      "qa systems\n\nconceptual diagram explanation",
      "bilstm",
      "high",
      "based neural networks",
      "word",
      "nlp",
      "long",
      "mrr",
      "strong user demand",
      "used stacked bilstm",
      "show",
      "start\n\nproject",
      "cdrom",
      "tree",
      "neural networks",
      "feng",
      "transfer learning",
      "answer identification",
      "kratzwald",
      "qa system",
      "what",
      "start\n\nlooking ahead\n\nthe",
      "efficient domain customization",
      "open domain",
      "wordnet",
      "selecting"
    ],
    "all_keywords": [
      "rule",
      "simple",
      "askjeeves\n\nabout",
      "answer selection",
      "practice",
      "matching",
      "who",
      "qa systems",
      "convolutional neural network",
      "convolutional rnn",
      "these",
      "conventional machine",
      "support vector machine",
      "similar",
      "recurrent neural networks",
      "significant",
      "computer science",
      "worldwide web\n\nauthors",
      "measures",
      "feb",
      "retrieving",
      "eric brill",
      "information extraction",
      "conceptual table explanation",
      "utilized bilstm",
      "confirmation",
      "typically",
      "lexical",
      "reported map",
      "enormous interest",
      "answer processing",
      "introduction",
      "factoid",
      "question answering system",
      "question processing",
      "cnn\n\nliterature review",
      "some",
      "askjeeves",
      "bidirectional lstm",
      "answerbus\n\nwebsite",
      "generates",
      "authors",
      "explored transformer",
      "answer",
      "types",
      "putting question",
      "related",
      "manages",
      "statistical",
      "answering systems",
      "system",
      "knowledge bases",
      "generally",
      "trec",
      "lstm",
      "example questions",
      "question answering using",
      "recurrent neural network",
      "wh-type",
      "extracting",
      "results\n\nexample questions",
      "why",
      "banko",
      "aaai symposium",
      "level components",
      "insurance qa",
      "deep",
      "literature review",
      "what is x?",
      "sources\n\naskmsr",
      "continuous",
      "deep learning\n\ndeep learning",
      "applied cnn",
      "text",
      "developed inner attention",
      "shao",
      "yes",
      "answer selection\n\nanswer",
      "named entity tagging",
      "answer extraction",
      "michele banko",
      "lec7",
      "answerbus",
      "is more always better",
      "mining answers",
      "list",
      "proposed",
      "precision",
      "aaai",
      "tan",
      "definition questions",
      "big muddy",
      "non",
      "example",
      "trec qa track",
      "verifying",
      "common",
      "right bubbles",
      "svm",
      "answer selection\n\nconventional",
      "questions",
      "bottom panel",
      "google\n\nquestion",
      "employed cnn",
      "when",
      "lei",
      "text modeling",
      "webclopedia\n\ntrec",
      "conventional approaches",
      "linguistic",
      "submit",
      "categories",
      "pattern matching",
      "qas architecture",
      "hybrid framework",
      "conventional",
      "syst",
      "answers",
      "text retrieval conference\n\ntrec",
      "wikiqa",
      "susan dumais",
      "where",
      "reasoning",
      "text retrieval conference",
      "rephrasing",
      "deep learning",
      "claude monet",
      "map",
      "presented collaborative learning",
      "the",
      "acm trans",
      "manage",
      "attention",
      "acm",
      "nil",
      "proceedings",
      "andrew ng\n\npaper url",
      "s) are typically",
      "approaches",
      "information retrieval",
      "initial",
      "categorizing",
      "march",
      "users",
      "answer validation",
      "question types\n\nquestion",
      "leveraging",
      "deep learning approaches",
      "continued",
      "based approaches",
      "sentiment analysis",
      "began",
      "question answering",
      "web question answering",
      "question analysis",
      "insuranceqa",
      "many",
      "shubert",
      "symposium",
      "ranking",
      "knowledge base",
      "domain",
      "closed domain",
      "achieved accuracy",
      "trec qa",
      "machine learning",
      "left",
      "problem",
      "its",
      "comparing",
      "qas",
      "casual",
      "rnn",
      "translation",
      "they",
      "hypothetical",
      "natural language processing",
      "handles",
      "name 4 cities that have a",
      "qas architecture\n\nconceptual diagram explanation",
      "conv",
      "requires",
      "implemented",
      "this",
      "wang",
      "restricted",
      "question",
      "machine",
      "nie",
      "page",
      "includes",
      "based categorization",
      "qa systems reasoning",
      "start",
      "conf",
      "question classification",
      "short term memory",
      "systems",
      "published",
      "paragraph ordering",
      "inf",
      "field",
      "name",
      "project",
      "research",
      "detailed sub",
      "examples",
      "how",
      "analysis",
      "jimmy lin\n\npaper url",
      "semantic analysis",
      "feuerriegel",
      "document processing",
      "included",
      "pattern",
      "there",
      "successes",
      "cnn",
      "retrieval",
      "url",
      "question reformulation",
      "altavista\n\nquestion",
      "components",
      "paragraph filtering",
      "pinpointing",
      "revisited",
      "what would happen if",
      "wh-",
      "jimmy lin",
      "qa systems\n\nconceptual diagram explanation",
      "bilstm",
      "high",
      "based neural networks",
      "word",
      "theater.",
      "nlp",
      "(can be about any topic) and",
      "long",
      "mrr",
      "strong user demand",
      "used stacked bilstm",
      "show",
      "start\n\nproject",
      "cdrom",
      "tree",
      "neural networks",
      "feng",
      "transfer learning",
      "answer identification",
      "kratzwald",
      "qa system",
      "what",
      "start\n\nlooking ahead\n\nthe",
      "efficient domain customization",
      "open domain",
      "wordnet",
      "selecting"
    ],
    "keyword_string": "rule simple askjeeves\n\nabout answer selection practice matching who qa systems convolutional neural network convolutional rnn these conventional machine support vector machine similar recurrent neural networks significant computer science worldwide web\n\nauthors measures feb retrieving eric brill information extraction conceptual table explanation utilized bilstm confirmation typically lexical reported map enormous interest answer processing introduction factoid question answering system question processing cnn\n\nliterature review some askjeeves bidirectional lstm answerbus\n\nwebsite generates authors explored transformer answer types putting question related manages statistical answering systems system knowledge bases generally trec lstm example questions question answering using recurrent neural network wh-type extracting results\n\nexample questions why banko aaai symposium level components insurance qa deep literature review what is x? sources\n\naskmsr continuous deep learning\n\ndeep learning applied cnn text developed inner attention shao yes answer selection\n\nanswer named entity tagging answer extraction michele banko lec7 answerbus is more always better mining answers list proposed precision aaai tan definition questions big muddy non example trec qa track verifying common right bubbles svm answer selection\n\nconventional questions bottom panel google\n\nquestion employed cnn when lei text modeling webclopedia\n\ntrec conventional approaches linguistic submit categories pattern matching qas architecture hybrid framework conventional syst answers text retrieval conference\n\ntrec wikiqa susan dumais where reasoning text retrieval conference rephrasing deep learning claude monet map presented collaborative learning the acm trans manage attention acm nil proceedings andrew ng\n\npaper url s) are typically approaches information retrieval initial categorizing march users answer validation question types\n\nquestion leveraging deep learning approaches continued based approaches sentiment analysis began question answering web question answering question analysis insuranceqa many shubert symposium ranking knowledge base domain closed domain achieved accuracy trec qa machine learning left problem its comparing qas casual rnn translation they hypothetical natural language processing handles name 4 cities that have a qas architecture\n\nconceptual diagram explanation conv requires implemented this wang restricted question machine nie page includes based categorization qa systems reasoning start conf question classification short term memory systems published paragraph ordering inf field name project research detailed sub examples how analysis jimmy lin\n\npaper url semantic analysis feuerriegel document processing included pattern there successes cnn retrieval url question reformulation altavista\n\nquestion components paragraph filtering pinpointing revisited what would happen if wh- jimmy lin qa systems\n\nconceptual diagram explanation bilstm high based neural networks word theater. nlp (can be about any topic) and long mrr strong user demand used stacked bilstm show start\n\nproject cdrom tree neural networks feng transfer learning answer identification kratzwald qa system what start\n\nlooking ahead\n\nthe efficient domain customization open domain wordnet selecting",
    "token_count": 2585,
    "word_count": 1571,
    "sentence_count": 105,
    "paragraph_count": 156,
    "technical_weight": 1.8,
    "narrative_weight": 1.0,
    "document_density": 0.6077369439071567,
    "avg_sentence_length": 14.961904761904762,
    "readability_score": 100,
    "has_technical_terms": true,
    "is_structured": false,
    "has_definitions": true,
    "content_type": "Technical, Definitions"
  }
]